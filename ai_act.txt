REGUL A TION (EU) 2024/1689 OF THE EUR OPEAN P ARLIAMENT AND OF THE CO UNCIL
of 13 June 2024
laying do wn har monised r ules on ar tif icial intelligence and amending Regulations (EC) No 300/2008,
(EU) No 167/2013, (EU) No 168/2013, (EU) 2018/858, (EU) 2018/1139 and (EU) 2019/2144 and
Directiv es 2014/90/EU, (EU) 2016/797 and (EU) 2020/1828 (Ar tif icial Intelligence A ct)
(T ext with EEA relevance)
THE EUR OPEAN P ARLIAMENT AND THE COUNCIL OF THE EUR OPEAN UNION,
Having regard to the T reaty on the Functioning of the European Union, and in par ticular Ar ticles 16 and 114 thereof,
Having regard to the proposal from the European Commission,
Af ter transmission of the draf t legislative act to the national parliaments,
Having regard to the opinion of the European Economic and Social Committe e (
1
),
Having regard to the opinion of the European Central Bank (
2
),
Having regard to the opinion of the Committee of the Regions (
3
),
A cting in accordance with the ordinar y legislative procedure (
4
),
Whereas:
(1) The pur pose of this Regulation is to imp rove the functioning of the internal marke t by la ying do wn a unif or m leg al
framew ork in par ticular f or the development, the placing on the market, the putting into ser vice and the use of
ar tificial inte lligence syste ms (AI systems) in the Uni on, in accordance with Union values, to promote the uptak e of
human centr ic and tr ustwor thy ar tificial intellig ence (AI) while ensur ing a high level of prot ection of health, saf ety ,
fundamental r ights as enshr ined in the Char te r of Fundamental Rights of the European Union (the ‘Char te r ’),
including democracy , the r ule of law and environmental prot ection, to protect against the har mful effe cts of AI
syste ms in the Uni on, and to suppor t inno vation. This Regulation ensures the free moveme nt, cross-border , of
AI-based goods and ser vices, thus preventing Member Stat es from imp osing restr ictions on the development,
mark eting and use of AI systems, unless explicitly author ised by this Regulation.
(2) This Regulation should be applied in accordance with the values of the Uni on enshr ined as in the Char te r , f acilitating
the protection of natural persons, under takings, democracy , the r ule of law and environmental prot ection, while
boosting innovation and emplo yment and making the Uni on a leader in the up take of tr ustwo r th y AI.
(3) AI systems can be easily deplo yed in a larg e var iety of sect ors of the economy and many par ts of society , including
across borders, and can easily circulat e throughout the Uni on. Cer tain Member States hav e already explored the
adop tion of national r ules to ensure that AI is tr ustwor th y and saf e and is developed and used in accordance with
fundamental r ights obliga tions. Diverging national r ules ma y lead to the fragmentation of the inter nal market and
ma y decrease leg al cer tainty f or operators that develop, im por t or use AI systems. A consistent and high level of
prot ection throughout the Union should theref ore be ensured in order to achi eve tr ustw or th y AI, while divergence s
ham per ing the free circulation, inno vation, deplo yment and the uptak e of AI systems and related products and
ser vices within the inter nal marke t should be prevent ed by la ying do wn unif or m obligations f or operato rs and
Offi cial Jour nal
of the European Union
EN
L ser ies
2024/1689
12.7.2024
ELI: http://data.europa.eu/eli/reg/2024/1689/oj 1/144
(
1
) OJ C 517, 22.12.2021, p. 56.
(
2
) OJ C 115, 11.3.2022, p. 5.
(
3
) OJ C 97, 28.2.2022, p. 60.
(
4
) P osition of the European Parl iament of 13 March 2024 (not yet published in the Official Jour nal) and decision of the Council of
21 Ma y 2024.guarante eing the unif or m protect ion of ove r r iding reasons of public inte rest and of r ights of persons throughout the
inte r nal marke t on the basis of Ar ticle 114 of the T reaty on the Functioning of the European Union (TFEU). T o the
exte nt that this Regulation contains specif ic r ules on the protect ion of individuals with regard to the processing of
personal data concer ning restr ictions of the use of AI syste ms f or remote biometr ic identification f or the pur pose of
la w enf orcement, of the use of AI systems f or r isk assessments of natural persons f or the pur pose of la w
enf orcement and of the use of AI systems of biometr ic cate gor isation f or the pur pose of law enf orcement, it is
appropr iat e to base this Regulation, in so f ar as those specific r ules are concer ned, on Ar ticle 16 TFEU. In light of
those specific r ules and the recourse to Ar ticle 16 TFEU, it is appropr iate to consult the European Data Protection
Board.
(4) AI is a f ast ev olving f amily of t echnologies that contr ibut es to a wide ar ra y of economic, environmental and societal
benefits across the entire spectr um of industr ies and social activities. By imp roving prediction, optimising operations
and resource allocation, and personalising digital solutions ava ilable f or individuals and org anisations, the use of AI
can provide k ey comp etitive advantages t o under takings and suppor t socially and environmentally beneficial
outcomes, f or exam ple in healthcare, agr iculture, f ood safety , education and training, media, spor ts, culture,
infrastr ucture manag ement, energy , transpor t and logistics, public ser vices, secur ity , justice, resource and energy
efficiency , en vironmental monitoring, the conser vation and restoration of biodiversity and ecosystems and climate
ch ange mitig ation and adap tation.
(5) At the same time, depending on the circumstances regarding its specific application, use, and level of te chnological
development, AI ma y g enerate r isks and cause har m to public interests and fundamental r ights that are protect ed by
Uni on la w . Such har m might be material or immater ial, including physical, psy ch ological, societal or economic
har m.
(6) Given the major im pact that AI can hav e on society and the need to build tr ust, it is vital f or AI and its regulator y
framew ork to be developed in accordance with Union values as enshr ined in Ar ticle 2 of the T reaty on European
Uni on (TEU), the fundamental r ights and freedoms enshr ined in the T reaties and, pursuant to Ar ticle 6 TEU, the
Char te r . As a prerequisite , AI should be a human-centr ic tec hnology . It should ser ve as a too l f or people, with the
ultimate aim of increasing human well-being.
(7) In order to ensure a consistent and high level of protection of public intere sts as regard s health, saf ety and
fundamental r ights, common r ules f or high-r isk AI syste ms should be established. Those r ules should be consiste nt
with the Char te r , non-discr iminat or y and in line with the Uni on’s inte r national trade commitments. They should
also take into account the European Declaration on Digital Rights and Pr inciples f or the Digital Decade and the
Ethics guidelines f or tr ustw or th y AI of the High-Level Exper t Group on Ar tif icial Intellig ence (AI HLEG).
(8) A Union lega l framew ork la ying do wn har monised r ules on AI is theref ore needed to f oster the development, use
and up take of AI in the inter nal marke t that at the same time meets a high level of protect ion of public interests, such
as health and saf ety and the prot ection of fundamental r ights, including democracy , the r ule of law and
en vironmental protect ion as recognised and protect ed by Uni on la w . T o ac hieve that objective, r ules regulating the
placing on the marke t, the putting into ser vice and the use of cer tain AI systems should be laid down, thus ensur ing
the smooth functioning of the inte r nal market and allowi ng those systems to benefit from the pr inciple of free
move ment of goods and ser vices. Those r ules should be clear and robust in protecting fundamental r ights,
suppor tive of new innovative solutions, enabling a European ecosyste m of public and pr ivate actor s creating AI
syste ms in line with Union values and unlocking the potential of the digital transf or mation across all regions of the
Uni on. By la ying down those r ules as well as measures in suppor t of inno vation with a par ticular f ocus on small and
medium ent er pr ises (SMEs), including star tups, this Regulation suppor ts the objective of promoting the European
human-centr ic approach to AI and being a g lobal leader in the development of secure, tr ustwor thy and ethical AI as
stat ed by the European Council (
5
), and it ensures the prot ection of ethical pr inciples, as specific ally request ed by the
European Pa rliament (
6
).
EN
OJ L, 12.7.2024
2/144 ELI: http://data.europa.eu/eli/reg/2024/1689/oj
(
5
) European Council, Special meeting of the European Council (1 and 2 October 2020) — Conclusions, EUC O 13/20, 2020, p. 6.
(
6
) European Parl iament resolution of 20 October 2020 with recommendations to the Commission on a framew ork of ethical aspects
of ar tificia l intellig ence, robotics and relat ed tec hnologies, 2020/2012(INL).(9) Har monised r ules applicable to the placing on the mark et, the putting into ser vice and the use of high-r isk AI
syste ms should be laid do wn consistently with Regulation (EC) No 765/2008 of the European Pa rliament and of the
Council (
7
), Decision No 768/2008/EC of the European Pa rliament and of the Council (
8
) and Regulation (EU)
2019/1020 of the European P arliament and of the Council (
9
) (New Legislative Framewo rk). The har monised r ules
laid down in this Regulation should apply across sectors and, in line with the New Legislative Framework, should be
without prejudice to existing Uni on law , in par ticular on data protect ion, consumer prot ection, fundamental r ights,
em plo yment, and prot ection of work ers, and product safety , to which this Regulation is comp lementar y . As
a consequence, all r ights and remedies provid ed f or by suc h Union law t o consumers, and other persons on whom
AI syste ms ma y hav e a nega tive impact, including as rega rds the comp ensation of possible damage s pursuant to
Council Directive 85/374/EEC (
10
) remain unaff ected and fully applicable. Fur ther more, in the context of
em plo yment and protect ion of wo rkers, this Regulation should theref ore not affe ct Uni on law on social policy and
national labour law , in compliance with Uni on law , concer ning emplo yment and wo rking conditions, including
health and saf ety at work and the relationship between emp lo yers and w orkers. This Regulation should also not
aff ect the ex ercise of fundamental r ights as recognised in the Member Stat es and at Uni on level, including the r ight or
freedom to str ike or to take other action covered b y the specif ic industr ial relations syste ms in Member States as well
as the r ight to negotiat e, to conclude and enf orce collective agreements or t o take collective action in accordance
with national law . This Regulation should not affect the provisions aiming t o im prove working conditions in
platf or m wo rk laid down in a Directive of the European Parliament and of the Council on imp roving working
conditions in platf or m work. Moreover , this Regulation aims to strengthen the effe ctiveness of suc h existing r ights
and remedies by establishing specific requirements and obliga tions, including in respect of the transparency ,
t echnical documentation and record-keepi ng of AI syste ms. Fur ther more, the obliga tions placed on var ious
operat ors in volved in the AI value chain under this Regulation should apply without prejudice to national la w , in
com pliance with Uni on law , having the eff ect of limiting the use of cer tain AI systems where suc h la w f alls outside
the scope of this Regulation or pursues legitimate public interest objectives other than those pursued by this
Regulation. For exam ple, national labour law and la w on the protection of minors, namely persons below the age of
18, taking into account the UNCR C General Comment No 25 (2021) on children’s r ights in relation to the digital
en vironment, insofa r as the y are not specif ic to AI syste ms and pursue other legitimate public inte rest objectives,
should not be affe cted by this Regulation.
(10) The fundamental r ight to the prot ection of personal data is safegua rded in par ticular by Regulations (EU)
2016/679 (
11
) and (EU) 2018/1725 (
12
) of the European Pa rliament and of the Council and Directive (EU) 2016/680
of the European Parliament and of the Council (
13
). Directive 2002/58/EC of the European Parliament and of the
Council (
14
) additionally prot ects pr ivate lif e and the confi dentiality of communications, including by wa y of
pro viding conditions f or any st or ing of personal and non-personal data in, and access from, ter minal equipment.
Those Union lega l acts provide the basis f or sustainable and responsible data processing, including where data sets
include a mix of personal and non-personal data. This Regulation does not seek to affect the application of existing
Uni on law gover ning the processing of personal data, including the tasks and po wers of the independent super visor y
author ities compet ent to monito r compliance with those instr uments. It also does not affe ct the obliga tions of
pro viders and deplo yers of AI syste ms in their role as data controllers or processors st emming from Union or
national law on the prot ection of personal data in so f ar as the design, the development or the use of AI syste ms
in v olves the processing of personal data. It is also appropr iate to clar ify that data subjects continue t o enjo y all the
OJ L, 12.7.2024
EN
ELI: http://data.europa.eu/eli/reg/2024/1689/oj 3/144
(
7
) Regulation (EC) No 765/2008 of the European Parl iament and of the Council of 9 July 2008 setting out the requirements f or
accreditation and repealing Regulation (EEC) No 339/93 (OJ L 218, 13.8.2008, p. 30).
(
8
) Decision No 768/2008/EC of the European Parliament and of the Council of 9 July 2008 on a common framework f or the
mark eting of products, and repealing Council Decision 93/465/EEC (OJ L 218, 13.8.2008, p. 82).
(
9
) Regulation (EU) 2019/1020 of the European Parl iament and of the Council of 20 June 2019 on market sur veillance and compliance
of products and amending Directive 2004/42/EC and Regulations (EC) No 765/2008 and (EU) No 305/2011 (OJ L 169, 25.6.2019,
p. 1).
(
10
) Council Directive 85/374/EEC of 25 July 1985 on the appro ximation of the la ws, regulations and administrative provisions of the
Member States concer ning liability f or defe ctive products (OJ L 210, 7.8.1985, p. 29).
(
11
) Regulation (EU) 2016/679 of the European Parliament and of the Council of 27 Apr il 2016 on the prot ection of natural persons
with regard t o the processing of personal data and on the free mo vement of suc h data, and repealing Directive 95/46/EC (General
Data Protection Regulation) (OJ L 119, 4.5.2016, p. 1).
(
12
) Regulation (EU) 2018/1725 of the European Parliament and of the Council of 23 October 2018 on the protection of natural
persons with regard t o the processing of personal data b y the Union institutions, bodies, offices and agencies and on the free
mo vement of such data, and repealing Regulation (EC) No 45/2001 and Decision No 1247/2002/EC (OJ L 295, 21.11.2018, p. 39).
(
13
) Directive (EU) 2016/680 of the European Parl iament and of the Council of 27 Apr il 2016 on the prot ection of natural persons with
reg ard t o the processing of personal data b y compet ent author ities f or the pur poses of the prevention, investig ation, detection or
prosecution of cr iminal offences or the execution of cr iminal penalties, and on the free movement of such data, and repealing
Council Framework Decision 2008/977/JHA (OJ L 119, 4.5.2016, p. 89).
(
14
) Directive 2002/58/EC of the European Parl iament and of the Council of 12 July 2002 concer ning the processing of personal data
and the prot ection of pr ivacy in the electronic communications sector (Directive on pr ivacy and electronic communications) (OJ
L 201, 31.7.2002, p. 37).r ights and guarantees a warded to them b y such Uni on law , including the r ights related to solely automat ed individual
decision-making, including prof iling. Har monised r ules f or the placing on the marke t, the putting into ser vice and
the use of AI systems established under this Regulation should f acilitate the effe ctive imp lementation and enable the
ex ercise of the data subjects’ r ights and other remedies guaranteed under Uni on law on the protect ion of personal
data and of other fundamental r ights.
(11) This Regulation should be without prejudice to the provisions rega rding the liability of provider s of inte r mediar y
ser vices as set out in Regulation (EU) 2022/2065 of the European Parliament and of the Council (
15
).
(12) The notion of ‘ AI system’ in this Regulation should be clearly defined and should be closely aligned with the w ork of
inte r national organisations w orking on AI to ensure leg al cer tainty , f acilitate inter national conve rgence and wide
accep tance, while providing the f lexibility to accommodate the rapid te chnologi cal developments in this field.
Moreo ver , the def inition should be based on k ey char acter istics of AI systems that distinguish it from simpler
traditional sof tware syste ms or programming approac hes and should not cover syste ms that are based on the r ules
defined solely by natural persons to auto matically ex ecute operations. A ke y ch aracter istic of AI syste ms is their
capability to infer . This capability t o infer refers t o the process of obtaining the outputs, suc h as predictions, cont ent,
recommendations, or decisions, which can inf luence physica l and vir tual envir onments, and to a capability of AI
syste ms to der ive models or algor ithms, or both, from in puts or data. The tec hniques that enable inf erence while
building an AI syste m include machi ne lear ning approaches that lear n from data how t o ac hieve cer tain objectives,
and logic- and kno wledge-based approac hes that inf er from encoded knowle dge or symbolic representation of the
task to be solved. The capacity of an AI syste m to infer transcends basic data processing by enabling lear ning,
reasoning or modelling. The ter m ‘mach ine-based’ refe rs to the f act that AI systems r un on machi nes. The reference
t o explicit or im plicit objectives underscores that AI syste ms can operat e according to explicit def ined objectives or
t o imp licit objectives. The objectives of the AI system ma y be diff erent from the inte nded pur pose of the AI system
in a specific cont ext. For the pur poses of this Regulation, envir onments should be understood to be the contexts in
which the AI systems operate, whereas outputs g enerated by the AI syste m ref lect diffe rent functions perfo r med by
AI syste ms and include predictions, cont ent, recommendations or decisions. AI syste ms are designed to operate with
var ying levels of autonom y , meaning that the y hav e some degree of independence of actions from human
in v olvement and of capabilities to operat e without human inte r vention. The adap tiveness that an AI syste m could
exhibit af te r deplo yment, refe rs to self-lear ning capabilities, allowi ng the syste m to ch ange while in use. AI syste ms
can be used on a stand-alone basis or as a compo nent of a product, ir respective of whether the syste m is phys ically
inte grated into the product (embedded) or ser ves the functionality of the product without being integrat ed therein
(non-embedded).
(13) The notion of ‘deplo y er ’ refe r red t o in this Regulation should be inte r preted as any natural or lega l person, including
a public author ity , agency or other body , using an AI system under its author ity , excep t where the AI syste m is used
in the course of a personal non-profess ional activity . Depending on the type of AI syste m, the use of the system ma y
aff ect persons other than the deplo y er .
(14) The notion of ‘biometr ic data’ used in this Regulation should be inter preted in light of the notion of biometr ic data
as defined in Ar ticle 4, point (14) of Regulation (EU) 2016/679, Ar ticle 3, point (18) of Regulation (EU) 2018/1725
and Ar ticle 3, point (13) of Directive (EU) 2016/680. Biometr ic data can allo w f or the authentication, identification
or cate gor isation of natural persons and f or the recognition of emotions of natural persons.
(15) The notion of ‘biometr ic identifi cation’ refe r red to in this Regulation should be def ined as the auto mated recognition
of physical, physiological and behavi oural human features such as the f ace, ey e move ment, body shape, v oice,
prosody , g ait, posture, hear t rate, blood pressure, odour , k eystrok es ch aracter istics, f or the pur pose of establishing an
individual’s identity b y compari ng biometr ic data of that individual to stored biometr ic data of individuals in
a refere nce database, ir respective of whether the individual has given its consent or not. This ex cludes AI syste ms
inte nded to be used f or biometr ic ver ificati on, which includes authentication, whose sole pur pose is to confi r m that
a specific natural person is the person he or she claims to be and to confi r m the identity of a natural person f or the
sole pur pose of hav ing access to a ser vice, unlocking a device or hav ing secur ity access to premises.
EN
OJ L, 12.7.2024
4/144 ELI: http://data.europa.eu/eli/reg/2024/1689/oj
(
15
) Regulation (EU) 2022/2065 of the European Parliament and of the Council of 19 October 2022 on a Sing le Market For Digital
Ser vices and amending Directive 2000/31/EC (Digital Ser vices A ct) (OJ L 277, 27.10.2022, p. 1).(16) The notion of ‘biometr ic catego r isation’ refer red to in this Regulation should be def ined as assigning natural persons
t o specif ic cate gor ies on the basis of their biometr ic data. Such specif ic catego r ies can relate to aspects suc h as sex,
ag e, hair colour , e ye colour , tattoos, behavio ural or personality traits, languag e, religion, membership of a national
minor ity , sexual or political or ientation. This does not include biometr ic catego r isation systems that are a purely
ancillar y f eature intr insically linked to another commercial ser vice, meaning that the f eature cannot, f or objective
t echnical reasons, be used without the pr incipal ser vice, and the inte gration of that feature or functionality is not
a means to circum vent the applicability of the r ules of this Regulation. For example, filters cate gor ising f acial or body
f eatures used on online marke tplaces could constitute suc h an ancillar y f eature as they can be used only in relation to
the pr incipal ser vice which consists in selling a product by allowing the consumer to preview the displa y of the
product on him or herself and help the consumer to mak e a purcha se decision. Filte rs used on online social netw ork
ser vices which cate gor ise f acial or body f eatures t o allow users to add or modify pictures or videos could also be
considered to be ancillar y f eature as suc h filter cannot be used without the pr incipal ser vice of the social netw ork
ser vices consisting in the shar ing of cont ent online.
(17) The notion of ‘remot e biometr ic identifica tion syste m’ refe r red to in this Regulation should be def ined functionally ,
as an AI syste m inte nded f or the identifica tion of natural persons without their active in volvement, typically at
a distance, through the comp ar ison of a person’s biometr ic data with the biometr ic data contained in a reference
database, ir respectively of the par ticular tec hnology , processes or types of biometr ic data used. Such remote
biometr ic identifica tion systems are typically used to perceive multiple persons or their behavio ur simultaneously in
order t o f acilitate signif icantly the identification of natural persons without their active inv olvement. This ex cludes
AI syste ms inte nded t o be used f or biometr ic ver if ication, which includes authentication, the sole pur pose of which
is to confi r m that a specif ic natural person is the person he or she claims to be and to confi r m the identity of
a natural person f or the sole pur pose of hav ing access to a ser vice, unloc king a device or hav ing secur ity access to
premises. That exclusion is justified b y the f act that suc h syste ms are like ly to hav e a minor imp act on fundamental
r ights of natural persons comp ared to the remote biometr ic identifi cation syste ms which ma y be used f or the
processing of the biometr ic data of a larg e number of persons without their active in v olvement. In the case of
‘real-time’ syste ms, the captu r ing of the biometr ic data, the comp ar ison and the identifi cation occur all
instantaneously , near -instantaneously or in any event without a significant dela y . In this regard, there should be no
scope f or circum venting the r ules of this Regulation on the ‘real-time’ use of the AI systems concer ned by providing
f or minor dela ys. ‘Real-time’ syste ms in v olve the use of ‘live’ or ‘near -live’ material, suc h as video f ootage , generat ed
b y a camera or other device with similar functionality . In the case of ‘post’ systems, in contrast, the biometr ic data
has already been captured and the comp ar ison and identifica tion occur only af ter a significant dela y . This inv olves
mat er ial, such as pictures or video f ootage g enerated by closed circuit te levision cameras or pr ivate devices, which
has been g enerated bef ore the use of the syste m in respect of the natural persons concer ned.
(18) The notion of ‘emotion recognition system’ refe r red to in this Regulation should be def ined as an AI syste m f or the
pur pose of identifying or inferr ing emotions or intentions of natural persons on the basis of their biometr ic data.
The notion refe rs to emotions or intent ions suc h as happiness, sadness, anger , sur pr ise, disgust, embar rassment,
ex cite ment, shame, contem pt , satisfaction and amusement. It does not include physical stat es, such as pain or
f atigue, including, f or exam ple, syste ms used in detecting the state of f atigue of profe ssional pilots or dr ivers f or the
pur pose of preventing accidents. This does also not include the mere detection of readily apparent expressions,
g estures or movements, unless the y are used f or identifying or inferr ing emotions. Those expressions can be basic
f acial expressions, suc h as a frown or a smile, or gestures suc h as the moveme nt of hands, ar ms or head, or
ch aracteristics of a person’s voice, suc h as a raised voice or whisper ing.
(19) For the pur poses of this Regulation the notion of ‘publicly accessible space’ should be understood as refe r r ing to any
phys ical space that is accessible to an undeter mined number of natural persons, and ir respective of whether the
space in question is pr ivat ely or publicly ow ned, ir respective of the activity f or which the space ma y be used, suc h as
f or commerce, f or exam ple, shops, restaurants, cafés; f or ser vices, f or exam ple, banks, professional activities,
hospitality ; f or spor t, f or exam ple, swimming pools, gyms, stadiums; f or transpor t, f or exam ple, bus, metro and
railwa y stations, air por ts, means of transpor t ; f or ent er tainment, f or exam ple, cinemas, theatres, museums, concer t
and confe rence halls; or f or leisure or other wise, f or example, public roads and squares, park s, f orests, pla ygrounds.
A space should also be classified as being publicly accessible if, regard less of pote ntial capacity or secur ity
restr ictions, access is subject t o cer tain predete r mined conditions which can be fulf illed by an undeter mined number
of persons, suc h as the purc hase of a tick et or title of transpor t, pr ior registration or hav ing a cer tain age. In contrast,
a space should not be considered to be publicly accessible if access is limited to specif ic and def ined natural persons
through either Uni on or national law directly related t o public safety or secur ity or through the clear manif estation
OJ L, 12.7.2024
EN
ELI: http://data.europa.eu/eli/reg/2024/1689/oj 5/144of will by the person having the relevant author ity ove r the space. The f actual possibility of access alone, such as an
unloc ked door or an open g at e in a f ence, does not imp ly that the space is publicly accessible in the presence of
indications or circumstances suggesting the contrar y , suc h as. signs prohibiting or restr icting access. Compan y and
f actory premises, as well as offices and workplaces that are intende d to be accessed only by relevant emp lo yees and
ser vice providers, are spaces that are not publicly accessible. Publicly accessible spaces should not include pr isons or
border control. Some other spaces ma y compr ise both publicly accessible and non-publicly accessible spaces, suc h as
the hallwa y of a pr ivat e residential building necessar y to access a docto r ’s off ice or an air por t. Online spaces are not
co vered, as they are not physical spaces. Whether a given space is accessible to the public should howe ver be
det er mined on a case-by-c ase basis, having regard to the specif icities of the individual situation at hand.
(20) In order to obtain the great est benefits from AI syste ms while prot ecting fundamental r ights, health and saf ety and to
enable democratic control, AI lite racy should equip providers, deplo yers and affect ed persons with the necessar y
notions to mak e inf or med decisions regarding AI systems. Those notions ma y var y with regard to the relevant
cont ext and can include understanding the cor rect application of t echnical elements dur ing the AI system’s
development phase, the measures to be applied dur ing its use, the suitable wa ys in which to inter pret the AI system’s
output, and, in the case of affe cted persons, the kno wledge necessar y to understand how decisions tak en with the
assistance of AI will hav e an imp act on them. In the context of the application this Regulation, AI literacy should
pro vide all relevant actors in the AI value ch ain with the insights required to ensure the appropr iat e compliance and
its cor rect enf orcement. Fur ther more, the wide im plementation of AI literac y measures and the introduction of
appropr iat e f ollow-up actions could contr ibut e to impro ving work ing conditions and ultimately sustain the
consolidation, and inno vation path of tr ustwor thy AI in the Uni on. The European Ar tificial Intellig ence Board (the
‘Board’) should suppor t the Commission, to promot e AI litera cy tools, public a wareness and understanding of the
benefits, r isks , safegua rds, r ights and obliga tions in relation to the use of AI syste ms. In cooperation with the relevant
stak eholders, the Commission and the Member States should f acilitate the drawing up of v oluntar y codes of conduct
t o advance AI lite racy among persons dealing with the development, operation and use of AI.
(21) In order to ensure a level pla ying field and an effective protect ion of r ights and freedoms of individuals across the
Uni on, the r ules established by this Regulation should apply to provid ers of AI syste ms in a non-discr iminat or y
manner , ir respective of whether the y are established within the Uni on or in a third countr y , and to deplo y ers of AI
syste ms established within the Uni on.
(22) In light of their digital nature, cer tain AI syste ms should f all within the scope of this Regulation even when they are
not placed on the marke t, put into ser vice, or used in the Uni on. This is the case, f or example, where an operator
established in the Union contracts cer tain ser vices to an operator established in a third countr y in relation t o an
activity to be perform ed by an AI system that would qualify as high-r isk. In those circumstances, the AI syste m used
in a third countr y by the operator could process data law fully collected in and transferre d from the Uni on, and
pro vide to the contracting operato r in the Uni on the output of that AI system resulting from that processing,
without that AI syste m being placed on the marke t, put into ser vice or used in the Uni on. T o prevent the
circum vention of this Regulation and to ensure an effe ctive prot ection of natural persons located in the Union, this
Regulation should also apply to providers and deplo yers of AI systems that are established in a third countr y , to the
exte nt the output produced b y those syste ms is intende d t o be used in the Union. Nonetheless, to take into account
existing ar rang ements and special needs f or future cooperation with f oreign par tners with whom inf or mation and
evidence is ex chang ed, this Regulation should not apply to public author ities of a third countr y and inter national
org anisations when acting in the framework of cooperation or inter national agreements concluded at Uni on or
national level f or law enf orcement and judicial cooperation with the Union or the Member Stat es, provided that the
relevant third countr y or inte r national organisation provides adequate safegua rds with respect to the prot ection of
fundamental r ights and freedoms of individuals. Where relevant, this ma y co ver activities of entities entr usted by the
third countr ies to car r y out specif ic tasks in suppor t of suc h law enf orcement and judicial cooperation. Such
framew ork f or cooperation or agreements hav e been established bilat erally between Member States and third
countr ies or between the European Union, Europol and other Union agencies and third countr ies and inte r national
org anisations. The author ities compet ent f or super vision of the law enf orcement and judicial author ities under this
Regulation should assess whether those framew orks f or cooperation or inter national agreements include adequate
saf eguards with respect to the protect ion of fundamental r ights and freedoms of individuals. Recipient national
EN
OJ L, 12.7.2024
6/144 ELI: http://data.europa.eu/eli/reg/2024/1689/ojauthor ities and Uni on institutions, bodies, off ices and agencies making use of suc h outputs in the Uni on remain
accountable to ensure their use comp lies with Uni on law . When those inter national agreements are revised or new
ones are concluded in the future, the contracting par ties should make utmost eff or ts to align those agreements with
the requirements of this Regulation.
(23) This Regulation should also apply to Uni on institutions, bodies, off ices and agencies when acting as a provid er or
deplo y er of an AI syste m.
(24) If, and insofa r as, AI systems are placed on the marke t, put into ser vice, or used with or without modification of such
syste ms f or militar y , defe nce or national secur ity pur poses, those should be excluded from the scope of this
Regulation regard less of which type of entity is car r ying out those activities, such as whether it is a public or pr ivate
entity . As regard s militar y and defe nce pur poses, suc h ex clusion is justif ied both by Ar ticle 4(2) TEU and b y the
specificities of the Member Stat es’ and the common Union defence policy covered by Chapt er 2 of Title V TEU that
are subject to public inter national law , which is theref ore the more appropr iate legal framew ork f or the regulation of
AI systems in the cont ext of the use of lethal f orce and other AI syste ms in the context of militar y and defence
activities. As rega rds national secur ity pur poses, the ex clusion is justified both by the fact that national secur ity
remains the sole responsibility of Member States in accordance with Ar ticle 4(2) TEU and by the specif ic nature and
operational needs of national secur ity activities and specific national r ules applicable to those activities. Nonetheless,
if an AI system developed, placed on the marke t, put into ser vice or used f or militar y , defence or national secur ity
pur poses is used outside those te m porar ily or per manently f or other pur poses, f or exam ple, civilian or humanitar ian
pur poses, law enf orcement or public secur ity pur poses, suc h a system would f all within the scope of this Regulation.
In that case, the entity using the AI syste m f or other than militar y , defe nce or national secur ity pur poses should
ensure the compliance of the AI syste m with this Regulation, unless the syste m is already compliant with this
Regulation. AI syste ms placed on the marke t or put into ser vice f or an ex cluded pur pose, namely militar y , defe nce or
national secur ity , and one or more non-exclu ded pur poses, suc h as civilian pur poses or law enf orcement, f all within
the scope of this Regulation and pro viders of those systems should ensure compliance with this Regulation. In those
cases, the f act that an AI system ma y f all within the scope of this Regulation should not affe ct the possibility of
entities car r ying out national secur ity , defence and militar y activities, rega rdless of the type of entity car r ying out
those activities, to use AI systems f or national secur ity , militar y and defe nce pur poses, the use of which is excluded
from the scope of this Regulation. An AI system placed on the marke t f or civilian or law enf orcement pur poses
which is used with or without modifi cation f or militar y , defence or national secur ity pur poses should not fall within
the scope of this Regulation, rega rdless of the type of entity car r ying out those activities.
(25) This Regulation should suppor t inno vation, should respect freedom of science, and should not under mine research
and development activity . It is theref ore necessar y to ex clude from its scope AI syste ms and models specifically
developed and put into ser vice f or the sole pur pose of scientifi c research and development. Moreove r , it is necessar y
t o ensure that this Regulation does not other wise affect scientific research and development activity on AI syste ms or
models pr ior to being placed on the market or put into ser vice. As regards product-or iented research, testing and
development activity rega rding AI syste ms or models, the provisions of this Regulation should also not apply pr ior
t o those syste ms and models being put into ser vice or placed on the marke t. That ex clusion is without prejudice to
the obligation to comp ly with this Regulation where an AI system f alling into the scope of this Regulation is placed
on the marke t or put into ser vice as a result of suc h research and development activity and to the application of
pro visions on AI regulato r y sandbo xe s and te sting in real world conditions. Fur ther more, without prejudice to the
ex clusion of AI syste ms specif ically developed and put into ser vice f or the sole pur pose of scientifi c researc h and
development, any other AI system that ma y be used f or the conduct of any research and development activity should
remain subject t o the provisions of this Regulation. In any event, any research and development activity should be
car r ied out in accordance with recognised ethical and profe ssional standards f or scientific research and should be
conducte d in accordance with applicable Uni on law .
(26) In order to introduce a propor tionate and effe ctive set of binding r ules f or AI syste ms, a clearly defined r isk -based
approac h should be f ollo wed. That approac h should tailor the type and content of suc h r ules t o the inte nsity and
scope of the r isks that AI syste ms can g enerate. It is theref ore necessar y to prohibit cer tain unaccept able AI practices,
t o la y do wn requirements f or high-r isk AI systems and obligati ons f or the relevant operators, and to la y do wn
transparency obligations f or cer tain AI syste ms.
OJ L, 12.7.2024
EN
ELI: http://data.europa.eu/eli/reg/2024/1689/oj 7/144(27) While the r isk -based approac h is the basis f or a propor tionate and effe ctive set of binding r ules, it is imp or tant to
recall the 2019 Ethics guidelines f or tr ustwo r th y AI developed by the independent AI HLEG appoint ed by the
Commission. In those guidelines, the AI HLEG developed seven non-binding ethical pr inciples f or AI which are
inte nded to help ensure that AI is tr ustwor thy and ethically sound. The seven pr inciples include human agency and
o versight ; te chnical robustness and safety; pr ivacy and data gover nance; transparency ; diversity , non-discr imination
and f air ness; societal and envir onmental well-being and accountability . Without prejudice to the leg ally binding
requirements of this Regulation and any other applicable Uni on law , those guidelines contr ibut e to the design of
coherent, tr ustwor thy and human-centr ic AI, in line with the Char te r and with the values on which the Uni on is
f ounded. A ccording to the guidelines of the AI HLEG, human age ncy and oversight means that AI systems are
developed and used as a t ool that ser ves people, respects human dignity and personal autonom y , and that is
functioning in a wa y that can be appropr iat ely controlled and ove rseen by humans. T ec hnical robustness and saf ety
means that AI syste ms are developed and used in a wa y that allows robustness in the case of problems and resilience
ag ainst atte mp ts to alter the use or perform ance of the AI syste m so as to allow unla wful use by third par ties, and
minimise uninte nded har m. Pr ivacy and data gove r nance means that AI syste ms are developed and used in
accordance with pr ivacy and data prot ection r ules, while processing data that meets high standards in ter ms of
quality and inte gr ity . T ransparency means that AI syste ms are developed and used in a wa y that allo ws appropr iate
traceability and explainability , while making humans aw are that they communicate or interact with an AI syste m, as
well as duly inf or ming deplo yers of the capabilities and limitations of that AI system and affe cted persons about their
r ights. Diversity , non-discr imination and fairne ss means that AI syste ms are developed and used in a wa y that
includes diverse actor s and promotes equal access, g ender equality and cultural diversity , while av oiding
discr iminatory imp acts and unfa ir biases that are prohibite d by Union or national law . Social and envir onmental
well-being means that AI systems are developed and used in a sustainable and environmentally fr iendly manner as
well as in a wa y to benefi t all human beings, while monitori ng and assessing the long-term imp acts on the
individual, society and democracy . The application of those pr inciples should be translat ed, when possible, in the
design and use of AI models. They should in any case ser ve as a basis f or the draf ting of codes of conduct under this
Regulation. All stakeholders, including industr y , academia, civil society and standardisation org anisations, are
encourag ed to take into account, as appropr iate, the ethical pr inciples f or the development of voluntary best
practices and standards.
(28) Aside from the many beneficial uses of AI, it can also be misused and provide novel and po werful to ols f or
manipulative, exploitative and social control practices. Such practices are par ticularly har mful and abusive and
should be prohibite d because they contradict Union values of respect f or human dignity , freedom, equality ,
democracy and the r ule of law and fundamental r ights enshr ined in the Char te r , including the r ight to
non-discr imination, to data prot ection and t o pr ivacy and the r ights of the child.
(29) AI-enabled manipulative t echniques can be used to persuade persons to eng age in unwant ed behaviours, or to
deceive them by nudging them into decisions in a wa y that subver ts and imp airs their autonom y , decision-making
and free choices. The placing on the marke t, the putting into ser vice or the use of cer tain AI syste ms with the
objective to or the effe ct of mat er ially distor ting human behavio ur , whereb y signifi cant har ms, in par ticular having
sufficiently imp or tant adverse im pacts on phys ical, psy chological health or financ ial intere sts are like ly to occur , are
par ticularly dangerous and should theref ore be prohibite d. Such AI syste ms deplo y subliminal comp onents suc h as
audio, image, video stimuli that persons cannot perceive, as those stimuli are bey ond human percep tion, or other
manipulative or decept ive tec hniques that subver t or imp air person’s auto nomy , decision-making or free ch oice in
wa ys that people are not consciously aw are of those te chniques or , where they are aw are of them, can still be
deceived or are not able to control or resist them. This could be f acilitated, f or example, by machine-brain inte rfaces
or vir tual reality as they allow f or a higher degree of control of what stimuli are presented t o persons, insofa r as they
ma y materially distor t their behavio ur in a significantly har mful manner . In addition, AI syste ms ma y also other wise
exploit the vulnerabilities of a person or a specif ic group of persons due to their age , disability within the meaning of
Directive (EU) 2019/882 of the European Pa rliament and of the Council (
16
), or a specific social or economic
situation that is likely to mak e those persons more vulnerable t o exploitation suc h as persons living in extreme
po ver ty , ethnic or religious minor ities. Such AI syste ms can be placed on the marke t, put into ser vice or used with
the objective to or the effe ct of materially distor ting the behavi our of a person and in a manner that causes or is
reasonably like ly to cause signifi cant har m to that or another person or groups of persons, including har ms that ma y
be accumulated over time and should theref ore be prohibite d. It ma y not be possible to assume that there is an
EN
OJ L, 12.7.2024
8/144 ELI: http://data.europa.eu/eli/reg/2024/1689/oj
(
16
) Directive (EU) 2019/882 of the European Parliament and of the Council of 17 Apr il 2019 on the accessibility requirements f or
products and ser vices (OJ L 151, 7.6.2019, p. 70).inte ntion to distor t behavio ur where the distor tion results from f actors exter nal to the AI syste m which are outside
the control of the provid er or the deplo y er , namely f actors that ma y not be reasonably f oreseeable and theref ore not
possible f or the provider or the deplo y er of the AI syste m to mitigat e. In any case, it is not necessar y f or the provid er
or the deplo y er t o hav e the inte ntion t o cause signifi cant har m, provided that such har m results from the
manipulative or exploitative AI-enabled practices. The prohibitions f or suc h AI practices are complement ar y to the
pro visions contained in Directive 2005/29/EC of the European Parliame nt and of the Council (
17
), in par ticular unfa ir
commercial practices leading to economic or financial har ms t o consumers are prohibited under all circumstances,
ir respective of whether they are put in place through AI systems or other wise. The prohibitions of manipulative and
exploitative practices in this Regulation should not affe ct lawful practices in the context of medical treatment such as
psy chologi cal treatment of a mental disease or phys ical rehabilitation, when those practices are car r ied out in
accordance with the applicable la w and medical standards, f or exam ple explicit consent of the individuals or their
leg al representatives. In addition, common and legitimate commercial practices, f or example in the fi eld of
adver tising, that comply with the applicable law should not, in themselves, be rega rded as constituting har mful
manipulative AI-enabled practices.
(30) Biometr ic categor isation syste ms that are based on natural persons’ biometr ic data, such as an individual person’s
f ace or finger pr int, to deduce or inf er an individuals ’ political opinions, trade union membership, religious or
philosophical beliefs, race, sex life or sexual or ientation should be prohibite d. That prohibition should not cover the
la wful labelling, filter ing or categori sation of biometr ic data sets acquired in line with Uni on or national la w
according to biometr ic data, suc h as the sor ting of imag es according t o hair colour or e ye colour , which can f or
exam ple be used in the area of law enf orcement.
(31) AI systems provid ing social scor ing of natural persons b y public or pr ivat e actors ma y lead to discr iminat or y
outcomes and the ex clusion of cer tain groups. They ma y violate the r ight to dignity and non-discr imination and the
values of equality and justice. Such AI syste ms evaluate or classify natural persons or groups thereof on the basis of
multiple data points related to their social behavio ur in multiple contexts or known, inf er red or predicted personal
or personality ch aracter istics ove r cer tain per iods of time. The social score obtained from suc h AI systems ma y lead
t o the detr imental or unfa v ourable treatment of natural persons or whole groups thereof in social cont exts, which
are unrelate d to the context in which the data was or iginally generat ed or collect ed or t o a detr imental treatment that
is dispropor tionate or unjustifie d to the gra vity of their social behavio ur . AI systems entailing suc h unaccept able
scor ing practices and leading to suc h detr imental or unfa v ourable outcomes should theref ore be prohibited. That
prohibition should not affe ct lawful evaluation practices of natural persons that are car r ied out f or a specific pur pose
in accordance with Uni on and national law .
(32) The use of AI syste ms f or ‘real-time’ remote biometr ic identification of natural persons in publicly accessible spaces
f or the pur pose of law enf orcement is par ticularly intr usive to the r ights and freedoms of the concer ned persons, to
the extent that it ma y affect the pr ivate life of a large par t of the population, evok e a f eeling of constant sur veillance
and indirectly dissuade the exercise of the freedom of assembly and other fundamental r ights. T echnical inaccuracies
of AI syste ms intende d f or the remote biometr ic identifi cation of natural persons can lead to biased results and entail
discr iminatory eff ects. Such possible biased results and discr iminatory eff ects are par ticularly relevant with regard to
ag e, ethnicity , race, sex or disabilities. In addition, the immediacy of the impact and the limited oppor tunities f or
fur ther chec ks or cor rections in relation to the use of suc h systems operating in real-time car r y height ened r isks f or
the r ights and freedoms of the persons concer ned in the cont ext of, or im pacted by , law enf orcement activities.
(33) The use of those systems f or the pur pose of law enf orcement should theref ore be prohibited, ex cept in exhaustively
list ed and nar ro wly defined situations, where the use is str ictly necessar y to ac hieve a substantial public interest, the
im por tance of which outweighs the r isks . Those situations inv olve the search f or cer tain victims of cr ime including
missing persons; cer tain threats to the life or to the phys ical safety of natural persons or of a ter ror ist attac k; and the
localisation or identification of per petrat ors or suspects of the cr iminal offences listed in an annex to this Regulation,
where those cr iminal offe nces are punishable in the Member Stat e concer ned by a custo dial sentence or a detention
OJ L, 12.7.2024
EN
ELI: http://data.europa.eu/eli/reg/2024/1689/oj 9/144
(
17
) Directive 2005/29/EC of the European Parl iament and of the Council of 11 Ma y 2005 concer ning unf air business-to-consumer
commercial practices in the inter nal market and amending Council Directive 84/450/EEC, Directives 97/7/EC, 98/27/EC and
2002/65/EC of the European Parl iament and of the Council and Regulation (EC) No 2006/2004 of the European Parl iament and of
the Council (‘Unf air Commercial Practices Directive’) (OJ L 149, 11.6.2005, p. 22).order f or a maximum per iod of at least f our y ears and as they are defined in the law of that Member Stat e. Such
a threshold f or the custo dial sent ence or detention order in accordance with national la w contr ibut es to ensur ing
that the offence should be ser ious enough to potentially justify the use of ‘real-time’ remot e biometr ic identification
syste ms. Moreover , the list of cr iminal offe nces provided in an annex to this Regulation is based on the 32 cr iminal
offe nces listed in the Council Framework Decision 2002/584/JHA (
18
), taking into account that some of those
offe nces are, in practice, likel y to be more relevant than others, in that the recourse to ‘real-time’ remote biometr ic
identification could, f oreseeably , be necessar y and propor tionate to highly var ying degrees f or the practical pursuit
of the localisation or identification of a per petrat or or suspect of the diff erent cr iminal offences listed and having
rega rd to the like ly differences in the ser iousness, probability and scale of the har m or possible negati ve
consequences. An imminent threat to life or the physical safety of natural persons could also result from a ser ious
disr up tion of cr itical infrastr ucture, as def ined in Ar ticle 2, point (4) of Directive (EU) 2022/2557 of the European
P arliament and of the Council (
19
), where the disr up tion or destr uction of such cr itical infrastr ucture wo uld result in
an imminent threat t o life or the physical safety of a person, including through ser ious har m to the provision of basic
supplies to the population or to the ex ercise of the core function of the State. In addition, this Regulation should
preser ve the ability f or law enf orcement, border control, immigration or asylum author ities to car r y out identity
ch ec k s in the presence of the person concer ned in accordance with the conditions set out in Union and national law
f or such chec ks. In par ticular , law enf orcement, border control, immigration or asylum author ities should be able to
use inf or mation syste ms, in accordance with Union or national law , to identify persons who, dur ing an identity
ch ec k , either refuse to be identified or are unable to stat e or prove their identity , without being required b y this
Regulation t o obtain pr ior author isation. This could be, f or exam ple, a person inv olved in a cr ime, being unwilling,
or unable due to an accident or a medical condition, to disclose their identity t o law enf orcement author ities.
(34) In order to ensure that those syste ms are used in a responsible and propor tionate manner , it is also impor tant to
establish that, in each of those exhaustively list ed and nar ro wly def ined situations, cer tain elements should be take n
into account, in par ticular as regards the nature of the situation giving r ise to the request and the consequences of
the use f or the r ights and freedoms of all persons concer ned and the saf eguards and conditions provid ed f or with the
use. In addition, the use of ‘real-time’ remot e biometr ic identifi cation systems in publicly accessible spaces f or the
pur pose of la w enf orcement should be deplo y ed only to confir m the specif ically targe ted individual’s identity and
should be limit ed to what is str ictly necessar y concer ning the per iod of time, as well as the g eographic and personal
scope, hav ing rega rd in par ticular to the evidence or indications rega rding the threats, the victims or per petrat or . The
use of the real-time remote biometr ic identifica tion system in publicly accessible spaces should be author ised only if
the relevant law enf orcement author ity has comp let ed a fundamental r ights im pact assessment and, unless provid ed
other wise in this Regulation, has register ed the syste m in the database as set out in this Regulation. The reference
database of persons should be appropr iate f or each use case in each of the situations mentioned above.
(35) Each use of a ‘real-time’ remote biometr ic identification system in publicly accessible spaces f or the pur pose of law
enf orcement should be subject to an express and specif ic author isation by a judicial author ity or by an independent
administrative author ity of a Member State whose decision is binding. Such author isation should, in pr inciple, be
obtained pr ior to the use of the AI system with a view to identifying a person or persons. Ex ceptions to that r ule
should be allowed in duly justifi ed situations on grounds of urg ency , namely in situations where the need to use the
syste ms concer ned is suc h as to mak e it effectively and objectively impossible to obtain an author isation bef ore
commencing the use of the AI system. In suc h situations of urg ency , the use of the AI syste m should be restr icted to
the absolute minimum necessar y and should be subject to appropr iate safeguards and conditions, as det er mined in
national law and specif ied in the cont ext of each individual urgent use case by the law enf orcement author ity itself.
In addition, the law enf orcement author ity should in such situations request suc h author isation while provid ing the
reasons f or not hav ing been able t o request it earlier , without undue dela y and at the lat est within 24 hours. If such
an author isation is rejected, the use of real-time biometr ic identification systems linked to that author isation should
cease with immediate effect and all the data related to suc h use should be discarded and delete d. Such data includes
EN
OJ L, 12.7.2024
10/144 ELI: http://data.europa.eu/eli/reg/2024/1689/oj
(
18
) Council Framework Decision 2002/584/JHA of 13 June 2002 on the European ar rest war rant and the sur render procedures
between Member States (OJ L 190, 18.7.2002, p. 1).
(
19
) Directive (EU) 2022/2557 of the European Parl iament and of the Council of 14 December 2022 on the resilience of cr itical entities
and repealing Council Directive 2008/114/EC (OJ L 333, 27.12.2022, p. 164).in put data directly acquired b y an AI syste m in the course of the use of such system as well as the results and outputs
of the use linked to that author isation. It should not include in put that is lega lly acquired in accordance with another
Uni on or national law . In any case, no decision producing an adverse lega l effe ct on a person should be taken based
solely on the output of the remot e biometr ic identification system.
(36) In order to car r y out their tasks in accordance with the requirements set out in this Regulation as well as in national
r ules, the relevant marke t sur veillance author ity and the national data prot ection author ity should be notif ied of each
use of the real-time biometr ic identification system. Market sur veillance author ities and the national data prot ection
author ities that hav e been notif ied should submit t o the Commission an annual repor t on the use of real-time
biometr ic identifi cation syste ms.
(37) Fur ther more, it is appropr iate to provid e, within the exhaustive framework set by this Regulation that suc h use in
the te r r itory of a Member Stat e in accordance with this Regulation should only be possible where and in as far as the
Member State concer ned has decided t o expressly pro vide f or the possibility to author ise such use in its detailed r ules
of national law . Consequently , Member Stat es remain free under this Regulation not to provide f or suc h a possibility
at all or to only provid e f or such a possibility in respect of some of the objectives capable of justifying author ised use
identified in this Regulation. Such national r ules should be notified to the Commission within 30 da ys of their
adop tion.
(38) The use of AI syste ms f or real-time remote biometr ic identification of natural persons in publicly accessible spaces
f or the pur pose of law enf orcement necessar ily inv olves the processing of biometr ic data. The r ules of this
Regulation that prohibit, subject t o cer tain ex ceptions, suc h use, which are based on Ar ticle 16 TFEU, should apply
as lex specialis in respect of the r ules on the processing of biometr ic data contained in Ar ticle 10 of Directive (EU)
2016/680, thus regulating such use and the processing of biometr ic data inv olved in an exhaustive manner .
Theref ore, suc h use and processing should be possible only in as f ar as it is compatible with the framework set by
this Regulation, without there being scope, outside that framework, f or the comp etent author ities, where they act f or
pur pose of law enf orcement, to use suc h syste ms and process suc h data in connection thereto on the grounds list ed
in Ar ticle 10 of Directive (EU) 2016/680. In that cont ext, this Regulation is not intended to provide the leg al basis
f or the processing of personal data under Ar ticle 8 of Directive (EU) 2016/680. Ho wever , the use of real-time remote
biometr ic identifica tion syste ms in publicly accessible spaces f or pur poses other than law enf orcement, including by
com petent author ities, should not be covered b y the specif ic framew ork regard ing suc h use f or the pur pose of law
enf orcement set by this Regulation. Such use f or pur poses other than law enf orcement should theref ore not be
subject t o the requirement of an author isation under this Regulation and the applicable detailed r ules of national law
that ma y give effect to that author isation.
(39) Any processing of biometr ic data and other personal data in volved in the use of AI syste ms f or biometr ic
identification, other than in connection to the use of real-time remote biometr ic identification syste ms in publicly
accessible spaces f or the pur pose of law enf orcement as regulated by this Regulation, should continue to comply
with all requirements resulting from Ar ticle 10 of Directive (EU) 2016/680. For pur poses other than la w
enf orcement, Ar ticle 9(1) of Regulation (EU) 2016/679 and Ar ticle 10(1) of Regulation (EU) 2018/1725 prohibit the
processing of biometr ic data subject to limit ed ex ceptions as provided in those Ar ticles. In the application of Ar ticle
9(1) of Regulation (EU) 2016/679, the use of remot e biometr ic identifi cation f or pur poses other than la w
enf orcement has already been subject to prohibition decisions by national data protection author ities.
(40) In accordance with Ar ticle 6a of Protocol No 21 on the position of the Uni ted Kingdom and Ireland in respect of the
area of freedom, secur ity and justice, as annexe d to the TEU and t o the TFEU, Ireland is not bound by the r ules laid
do wn in Ar ticle 5(1), first subparagraph, point (g), to the exte nt it applies t o the use of biometr ic categor isation
syste ms f or activities in the field of police cooperation and judicial cooperation in cr iminal matt ers, Ar ticle 5(1), fi rst
subparagraph, point (d), to the exte nt it applies to the use of AI syste ms covered by that provision, Ar ticle 5(1), fi rst
subparagraph, point (h), Ar ticle 5(2) to (6) and Ar ticle 26(10) of this Regulation adop ted on the basis of Ar ticle 16
TFEU which relate to the processing of personal data by the Member Stat es when car r ying out activities f alling
within the scope of Chapt er 4 or Chapt er 5 of Title V of Pa r t Three of the TFEU, where Ireland is not bound by the
r ules gover ning the f or ms of judicial cooperation in cr iminal matt ers or police cooperation which require
com pliance with the provisions laid do wn on the basis of Ar ticle 16 TFEU.
(41) In accordance with Ar ticles 2 and 2a of Proto col No 22 on the position of Denmark, annexe d to the TEU and t o the
TFEU, Denmark is not bound by r ules laid down in Ar ticle 5(1), first subparagraph, point (g), to the exte nt it applies
t o the use of biometr ic categor isation syste ms f or activities in the fi eld of police cooperation and judicial cooperation
in cr iminal matt ers, Ar ticle 5(1), first subparagraph, point (d), to the extent it applies t o the use of AI syste ms
co vered b y that provision, Ar ticle 5(1), fir st subparagraph, point (h), (2) to (6) and Ar ticle 26(10) of this Regulation
OJ L, 12.7.2024
EN
ELI: http://data.europa.eu/eli/reg/2024/1689/oj 11/144adop te d on the basis of Ar ticle 16 TFEU, or subject to their application, which relate to the processing of personal
data by the Member States when car r ying out activities f alling within the scope of Chap ter 4 or Chapt er 5 of
Title V of Par t Three of the TFEU.
(42) In line with the presump tion of innocence, natural persons in the Uni on should alwa ys be judged on their actual
behavi our . Natural persons should never be judg ed on AI-predicted behavio ur based solely on their prof iling,
personality traits or char acter istics, such as nationality , place of bir th, place of residence, number of children, level of
debt or type of car , without a reasonable suspicion of that person being in volved in a cr iminal activity based on
objective ver ifi able f acts and without human assessment thereof. Theref ore, r isk assessments car r ied out with rega rd
t o natural persons in order to assess the likelihood of their offending or to predict the occur rence of an actual or
pote ntial cr iminal offence based solely on prof iling them or on assessing their personality traits and ch aracter istics
should be prohibited. In any case, that prohibition does not refer to or tou ch upon r isk analytics that are not based
on the profil ing of individuals or on the personality traits and charact er istics of individuals, suc h as AI systems using
r isk analytics t o assess the like lihood of financ ial fraud by under takings on the basis of suspicious transactions or
r isk analytic to ols to predict the likelihood of the localisation of narcotics or illicit goods by customs author ities, f or
exam ple on the basis of known traff ic king routes.
(43) The placing on the mark et, the putting into ser vice f or that specific pur pose, or the use of AI syste ms that create or
expand f acial recognition databases through the untarg eted scraping of f acial images from the inter net or CCT V
f ootage, should be prohibite d because that practice adds to the f eeling of mass sur veillance and can lead to gross
violations of fundamental r ights, including the r ight to pr ivacy .
(44) There are ser ious concer ns about the scientific basis of AI systems aiming to identify or infer emotions, par ticularly
as expression of emotions var y considerably across cultures and situations, and even within a sing le individual.
Among the ke y shor tcomings of such syste ms are the limited reliability , the lac k of specificity and the limit ed
g eneralisability . Theref ore, AI systems identifying or inferr ing emotions or intentions of natural persons on the basis
of their biometr ic data ma y lead t o discr iminat or y outcomes and can be intr usive to the r ights and freedoms of the
concer ned persons. Consider ing the imbalance of power in the cont ext of w ork or education, combined with the
intr usive nature of these syste ms, suc h systems could lead t o detr imental or unfa v ourable treatment of cer tain
natural persons or whole groups thereof. Theref ore, the placing on the marke t, the putting into ser vice, or the use of
AI syste ms inte nded to be used to detect the emotional state of individuals in situations related to the w orkplace and
education should be prohibite d. That prohibition should not cover AI systems placed on the mark et str ictly f or
medical or saf ety reasons, suc h as systems inte nded f or therapeutical use.
(45) Practices that are prohibited by Uni on law , including data prot ection law , non-discr imination law , consumer
prot ection law , and competition law , should not be affe cted by this Regulation.
(46) High-r isk AI systems should only be placed on the Uni on marke t, put into ser vice or used if the y comply with
cer tain mandato r y requirements. Those requirements should ensure that high-r isk AI systems available in the Uni on
or whose output is other wise used in the Union do not pose unaccepta ble r isks t o im por tant Uni on public inte rests
as recognised and protect ed by Uni on law . On the basis of the New Legislative Framework, as clar ified in the
Commission notice ‘ The “Blue Guide” on the imp lementation of EU product r ules 2022’ (
20
), the general r ule is that
more than one lega l act of Uni on har monisation legislation, suc h as Regulations (EU) 2017/745 (
21
) and (EU)
2017/746 (
22
) of the European Parliament and of the Council or Directive 2006/42/EC of the European Pa rliament
and of the Council (
23
), ma y be applicable to one product, since the making availa ble or putting into ser vice can take
EN
OJ L, 12.7.2024
12/144 ELI: http://data.europa.eu/eli/reg/2024/1689/oj
(
20
) OJ C 247, 29.6.2022, p. 1.
(
21
) Regulation (EU) 2017/745 of the European Parliament and of the Council of 5 Apr il 2017 on medical devices, amending Directive
2001/83/EC, Regulation (EC) No 178/2002 and Regulation (EC) No 1223/2009 and repealing Council Directives 90/385/EEC and
93/42/EEC (OJ L 117, 5.5.2017, p. 1).
(
22
) Regulation (EU) 2017/746 of the European Parliament and of the Council of 5 Apr il 2017 on in vitro diagnostic medical devices and
repealing Directive 98/79/EC and Commission Decision 2010/227/EU (OJ L 117, 5.5.2017, p. 176).
(
23
) Directive 2006/42/EC of the European Parl iament and of the Council of 17 Ma y 2006 on machiner y , and amending Directive
95/16/EC (OJ L 157, 9.6.2006, p. 24).place only when the product comp lies with all applicable Uni on har monisation legislation. T o ensure consiste ncy
and av oid unnecessar y administrative burdens or costs, provid ers of a product that contains one or more high-r isk
AI syste ms, to which the requirements of this Regulation and of the Union har monisation legislation listed in an
annex t o this Regulation apply , should hav e f lexibility with regard to operational decisions on ho w t o ensure
com pliance of a product that contains one or more AI syste ms with all applicable requirements of the Uni on
har monisation legislation in an optima l manner . AI syste ms identified as high-r isk should be limit ed to those that
hav e a significant har mful impact on the health, saf ety and fundamental r ights of persons in the Uni on and such
limitation should minimise any pote ntial restr iction to inte r national trade.
(47) AI syste ms could have an adverse impact on the health and saf ety of persons, in par ticular when suc h syste ms
operat e as safety comp onents of products. Consiste nt with the objectives of Union har monisation legislation to
f acilitate the free move ment of products in the inter nal market and to ensure that only saf e and other wise comp liant
products fi nd their wa y into the market, it is imp or tant that the safety r isks that ma y be g enerated by a product as
a whole due to its digital com ponents, including AI syste ms, are duly prevented and mitigat ed. For instance,
increasing ly autono mous robots, whether in the cont ext of manuf actur ing or personal assistance and care should be
able to safely operate and perfo r ms their functions in complex environments. Similarly , in the health secto r where
the stakes f or lif e and health are par ticularly high, increasing ly sophisticat ed diagnostics systems and syste ms
suppor ting human decisions should be reliable and accurate.
(48) The extent of the adverse imp act caused b y the AI syste m on the fundamental r ights protect ed b y the Char ter is of
par ticular relevance when classifying an AI syste m as high r isk. Those r ights include the r ight to human dignity ,
respect f or pr ivate and f amily life, protection of personal data, freedom of expression and inf or mation, freedom of
assembly and of association, the r ight to non-discr imination, the r ight to education, consumer protection, work ers’
r ights, the r ights of persons with disabilities, gender equality , inte llectual proper ty r ights, the r ight t o an eff ective
remedy and to a f air tr ial, the r ight of defence and the presump tion of innocence, and the r ight to good
administration. In addition to those r ights, it is impor tant t o highlight the f act that ch ildren hav e specif ic r ights as
enshr ined in Ar ticle 24 of the Char te r and in the Unit ed Nations Convention on the Rights of the Child, fur ther
developed in the UNCR C General Comment No 25 as regard s the digital en vironment, both of which require
consideration of the ch ildren’s vulnerabilities and provision of suc h protection and care as necessar y f or their
well-being. The fundamental r ight to a high level of envir onmental protection enshr ined in the Char te r and
im plemented in Union policies should also be considered when assessing the sever ity of the har m that an AI system
can cause, including in relation to the health and safety of persons.
(49) As regard s high-r isk AI systems that are safety com ponents of products or syste ms, or which are themselves
products or systems f alling within the scope of Regulation (EC) No 300/2008 of the European Parliame nt and of the
Council (
24
), Regulation (EU) No 167/2013 of the European Parliame nt and of the Council (
25
), Regulation
(EU) No 168/2013 of the European Parliament and of the Council (
26
), Directive 2014/90/EU of the European
P arliament and of the Council (
27
), Directive (EU) 2016/797 of the European Pa rliament and of the Council (
28
),
Regulation (EU) 2018/858 of the European Pa rliament and of the Council (
29
), Regulation (EU) 2018/1139 of the
OJ L, 12.7.2024
EN
ELI: http://data.europa.eu/eli/reg/2024/1689/oj 13/144
(
24
) Regulation (EC) No 300/2008 of the European Parliament and of the Council of 11 March 2008 on common r ules in the f ield of
civil aviation secur ity and repealing Regulation (EC) No 2320/2002 (OJ L 97, 9.4.2008, p. 72).
(
25
) Regulation (EU) No 167/2013 of the European Parl iament and of the Council of 5 Febr uar y 2013 on the approv al and market
sur veillance of agr icultural and f orestr y vehicles (OJ L 60, 2.3.2013, p. 1).
(
26
) Regulation (EU) No 168/2013 of the European Parl iament and of the Council of 15 Januar y 2013 on the approval and market
sur veillance of two- or three-wheel vehicles and quadr icy cles (OJ L 60, 2.3.2013, p. 52).
(
27
) Directive 2014/90/EU of the European Parliament and of the Council of 23 July 2014 on mar ine equipment and repealing Council
Directive 96/98/EC (OJ L 257, 28.8.2014, p. 146).
(
28
) Directive (EU) 2016/797 of the European Parliament and of the Council of 11 Ma y 2016 on the int eroperability of the rail system
within the European Union (OJ L 138, 26.5.2016, p. 44).
(
29
) Regulation (EU) 2018/858 of the European Parl iament and of the Council of 30 Ma y 2018 on the approval and market sur veillance
of motor vehicles and their trailers, and of systems, componen ts and separate tec hnical units intended f or suc h vehicles, amending
Regulations (EC) No 715/2007 and (EC) No 595/2009 and repealing Directive 2007/46/EC (OJ L 151, 14.6.2018, p. 1).European P arliament and of the Council (
30
), and Regulation (EU) 2019/2144 of the European Parliament and of the
Council (
31
), it is appropr iate to amend those acts t o ensure that the Commission tak es into account, on the basis of
the te chnical and regulato r y specif icities of each secto r , and without interfer ing with existing gover nance, conf or mity
assessment and enf orcement mechanisms and author ities established therein, the mandato r y requirements f or
high-r isk AI systems laid do wn in this Regulation when adopti ng any relevant deleg ated or imp lementing acts on the
basis of those acts.
(50) As regards AI syste ms that are safety comp onents of products, or which are themselves products, f alling within the
scope of cer tain Union har monisation legislation list ed in an annex to this Regulation, it is appropr iat e to classify
them as high-r isk under this Regulation if the product concer ned undergoes the conf or mity assessment procedure
with a third-par ty conf or mity assessment body pursuant to that relevant Uni on har monisation legislation. In
par ticular , suc h products are machi ner y , t o ys, lif ts, equipment and prot ective systems intended f or use in pote ntially
explosive atmospheres, radio equipment, pressure equipment, recreational craf t equipment, cablewa y installations,
appliances bur ning g aseous fuels, medical devices, in vitro diagnostic medical devices, automot ive and aviation.
(51) The classif ication of an AI syste m as high-r isk pursuant t o this Regulation should not necessar ily mean that the
product whose saf ety component is the AI system, or the AI system itself as a product, is considered to be high-r isk
under the cr iteria established in the relevant Uni on har monisation legislation that applies to the product. This is, in
par ticular , the case f or Regulations (EU) 2017/745 and (EU) 2017/746, where a third-par ty conf or mity assessment is
pro vided f or medium-r isk and high-r isk products.
(52) As rega rds stand-alone AI syste ms, namely high-r isk AI syste ms other than those that are saf ety com ponents of
products, or that are themselves products, it is appropr iat e to classify them as high-r isk if, in light of their intended
pur pose, the y pose a high r isk of har m t o the health and safety or the fundamental r ights of persons, taking into
account both the sever ity of the possible har m and its probability of occur rence and they are used in a number of
specifically pre-defi ned areas specified in this Regulation. The identification of those systems is based on the same
methodology and cr ite r ia envisag ed also f or any future amendments of the list of high-r isk AI syste ms that the
Commission should be empo wered to adop t, via delegat ed acts, to take into account the rapid pace of te chnological
development, as well as the potential ch ange s in the use of AI systems.
(53) It is also impor tant to clar ify that there ma y be specific cases in which AI syste ms refer red to in pre-define d areas
specified in this Regulation do not lead to a signifi cant r isk of har m to the lega l intere sts protect ed under those areas
because they do not materi ally inf luence the decision-making or do not har m those interests substantially . For the
pur poses of this Regulation, an AI system that does not mat er ially inf luence the outcome of decision-making should
be understood to be an AI syste m that does not have an im pact on the substance, and thereby the outcome, of
decision-making, whether human or automa ted. An AI system that does not materially inf luence the outcome of
decision-making could include situations in which one or more of the f ollo wing conditions are fulfilled. The fir st
suc h condition should be that the AI syste m is intende d t o perfo r m a nar row procedural task, suc h as an AI system
that transf or ms unstr uctured data into str uctured data, an AI system that classif ies incoming documents into
cate gor ies or an AI syste m that is used to det ect duplicate s among a large number of applications. Those tasks are of
suc h nar row and limited nature that they pose only limited r isks which are not increased through the use of an AI
syste m in a cont ext that is list ed as a high-r isk use in an annex to this Regulation. The second condition should be
EN
OJ L, 12.7.2024
14/144 ELI: http://data.europa.eu/eli/reg/2024/1689/oj
(
30
) Regulation (EU) 2018/1139 of the European Parliament and of the Council of 4 July 2018 on common r ules in the f ield of civil
av iation and establishing a European Union A viation Safety Ag ency , and amending Regulations (EC) No 2111/2005, (EC)
No 1008/2008, (EU) No 996/2010, (EU) No 376/2014 and Directives 2014/30/EU and 2014/53/EU of the European Parl iament
and of the Council, and repealing Regulations (EC) No 552/2004 and (EC) No 216/2008 of the European Parl iament and of the
Council and Council Regulation (EEC) No 3922/91 (OJ L 212, 22.8.2018, p. 1).
(
31
) Regulation (EU) 2019/2144 of the European Parliament and of the Council of 27 November 2019 on type-approval requirements
f or motor vehicles and their trailers, and systems, components and separate tec hnical units int ended f or such vehicles, as reg ards
their ge neral safety and the protection of vehicle occupants and vulnerable road users, amending Regulation (EU) 2018/858 of the
European Parl iament and of the Council and repealing Regulations (EC) No 78/2009, (EC) No 79/2009 and (EC) No 661/2009 of
the European Parl iament and of the Council and Commission Regulations (EC) No 631/2009, (EU) No 406/2010, (EU)
No 672/2010, (EU) No 1003/2010, (EU) No 1005/2010, (EU) No 1008/2010, (EU) No 1009/2010, (EU) No 19/2011, (EU)
No 109/2011, (EU) No 458/2011, (EU) No 65/2012, (EU) No 130/2012, (EU) No 347/2012, (EU) No 351/2012, (EU)
No 1230/2012 and (EU) 2015/166 (OJ L 325, 16.12.2019, p. 1).that the task perfo r med by the AI system is intended to imp ro ve the result of a previously comp leted human activity
that ma y be relevant f or the pur poses of the high-r isk uses list ed in an annex to this Regulation. Consider ing those
ch aracteristics , the AI system provid es only an additional la yer to a human activity with consequently lower ed r isk .
That condition w ould, f or exam ple, apply t o AI syste ms that are intended to impro ve the language used in previously
draf ted documents, f or exam ple in relation to profe ssional tone, academic style of languag e or by aligning te xt to
a cer tain brand messaging. The third condition should be that the AI syste m is intended to det ect decision-making
patt er ns or deviations from pr ior decision-making patterns . The r isk wo uld be lower ed because the use of the AI
syste m f ollo ws a previously complet ed human assessment which it is not meant to replace or inf luence, without
proper human review . Such AI syste ms include f or instance those that, given a cer tain grading patt er n of a teac her ,
can be used to ch eck ex post whether the t eacher ma y hav e deviated from the grading pattern so as to f lag potent ial
inconsiste ncies or anomalies. The f our th condition should be that the AI syste m is intended to perf or m a task that is
only preparat or y to an assessment relevant f or the pur poses of the AI systems listed in an annex t o this Regulation,
thus making the possible imp act of the output of the system ver y low in t er ms of representing a r isk f or the
assessment to f ollow . That condition covers, inte r alia, smar t solutions f or file handling, which include var ious
functions from indexing, search ing, te xt and speech processing or linking data t o other data sources, or AI syste ms
used f or translation of initial documents. In any case, AI systems used in high-r isk use-cases list ed in an annex to this
Regulation should be considered to pose signifi cant r isks of har m to the health, safety or fundamental r ights if the AI
syste m imp lies profiling within the meaning of Ar ticle 4, point (4) of Regulation (EU) 2016/679 or Ar ticle 3,
point (4) of Directive (EU) 2016/680 or Ar ticle 3, point (5) of Regulation (EU) 2018/1725. T o ensure traceability
and transparency , a provid er who considers that an AI syste m is not high-r isk on the basis of the conditions refe r red
t o above should dra w up documentation of the assessment bef ore that syste m is placed on the marke t or put into
ser vice and should provide that documentation to national compet ent author ities upon request. Such a provid er
should be obliged to register the AI syste m in the EU database established under this Regulation. With a view to
pro viding fur ther guidance f or the practical imp lementation of the conditions under which the AI syste ms listed in
an annex to this Regulation are, on an excep tional basis, non-high-r isk, the Commission should, af te r consulting the
Board, provid e guidelines specifying that practical implementation, comp let ed b y a compre hensive list of practical
exam ples of use cases of AI syste ms that are high-r isk and use cases that are not.
(54) As biometr ic data constitute s a special categor y of personal data, it is appropr iat e to classify as high-r isk several
cr itical-use cases of biometr ic syste ms, insofar as their use is per mitte d under relevant Uni on and national law .
T ec hnical inaccuracies of AI systems intende d f or the remote biometr ic identifica tion of natural persons can lead to
biased results and entail discr iminat or y eff ects. The r isk of suc h biased results and discr iminat or y effects is
par ticularly relevant with regard to age , ethnicity , race, sex or disabilities. Remote biometr ic identifica tion syste ms
should theref ore be classif ied as high-r isk in view of the r isks that they pose. Such a classif ication excludes AI
syste ms inte nded to be used f or biometr ic ver ifi cation, including authentication, the sole pur pose of which is to
confir m that a specif ic natural person is who that person claims t o be and to confir m the identity of a natural person
f or the sole pur pose of hav ing access to a ser vice, unlocking a device or having secure access to premises. In addition,
AI syste ms intended to be used f or biometr ic categor isation according to sensitive attr ibut es or ch aracter istics
prot ected under Ar ticle 9(1) of Regulation (EU) 2016/679 on the basis of biometr ic data, in so far as these are not
prohibite d under this Regulation, and emotion recognition systems that are not prohibite d under this Regulation,
should be classif ied as high-r isk. Biometr ic syste ms which are intende d t o be used solely f or the pur pose of enabling
cybersecur ity and personal data prot ection measures should not be considered to be high-r isk AI syste ms.
(55) As regards the management and operation of cr itical infrastr ucture, it is appropr iate to classify as high-r isk the AI
syste ms inte nded to be used as safety components in the managem ent and operation of cr itical digital infrastr ucture
as list ed in point (8) of the Annex to Directive (EU) 2022/2557, road traffic and the supply of wate r , g as, heating and
electr icity , since their f ailure or malfunctioning ma y put at r isk the life and health of persons at larg e scale and lead t o
appreciable disr up tions in the ordinar y conduct of social and economic activities. Saf ety compo nents of cr itical
infrastr ucture, including cr itical digital infrastr ucture, are systems used to directly protect the physica l inte gr ity of
cr itical infrastr ucture or the health and safety of persons and proper ty but which are not necessar y in order f or the
OJ L, 12.7.2024
EN
ELI: http://data.europa.eu/eli/reg/2024/1689/oj 15/144syste m to function. The f ailure or malfunctioning of suc h comp onents might directly lead to r isks to the phys ical
inte gr ity of cr itical infrastr ucture and thus t o r isks to health and safety of persons and proper ty . Compo nents
inte nded to be used solely f or cybersecur ity pur poses should not qualify as safety compo nents. Examples of saf ety
com ponents of suc h cr itical infrastr ucture ma y include syste ms f or monitoring wate r pressure or fire alar m
controlling systems in cloud comp uting centres.
(56) The deplo yment of AI systems in education is impor tant to promot e high-quality digital education and training and
t o allo w all lear ners and te ache rs to acquire and share the necessar y digital skills and comp etences, including media
lite racy , and cr itical thinking, to take an active par t in the economy , society , and in democratic processes. How ever ,
AI systems used in education or v ocational training, in par ticular f or det er mining access or admission, f or assigning
persons to educational and v ocational training institutions or programmes at all levels, f or evaluating lear ning
outcomes of persons, f or assessing the appropr iate level of education f or an individual and mat er ially inf luencing the
level of education and training that individuals will receive or will be able to access or f or monitoring and detecting
prohibite d behaviour of students dur ing tests should be classif ied as high-r isk AI systems, since the y ma y determi ne
the educational and profe ssional course of a person’s life and theref ore ma y affe ct that person’s ability to secure
a livelihood. When improperly designed and used, suc h syste ms ma y be par ticularly intr usive and ma y violate the
r ight to education and training as well as the r ight not to be discr iminated aga inst and per petuate histor ical patt er ns
of discr imination, f or exam ple ag ainst women, cer tain age groups, persons with disabilities, or persons of cer tain
racial or ethnic or igins or sexual or ientation.
(57) AI systems used in emplo yment, wo rkers management and access to self-emplo yment, in par ticular f or the
recr uitment and selection of persons, f or making decisions affe cting te r ms of the work -related relationship,
promotion and term ination of w ork -relate d contractual relationships, f or allocating tasks on the basis of individual
behavi our , personal traits or ch aracter istics and f or monitoring or evaluation of persons in work -relate d contractual
relationships, should also be classif ied as high-r isk, since those systems ma y hav e an appreciable im pact on future
career prospects, livelihoods of those persons and work ers’ r ights. Relevant wo rk -related contractual relationships
should, in a meaningful manner , in volve emp lo y ees and persons provid ing ser vices through platf or ms as refe r red to
in the Commission W ork Programme 2021. Throughout the recr uitment process and in the evaluation, promotion,
or retention of persons in w ork -related contractual relationships, suc h syste ms ma y per petuat e historical patterns of
discr imination, f or exam ple against w omen, cer tain age groups, persons with disabilities, or persons of cer tain racial
or ethnic or igins or sexual or ientation. AI syste ms used to monitor the perf or mance and behaviour of suc h persons
ma y also under mine their fundamental r ights to data protection and pr ivacy .
(58) Another area in which the use of AI syste ms deser ves special consideration is the access to and enjo yment of cer tain
essential pr ivate and public ser vices and benef its necessar y f or people to fully par ticipate in society or to imp rove
one’s standard of living. In par ticular , natural persons applying f or or receiving essential public assistance benef its
and ser vices from public author ities namely healthcare ser vices, social secur ity benef its, social ser vices providing
prot ection in cases suc h as maternity , illness, industr ial accidents, dependency or old age and loss of emp lo yment
and social and housing assistance, are typically dependent on those benef its and ser vices and in a vulnerable position
in relation to the responsible author ities. If AI syste ms are used f or det er mining whether suc h benefits and ser vices
should be granted, denied, reduced, revok ed or reclaimed by author ities, including whether beneficiar ies are
legitimat ely entitled to suc h benefi ts or ser vices, those syste ms ma y hav e a significant impact on persons’ livelihood
and ma y infr inge their fundamental r ights, suc h as the r ight t o social protection, non-discr imination, human dignity
or an effe ctive remedy and should theref ore be classif ied as high-r isk. Nonetheless, this Regulation should not
ham per the development and use of innovative approac hes in the public administration, which w ould stand to
benefit from a wider use of compliant and safe AI systems, provid ed that those syste ms do not entail a high r isk to
leg al and natural persons. In addition, AI syste ms used to evaluate the credit score or creditwor thiness of natural
persons should be classif ied as high-r isk AI systems, since the y determi ne those persons’ access to financ ial resources
or essential ser vices suc h as housing, electr icity , and te lecommunication ser vices. AI syste ms used f or those pur poses
ma y lead t o discr imination between persons or groups and ma y per petuat e histor ical patt er ns of discr imination,
suc h as that based on racial or ethnic or igins, g ender , disabilities, age or sexual or ientation, or ma y create new f or ms
of discr iminatory im pacts. However , AI systems provid ed f or by Uni on law f or the pur pose of det ecting fraud in the
offe r ing of financ ial ser vices and f or pr udential pur poses to calculate credit institutions’ and insurance under takings ’
capital requirements should not be considered t o be high-r isk under this Regulation. Moreove r , AI systems intended
EN
OJ L, 12.7.2024
16/144 ELI: http://data.europa.eu/eli/reg/2024/1689/ojt o be used f or r isk assessment and pr icing in relation to natural persons f or health and life insurance can also have
a significant impact on persons’ livelihood and if not duly designed, developed and used, can infr inge their
fundamental r ights and can lead t o ser ious consequences f or people ’s lif e and health, including financ ial exclusion
and discr imination. Finally , AI systems used to evaluate and classify emerge ncy calls by natural persons or to
dispatc h or establish pr ior ity in the dispatch ing of emerg ency fir st response ser vices, including by police, fir efighters
and medical aid, as well as of emerge ncy healthcare patient tr iage syste ms, should also be classif ied as high-r isk since
the y make decisions in ver y cr itical situations f or the lif e and health of persons and their proper ty .
(59) Given their role and responsibility , actions b y la w enf orcement author ities inv olving cer tain uses of AI systems are
ch aracterised b y a signifi cant degree of power imbalance and ma y lead to sur veillance, ar rest or depr ivation of
a natural person’s liber ty as well as other adverse im pacts on fundamental r ights guaranteed in the Char te r . In
par ticular , if the AI syste m is not trained with high-quality data, does not meet adequate requirements in ter ms of its
perf or mance, its accuracy or robustness, or is not properly designed and te sted bef ore being put on the market or
other wise put into ser vice, it ma y sing le out people in a discr iminat or y or other wise incor rect or unjust manner .
Fur ther more, the ex ercise of imp or tant procedural fundamental r ights, suc h as the r ight to an eff ective remedy and
t o a f air tr ial as well as the r ight of defence and the presump tion of innocence, could be hampered, in par ticular ,
where such AI syste ms are not suffi ciently transparent, explainable and documente d. It is theref ore appropr iate to
classify as high-r isk, insofa r as their use is per mitte d under relevant Union and national law , a number of AI syste ms
inte nded t o be used in the law enf orcement cont ext where accuracy , reliability and transparency is par ticularly
im por tant to av oid adverse im pacts, retain public tr ust and ensure accountability and eff ective redress. In view of the
nature of the activities and the r isks relating thereto , those high-r isk AI systems should include in par ticular AI
syste ms inte nded t o be used by or on behalf of law enf orcement author ities or by Union institutions, bodies, offices,
or age ncies in suppor t of law enf orcement author ities f or assessing the r isk of a natural person t o become a victim of
cr iminal offenc es, as polygraphs and similar too ls, f or the evaluation of the reliability of evidence in in the course of
in vestig ation or prosecution of cr iminal offences, and, insofar as not prohibited under this Regulation, f or assessing
the r isk of a natural person offe nding or reoffe nding not solely on the basis of the prof iling of natural persons or the
assessment of personality traits and ch aracter istics or the past cr iminal behavio ur of natural persons or groups, f or
profiling in the course of detection, investig ation or prosecution of cr iminal offences. AI syste ms specifically
inte nded to be used f or administrative proceedings b y tax and custo ms author ities as well as by fin ancial intellig ence
units car r ying out administrative tasks analysing inf or mation pursuant t o Uni on anti-money launder ing law should
not be classif ied as high-r isk AI syste ms used by law enf orcement author ities f or the pur pose of prevention,
det ection, in vestig ation and prosecution of cr iminal offenc es. The use of AI too ls by la w enf orcement and other
relevant author ities should not become a f actor of inequality , or exclus ion. The imp act of the use of AI to ols on the
defe nce r ights of suspects should not be ignored, in par ticular the diff iculty in obtaining meaningful inf or mation on
the functioning of those systems and the resulting difficulty in ch allenging their results in cour t, in par ticular by
natural persons under investig ation.
(60) AI systems used in migration, asylum and border control managem ent affe ct persons who are of t en in par ticularly
vulnerable position and who are dependent on the outcome of the actions of the comp etent public author ities. The
accuracy , non-discr iminat or y nature and transparency of the AI systems used in those contexts are theref ore
par ticularly im por tant to guarante e respect f or the fundamental r ights of the aff ected persons, in par ticular their
r ights t o free move ment, non-discr imination, prot ection of pr ivate life and personal data, international prot ection
and good administration. It is theref ore appropr iate to classify as high-r isk, insofa r as their use is per mitted under
relevant Uni on and national law , AI syste ms intended to be used by or on behalf of compet ent public author ities or
b y Uni on institutions, bodies, off ices or age ncies ch arged with tasks in the fi elds of migration, asylum and border
control management as polygraphs and similar t ools, f or assessing cer tain r isk s posed b y natural persons ent er ing
the terr itory of a Member Stat e or applying f or visa or asylum, f or assisting compet ent public author ities f or the
examination, including related assessment of the reliability of evidence, of applications f or asylum, visa and residence
per mits and associat ed comp laints with regard to the objective to establish the eligibility of the natural persons
applying f or a status, f or the pur pose of detecting, recognising or identifying natural persons in the context of
migration, asylum and border control manag ement, with the excep tion of ver ification of tra vel documents. AI
syste ms in the area of migration, asylum and border control management covered by this Regulation should comply
with the relevant procedural requirements set by the Regulation (EC) No 810/2009 of the European Pa rliament and
OJ L, 12.7.2024
EN
ELI: http://data.europa.eu/eli/reg/2024/1689/oj 17/144of the Council (
32
), the Directive 2013/32/EU of the European Parliament and of the Council (
33
), and other relevant
Uni on law . The use of AI systems in migration, asylum and border control management should, in no circumstances,
be used by Member Stat es or Uni on institutions, bodies, off ices or age ncies as a means to circum vent their
inte r national obligations under the UN Conve ntion relating to the Status of Refuge es done at Geneva on 28 July
1951 as amended by the Prot ocol of 31 Januar y 1967. Nor should they be used to in any wa y infr ing e on the
pr inciple of non-ref oulement, or t o deny safe and effective legal avenues into the ter r itory of the Union, including
the r ight t o inter national prot ection.
(61) Cer tain AI systems intended f or the administration of justice and democratic processes should be classified as
high-r isk , consider ing their potent ially signifi cant im pact on democracy , the r ule of law , individual freedoms as well
as the r ight to an effe ctive remedy and t o a f air tr ial. In par ticular , to address the r isks of potential biases, er rors and
opacity , it is appropr iate t o qualify as high-r isk AI syste ms inte nded to be used by a judicial author ity or on its behalf
t o assist judicial author ities in researching and inte r preting f acts and the law and in applying the law to a concrete set
of f acts. AI syste ms inte nded to be used b y alt er native dispute resolution bodies f or those pur poses should also be
considered to be high-r isk when the outcomes of the alter native dispute resolution proceedings produce leg al effe cts
f or the par ties. The use of AI tools can suppor t the decision-making po wer of judges or judicial independence, but
should not replace it: the final decision-making must remain a human-dr iven activity . The classification of AI
syste ms as high-r isk should not, ho wever , extend to AI syste ms inte nded f or purely ancillar y administrative activities
that do not affect the actual administration of justice in individual cases, suc h as anonymisa tion or
pseudon ymisation of judicial decisions, documents or data, communication between personnel, administrative tasks.
(62) Without prejudice to the r ules pro vided f or in Regulation (EU) 2024/900 of the European Pa rliament and of the
Council (
34
), and in order to address the r isk s of undue exte r nal interfere nce with the r ight to v ote enshr ined in
Ar ticle 39 of the Char te r , and of adverse effe cts on democracy and the r ule of law , AI syste ms intended to be used to
inf luence the outcome of an election or refe rendum or the v oting behaviour of natural persons in the exercise of
their v ote in elections or referenda should be classif ied as high-r isk AI systems with the ex ception of AI syste ms
whose output natural persons are not directly exposed to, suc h as tools used to organise, optimise and str ucture
political cam paigns from an administrative and logistical point of view .
(63) The fact that an AI syste m is classif ied as a high-r isk AI system under this Regulation should not be inter preted as
indicating that the use of the system is law ful under other acts of Uni on law or under national law compatible with
Uni on law , such as on the prot ection of personal data, on the use of polygraphs and similar t ools or other systems to
det ect the emotional stat e of natural persons. Any suc h use should continue to occur solely in accordance with the
applicable requirements resulting from the Char t er and from the applicable acts of secondar y Uni on law and
national law . This Regulation should not be understood as provid ing f or the lega l ground f or processing of personal
data, including special categor ies of personal data, where relevant, unless it is specifically other wise provid ed f or in
this Regulation.
(64) T o mitigat e the r isks from high-r isk AI syste ms placed on the marke t or put into ser vice and t o ensure a high level of
tr ustwo r thiness, cer tain mandat or y requirements should apply to high-r isk AI systems, taking into account the
inte nded pur pose and the cont ext of use of the AI syste m and according to the r isk -management system to be
established by the pro vider . The measures adop ted b y the providers to comply with the mandato r y requirements of
this Regulation should take into account the g enerally ackno wledged state of the ar t on AI, be propor tionate and
eff ective t o meet the objectives of this Regulation. Based on the New Legislative Framework, as clar ified in
Commission notice ‘ The “Blue Guide” on the im plementation of EU product r ules 2022’, the g eneral r ule is that
more than one legal act of Union har monisation legislation ma y be applicable to one product, since the making
a vailable or putting into ser vice can take place only when the product complie s with all applicable Uni on
har monisation legislation. The hazards of AI systems covered by the requirements of this Regulation concer n
diff erent aspects than the existing Union har monisation legislation and theref ore the requirements of this Regulation
w ould complement the existing body of the Uni on har monisation legislation. For exam ple, machi ner y or medical
devices products incor porating an AI system might present r isks not addressed by the essential health and safety
EN
OJ L, 12.7.2024
18/144 ELI: http://data.europa.eu/eli/reg/2024/1689/oj
(
32
) Regulation (EC) No 810/2009 of the European Parliament and of the Council of 13 July 2009 establishing a Community Code on
Visas (Visa Code) (OJ L 243, 15.9.2009, p. 1).
(
33
) Directive 2013/32/EU of the European Parl iament and of the Council of 26 June 2013 on common procedures f or granting and
withdraw ing int er national prot ection (OJ L 180, 29.6.2013, p. 60).
(
34
) Regulation (EU) 2024/900 of the European parliament and of the Council of 13 March 2024 on the transparency and targe ting of
political adver tising (OJ L, 2024/900, 20.3.2024, ELI: http://data.europa.eu/eli/reg/2024/900/oj).requirements set out in the relevant Union har monised legislation, as that sectoral la w does not deal with r isks
specific to AI systems. This calls f or a simultaneous and comp lementar y application of the var ious legislative acts. T o
ensure consist ency and to av oid an unnecessar y administrative burden and unnecessar y costs, provid ers of a product
that contains one or more high-r isk AI system, to which the requirements of this Regulation and of the Uni on
har monisation legislation based on the New Legislative Framewo rk and listed in an annex t o this Regulation apply ,
should hav e f lexibility with regard to operational decisions on how to ensure compliance of a product that contains
one or more AI syste ms with all the applicable requirements of that Uni on har monised legislation in an op timal
manner . That f lexibility could mean, f or exam ple a decision by the pro vider to inte grate a par t of the necessar y
t esting and repor ting processes, inf or mation and documentation required under this Regulation into already existing
documentation and procedures required under existing Uni on har monisation legislation based on the New
Legislative Framework and listed in an annex to this Regulation. This should not, in any wa y , under mine the
oblig ation of the provid er to comply with all the applicable requirements.
(65) The r isk -managem ent syste m should consist of a continuous, ite rative process that is planned and r un throughout
the entire lif ecy cle of a high-r isk AI syste m. That process should be aimed at identifying and mitigating the relevant
r isks of AI syste ms on health, saf ety and fundamental r ights. The r isk -manag ement system should be regularly
reviewed and update d to ensure its continuing effe ctiveness, as well as justifi cation and documentation of any
significant decisions and actions take n subject to this Regulation. This process should ensure that the provid er
identifies r isks or adverse im pacts and imp lements mitig ation measures f or the kno wn and reasonably f oreseeable
r isks of AI syste ms to the health, safety and fundamental r ights in light of their intended pur pose and reasonably
f oreseeable misuse, including the possible r isks ar ising from the inte raction between the AI system and the
en vironment within which it operates. The r isk -management syste m should adop t the most appropr iate
r isk -manag ement measures in light of the state of the ar t in AI. When identifying the most appropr iate
r isk -manag ement measures, the provid er should document and explain the choices made and, when relevant,
in v olve exper ts and exter nal stak eholders. In identifying the reasonably f oreseeable misuse of high-r isk AI systems,
the provid er should cover uses of AI syste ms which, while not directly co vered by the intende d pur pose and
pro vided f or in the instr uction f or use ma y never theless be reasonably expected t o result from readily predictable
human behavi our in the cont ext of the specif ic charact er istics and use of a par ticular AI syste m. Any kno wn or
f oreseeable circumstances related to the use of the high-r isk AI syste m in accordance with its inte nded pur pose or
under conditions of reasonably f oreseeable misuse, which ma y lead t o r isks to the health and safety or fundamental
r ights should be included in the instr uctions f or use that are provid ed by the provider . This is to ensure that the
deplo y er is aw are and tak es them into account when using the high-r isk AI syste m. Identifying and im plementing
r isk mitigation measures f or f oreseeable misuse under this Regulation should not require specific additional training
f or the high-r isk AI syste m by the provid er to address f oreseeable misuse. The provider s ho wever are encourage d to
consider suc h additional training measures t o mitiga te reasonable f oreseeable misuses as necessar y and appropr iate.
(66) Requirements should apply to high-r isk AI syste ms as regard s r isk management, the quality and relevance of data
sets used, te chnical documentation and record-k eeping, transparency and the provision of inf or mation to deplo y ers,
human ove rsight, and robustness, accuracy and cybersecur ity . Those requirements are necessar y to eff ectively
mitig ate the r isks f or health, safety and fundamental r ights. As no other less trade restr ictive measures are reasonably
a vailable those requirements are not unjustified restr ictions to trade.
(67) High-quality data and access to high-quality data pla ys a vital role in providing str ucture and in ensur ing the
perf or mance of many AI systems, especially when tec hniques inv olving the training of models are used, with a view
t o ensure that the high-r isk AI syste m per f or ms as inte nded and safely and it does not become a source of
discr imination prohibite d by Uni on law . High-quality data sets f or training, validation and te sting require the
im plementation of appropr iat e data gover nance and managem ent practices. Data sets f or training, validation and
t esting, including the labels, should be relevant, sufficiently representative, and t o the best extent possible free of
er rors and complet e in view of the intended pur pose of the system. In order to f acilitate comp liance with Uni on data
prot ection law , suc h as Regulation (EU) 2016/679, data gove r nance and management practices should include, in
the case of personal data, transparency about the or iginal pur pose of the data collection. The data sets should also
hav e the appropr iat e statistical proper ties, including as regard s the persons or groups of persons in relation to whom
the high-r isk AI syste m is inte nded to be used, with specific atte ntion to the mitigation of possible biases in the data
sets, that are like ly t o affect the health and saf ety of persons, hav e a negative imp act on fundamental r ights or lead to
discr imination prohibite d under Union law , especially where data outputs inf luence in puts f or future operations
OJ L, 12.7.2024
EN
ELI: http://data.europa.eu/eli/reg/2024/1689/oj 19/144(f eedbac k loops). Biases can f or example be inherent in underlying data sets, especially when histor ical data is being
used, or g enerated when the syste ms are implement ed in real world settings. Results provided b y AI systems could be
inf luenced by such inherent biases that are inclined t o gradually increase and thereby per petuate and amplify existing
discr imination, in par ticular f or persons belonging to cer tain vulnerable groups, including racial or ethnic groups.
The requirement f or the data sets t o be to the best exte nt possible complet e and free of er rors should not affect the
use of pr ivacy-preser ving te chniques in the cont ext of the development and te sting of AI systems. In par ticular , data
sets should tak e into account, to the extent required by their intended pur pose, the f eatures, ch aracter istics or
elements that are par ticular to the specif ic g eographical, contextual, behavi oural or functional setting which the AI
syste m is inte nded to be used. The requirements related to data gove r nance can be complied with b y hav ing recourse
t o third par ties that offe r cer tif ied compliance ser vices including ver ifi cation of data gover nance, data set inte gr ity ,
and data training, validation and te sting practices, as f ar as comp liance with the data requirements of this Regulation
are ensured.
(68) For the development and assessment of high-r isk AI systems, cer tain actors, such as provid ers, notif ied bodies and
other relevant entities, such as European Digital Innovation Hubs, te sting exper imentation f acilities and researchers ,
should be able to access and use high-quality data sets within the fie lds of activities of those actor s which are related
t o this Regulation. European common data spaces established by the Commission and the f acilitation of data shar ing
between businesses and with gover nment in the public intere st will be instr umental to provide tr ustful, accountable
and non-discr iminat or y access to high-quality data f or the training, validation and testing of AI syste ms. For
exam ple, in health, the European health data space will facilitat e non-discr iminatory access to health data and the
training of AI algor ithms on those data sets, in a pr ivacy-preser ving, secure, timely , transparent and tr ustw or th y
manner , and with an appropr iate institutional gove r nance. Relevant comp etent author ities, including sect oral ones,
pro viding or suppor ting the access to data ma y also suppor t the pro vision of high-quality data f or the training,
validation and testing of AI syste ms.
(69) The r ight to pr ivacy and to protection of personal data must be guarante ed throughout the entire lif ecy cle of the AI
syste m. In this regard , the pr inciples of data minimisation and data protection b y design and by default, as set out in
Uni on data protect ion law , are applicable when personal data are processed. Measures take n by provid ers to ensure
com pliance with those pr inciples ma y include not only anonymisati on and encr ypt ion, but also the use of
t echnology that per mits algor ithms to be brought to the data and allows training of AI systems without the
transmission between par ties or cop ying of the raw or str uctured data themselves, without prejudice to the
requirements on data gover nance provid ed f or in this Regulation.
(70) In order t o protect the r ight of others from the discr imination that might result from the bias in AI syste ms, the
pro viders should, ex ceptiona lly , to the extent that it is str ictly necessar y f or the pur pose of ensur ing bias detection
and cor rection in relation to the high-r isk AI systems, subject to appropr iate safeguards f or the fundamental r ights
and freedoms of natural persons and f ollowi ng the application of all applicable conditions laid do wn under this
Regulation in addition to the conditions laid down in Regulations (EU) 2016/679 and (EU) 2018/1725 and Directive
(EU) 2016/680, be able to process also special cate gor ies of personal data, as a matt er of substantial public interest
within the meaning of Ar ticle 9(2), point (g) of Regulation (EU) 2016/679 and Ar ticle 10(2), point (g) of Regulation
(EU) 2018/1725.
(71) Ha ving comp rehensible inf or mation on ho w high-r isk AI syste ms hav e been developed and how they perf or m
throughout their lifetime is essential to enable traceability of those syste ms, ver ify compliance with the requirements
under this Regulation, as well as monitoring of their operations and post mark et monitoring. This requires keeping
records and the availability of te chnical documentation, containing inf or mation which is necessar y to assess the
com pliance of the AI syste m with the relevant requirements and f acilitate post market monitorin g. Such inf or mation
should include the general charact er istics, capabilities and limitations of the system, algor ithms, data, training,
t esting and validation processes used as well as documentation on the relevant r isk -manag ement system and drawn
in a clear and comp rehensive f or m. The tec hnical documentation should be k ept up t o date, appropr iately
throughout the lif etime of the AI syste m. Fur ther more, high-r isk AI systems should te ch nically allo w f or the
auto matic recording of events, b y means of logs, over the duration of the lif etime of the syste m.
EN
OJ L, 12.7.2024
20/144 ELI: http://data.europa.eu/eli/reg/2024/1689/oj(72) T o address concer ns relate d to opacity and complexity of cer tain AI systems and help deplo yers to fulfil their
oblig ations under this Regulation, transparency should be required f or high-r isk AI syste ms bef ore they are placed
on the market or put it into ser vice. High-r isk AI systems should be designed in a manner t o enable deplo y ers to
understand ho w the AI syste m works, evaluate its functionality , and comp rehend its strengths and limitations.
High-r isk AI systems should be accom panied by appropr iate inf or mation in the f or m of instr uctions of use. Such
inf or mation should include the charact er istics, capabilities and limitations of perfo r mance of the AI system. Those
w ould cover inf or mation on possible kno wn and f oreseeable circumstances related to the use of the high-r isk AI
syste m, including deplo y er action that ma y inf luence syste m behavio ur and perfor mance, under which the AI system
can lead to r isks to health, saf ety , and fundamental r ights, on the ch anges that hav e been pre-deter mined and
assessed f or conf or mity by the pro vider and on the relevant human oversight measures, including the measures to
f acilitate the inter pretation of the outputs of the AI syste m by the deplo y ers. T ransparency , including the
accom pan ying instr uctions f or use, should assist deplo y ers in the use of the syste m and suppor t inf or med decision
making b y them. Deplo y ers should, inter alia, be in a better position to mak e the cor rect ch oice of the syste m that
the y inte nd to use in light of the obligations applicable to them, be educat ed about the inte nded and precluded uses,
and use the AI system cor rectly and as appropr iate. In order to enhance legibility and accessibility of the inf or mation
included in the instr uctions of use, where appropr iat e, illustrative exam ples, f or instance on the limitations and on
the intende d and precluded uses of the AI syste m, should be included. Provi ders should ensure that all
documentation, including the instr uctions f or use, contains meaningful, compre hensive, accessible and
understandable inf or mation, taking into account the needs and f oreseeable kno wledge of the targe t deplo y ers.
Instr uctions f or use should be made a vailable in a language which can be easily understood by targ et deplo y ers, as
det er mined by the Member Stat e concer ned.
(73) High-r isk AI systems should be designed and developed in such a wa y that natural persons can ove rsee their
functioning, ensure that they are used as intended and that their imp acts are addressed over the system’s lifecy cle. T o
that end, appropr iate human oversight measures should be identifie d by the pro vider of the syste m bef ore its placing
on the market or putting into ser vice. In par ticular , where appropr iate, suc h measures should guarantee that the
syste m is subject to in-built operational constraints that cannot be overr idden by the system itself and is responsive
t o the human operat or , and that the natural persons to whom human oversight has been assigned have the necessar y
com petence, training and author ity to car r y out that role. It is also essential, as appropr iate, t o ensure that high-r isk
AI syste ms include mechanisms to guide and inf or m a natural person to whom human oversight has been assigned
t o mak e inf or med decisions if, when and ho w to inte r vene in order to av oid nega tive consequences or r isks , or stop
the syste m if it does not per f or m as intended. Consider ing the significant consequences f or persons in the case of an
incor rect matc h by cer tain biometr ic identification systems, it is appropr iat e to provide f or an enhanced human
o versight requirement f or those systems so that no action or decision ma y be take n by the deplo yer on the basis of
the identifica tion resulting from the system unless this has been separate ly ver ified and conf ir med by at least tw o
natural persons. Those persons could be from one or more entities and include the person operating or using the
syste m. This requirement should not pose unnecessar y burden or dela ys and it could be suffi cient that the separate
ver ificati ons by the diffe rent persons are auto matically recorded in the logs g enerated by the syste m. Given the
specificities of the areas of law enf orcement, migration, border control and asylum, this requirement should not
apply where Uni on or national law considers the application of that requirement to be dispropor tionate.
(74) High-r isk AI syste ms should perf or m consiste ntly throughout their lif ecy cle and meet an appropr iate level of
accuracy , robustness and cybersecur ity , in light of their inte nded pur pose and in accordance with the g enerally
ac knowledg ed stat e of the ar t. The Commission and relevant organisations and stak eholders are encourage d to take
due consideration of the mitiga tion of r isks and the nega tive impacts of the AI system. The expecte d level of
perf or mance metr ics should be declared in the accom panying instr uctions of use. Providers are urg ed to
communicate that inf or mation t o deplo yers in a clear and easily understandable wa y , free of misunderstandings or
misleading statements. Union law on lega l metrology , including Directives 2014/31/EU (
35
) and 2014/32/EU (
36
) of
the European Parliament and of the Council, aims to ensure the accuracy of measurements and to help the
transparency and fairne ss of commercial transactions. In that context, in cooperation with relevant stakeh olders and
org anisation, such as metrology and bench marking author ities, the Commission should encourage , as appropr iate,
the development of bench mark s and measurement methodologies f or AI systems. In doing so, the Commission
should take note and collaborate with inter national par tners wo rking on metrology and relevant measurement
indicato rs relating to AI.
OJ L, 12.7.2024
EN
ELI: http://data.europa.eu/eli/reg/2024/1689/oj 21/144
(
35
) Directive 2014/31/EU of the European Parliament and of the Council of 26 Febr uar y 2014 on the har monisation of the la ws of the
Member State s relating to the making available on the market of non-aut omatic weighing instr uments (OJ L 96, 29.3.2014, p. 107).
(
36
) Directive 2014/32/EU of the European Parliament and of the Council of 26 Febr uar y 2014 on the har monisation of the la ws of the
Member States relating t o the making available on the market of measur ing instr uments (OJ L 96, 29.3.2014, p. 149).(75) T ec hnical robustness is a ke y requirement f or high-r isk AI systems. They should be resilient in relation to har mful or
other wise undesirable behavi our that ma y result from limitations within the syste ms or the envir onment in which
the systems operate (e.g. er rors, faults, inconsiste ncies, unexpected situations). Theref ore, tec hnical and
org anisational measures should be take n to ensure robustness of high-r isk AI systems, f or exam ple b y designing
and developing appropr iate te chnical solutions to prevent or minimise har mful or other wise undesirable behavi our .
Those te chnical solution ma y include f or instance mec hanisms enabling the system to saf ely inter r up t its operation
(f ail-saf e plans) in the presence of cer tain anomalies or when operation take s place outside cer tain predeter mined
boundar ies. Fa ilure to protect aga inst these r isks could lead to safety im pacts or nega tively aff ect the fundamental
r ights, f or exam ple due to er roneous decisions or wrong or biased outputs g enerated by the AI syste m.
(76) Cybersecur ity pla ys a cr ucial role in ensur ing that AI syste ms are resilient aga inst attem pts to alt er their use,
behavi our , perf or mance or comp romise their secur ity proper ties by malicious third par ties exploiting the syste m’s
vulnerabilities. Cyberattacks ag ainst AI syste ms can leverage AI specific assets, suc h as training data sets (e.g. data
poisoning) or trained models (e.g. adversar ial attacks or membership inf erence), or exploit vulnerabilities in the AI
syste m’s digital assets or the underlying ICT infrastr ucture. T o ensure a level of cybersecur ity appropr iate t o the r isks ,
suitable measures, suc h as secur ity controls, should theref ore be take n by the provid ers of high-r isk AI systems, also
taking into account as appropr iate the underlying ICT infrastr ucture.
(77) Without prejudice to the requirements related to robustness and accuracy set out in this Regulation, high-r isk AI
syste ms which f all within the scope of a regulation of the European Parliament and of the Council on hor izontal
cybersecur ity requirements f or products with digital elements, in accordance with that regulation ma y demonstrate
com pliance with the cybersecur ity requirements of this Regulation by fulf illing the essential cybersecur ity
requirements set out in that regulation. When high-r isk AI systems fulf il the essential requirements of a regulation of
the European Parliament and of the Council on hor izontal cybersecur ity requirements f or products with digital
elements, the y should be deemed compliant with the cybersecur ity requirements set out in this Regulation in so f ar
as the ac hievement of those requirements is demonstrated in the EU declaration of conf or mity or par ts thereof
issued under that regulation. T o that end, the assessment of the cybersecur ity r isks , associated to a product with
digital elements classif ied as high-r isk AI syste m according to this Regulation, car r ied out under a regulation of the
European Parliament and of the Council on hor izontal cybersecur ity requirements f or products with digital
elements, should consider r isks to the cyber resilience of an AI system as regard s attem pts by unauthor ised third
par ties to alt er its use, behavio ur or perf or mance, including AI specif ic vulnerabilities suc h as data poisoning or
adversar ial attacks, as well as, as relevant, r isks to fundamental r ights as required b y this Regulation.
(78) The conf or mity assessment procedure provid ed by this Regulation should apply in relation t o the essential
cybersecur ity requirements of a product with digital elements covered by a regulation of the European Pa rliament
and of the Council on hor izontal cybersecur ity requirements f or products with digital elements and classified as
a high-r isk AI system under this Regulation. However , this r ule should not result in reducing the necessar y level of
assurance f or cr itical products with digital elements covered by a regulation of the European Pa rliament and of the
Council on hor izontal cybersecur ity requirements f or products with digital elements. Theref ore, by wa y of
deroga tion from this r ule, high-r isk AI syste ms that f all within the scope of this Regulation and are also qualified as
im por tant and cr itical products with digital elements pursuant to a regulation of the European Parliament and of the
Council on hor izontal cybersecur ity requirements f or products with digital elements and t o which the conf or mity
assessment procedure based on inter nal control set out in an annex t o this Regulation applies, are subject to the
conf or mity assessment provisions of a regulation of the European Parliament and of the Council on hor izontal
cybersecur ity requirements f or products with digital elements insofar as the essential cybersecur ity requirements of
that regulation are concer ned. In this case, f or all the other aspects covered by this Regulation the respective
pro visions on conf or mity assessment based on inter nal control set out in an annex to this Regulation should apply .
Building on the kno wledge and exper tise of ENISA on the cybersecur ity policy and tasks assigned t o ENISA under
the Regulation (EU) 2019/881 of the European Parliament and of the Council (
37
), the Commission should cooperate
with ENISA on issues related t o cybersecur ity of AI syste ms.
EN
OJ L, 12.7.2024
22/144 ELI: http://data.europa.eu/eli/reg/2024/1689/oj
(
37
) Regulation (EU) 2019/881 of the European Parliament and of the Council of 17 Apr il 2019 on ENISA (the European Union Ag ency
f or Cybersecur ity) and on inf or mation and communications tec hnology cybersecur ity cer tification and repealing Regulation
(EU) No 526/2013 (Cybersecur ity A ct) (OJ L 151, 7.6.2019, p. 15).(79) It is appropr iate that a specif ic natural or lega l person, def ined as the pro vider , takes responsibility f or the placing on
the marke t or the putting into ser vice of a high-r isk AI syste m, regardless of whether that natural or lega l person is
the person who designed or developed the syste m.
(80) As signat or ies t o the Uni te d Nations Convention on the Rights of Pe rsons with Disabilities, the Union and the
Member Stat es are legally oblige d to prot ect persons with disabilities from discr imination and promot e their equality ,
t o ensure that persons with disabilities hav e access, on an equal basis with others, to inf or mation and
communications te chnologies and systems, and to ensure respect f or pr ivacy f or persons with disabilities. Given the
gro wing impor tance and use of AI systems, the application of universal design pr inciples to all new te chnologi es and
ser vices should ensure full and equal access f or ever y one potentially affect ed by or using AI t echnologies, including
persons with disabilities, in a wa y that take s full account of their inherent dignity and diversity . It is theref ore
essential that provid ers ensure full compliance with accessibility requirements, including Directive (EU) 2016/2102
of the European Pa rliament and of the Council (
38
) and Directive (EU) 2019/882. Providers should ensure
com pliance with these requirements by design. Theref ore, the necessar y measures should be integrat ed as much as
possible into the design of the high-r isk AI syste m.
(81) The provider should establish a sound quality manag ement syste m, ensure the accom plishment of the required
conf or mity assessment procedure, draw up the relevant documentation and establish a robust post-mark et
monito r ing syste m. Provi ders of high-r isk AI systems that are subject to oblig ations regarding quality management
syste ms under relevant secto ral Uni on la w should have the possibility t o include the elements of the quality
manag ement syste m provid ed f or in this Regulation as par t of the existing quality management system provid ed f or
in that other sectoral Uni on law . The complement ar ity between this Regulation and existing secto ral Union la w
should also be take n into account in future standardisation activities or guidance adop ted by the Commission. Public
author ities which put into ser vice high-r isk AI syste ms f or their own use ma y adop t and im plement the r ules f or the
quality managem ent syste m as par t of the quality managem ent syste m adop te d at a national or regional level, as
appropr iat e, taking into account the specificities of the secto r and the comp etences and org anisation of the public
author ity concer ned.
(82) T o enable enf orcement of this Regulation and create a level pla ying field f or operat ors, and, taking into account the
diff erent f or ms of making ava ilable of digital products, it is impor tant to ensure that, under all circumstances,
a person established in the Uni on can provide author ities with all the necessar y inf or mation on the comp liance of an
AI syste m. Theref ore, pr ior to making their AI systems ava ilable in the Uni on, provid ers established in third
countr ies should, by wr itten mandat e, appoint an author ised representative established in the Union. This author ised
representative pla ys a pivot al role in ensur ing the comp liance of the high-r isk AI systems placed on the marke t or
put into ser vice in the Union by those providers who are not established in the Union and in ser ving as their contact
person established in the Union.
(83) In light of the nature and comp lexity of the value chain f or AI syste ms and in line with the New Legislative
Framewo rk , it is essential to ensure lega l cer tainty and f acilitate the compliance with this Regulation. Theref ore, it is
necessar y to clar ify the role and the specific obliga tions of relevant operat ors along that value chain, suc h as
im por te rs and distr ibutors who ma y contr ibut e to the development of AI systems. In cer tain situations those
operat ors could act in more than one role at the same time and should theref ore fulfil cumulatively all relevant
oblig ations associate d with those roles. For exam ple, an operator could act as a distr ibutor and an imp or te r at the
same time.
(84) T o ensure legal cer tainty , it is necessar y to clar ify that, under cer tain specific conditions, any distr ibutor , imp or te r ,
deplo y er or other third-par ty should be considered to be a provider of a high-r isk AI syste m and theref ore assume all
the relevant obliga tions. This would be the case if that par ty puts its name or trademark on a high-r isk AI system
already placed on the marke t or put into ser vice, without prejudice to contractual ar rang ements stipulating that the
oblig ations are allocated other wise. This would also be the case if that par ty makes a substantial modifi cation to
a high-r isk AI syste m that has already been placed on the market or has already been put into ser vice in a wa y that it
remains a high-r isk AI syste m in accordance with this Regulation, or if it modif ies the inte nded pur pose of an AI
syste m, including a general-purpo se AI system, which has not been classif ied as high-r isk and has already been
placed on the mark et or put into ser vice, in a wa y that the AI syste m becomes a high-r isk AI syste m in accordance
with this Regulation. Those provis ions should apply without prejudice to more specific pro visions established in
cer tain Uni on har monisation legislation based on the New Legislative Framewo rk , tog ether with which this
OJ L, 12.7.2024
EN
ELI: http://data.europa.eu/eli/reg/2024/1689/oj 23/144
(
38
) Directive (EU) 2016/2102 of the European Parl iament and of the Council of 26 October 2016 on the accessibility of the websit es
and mobile applications of public sector bodies (OJ L 327, 2.12.2016, p. 1).Regulation should apply . For example, Ar ticle 16(2) of Regulation (EU) 2017/745, establishing that cer tain ch ange s
should not be considered to be modifi cations of a device that could affe ct its comp liance with the applicable
requirements, should continue to apply to high-r isk AI syste ms that are medical devices within the meaning of that
Regulation.
(85) General-pur pose AI syste ms ma y be used as high-r isk AI syste ms by themselves or be comp onents of other high-r isk
AI syste ms. Theref ore, due to their par ticular nature and in order to ensure a f air shar ing of responsibilities along the
AI value ch ain, the provider s of such syste ms should, ir respective of whether they ma y be used as high-r isk AI
syste ms as such by other providers or as compo nents of high-r isk AI syste ms and unless provid ed other wise under
this Regulation, closely cooperate with the providers of the relevant high-r isk AI systems to enable their compliance
with the relevant obligations under this Regulation and with the compet ent author ities established under this
Regulation.
(86) Where, under the conditions laid do wn in this Regulation, the provid er that initially placed the AI syste m on the
mark et or put it into ser vice should no longe r be considered to be the provid er f or the pur poses of this Regulation,
and when that provid er has not expressly excluded the chang e of the AI syste m into a high-r isk AI syste m, the
f or mer provid er should nonetheless closely cooperate and make ava ilable the necessar y inf or mation and provid e the
reasonably expecte d te chnical access and other assistance that are required f or the fulf ilment of the obligations set
out in this Regulation, in par ticular regarding the comp liance with the conf or mity assessment of high-r isk AI
syste ms.
(87) In addition, where a high-r isk AI syste m that is a saf ety comp onent of a product which f alls within the scope of
Uni on har monisation legislation based on the New Legislative Framework is not placed on the market or put into
ser vice independently from the product, the product manufa cturer def ined in that legislation should comp ly with
the obliga tions of the provider established in this Regulation and should, in par ticular , ensure that the AI system
embedded in the final product comp lies with the requirements of this Regulation.
(88) Along the AI value chain multiple par ties of te n supply AI syste ms, tools and ser vices but also compo nents or
processes that are incor porate d b y the provid er into the AI syste m with var ious objectives, including the model
training, model retraining, model te sting and evaluation, inte gration into sof tware, or other aspects of model
development. Those par ties have an imp or tant role to pla y in the value ch ain to wards the provid er of the high-r isk
AI syste m into which their AI systems, tools, ser vices, comp onents or processes are integrat ed, and should provide
b y wr itt en agreement this provid er with the necessar y inf or mation, capabilities, tec hnical access and other assistance
based on the generally ackno wledged stat e of the ar t, in order to enable the provid er to fully comply with the
oblig ations set out in this Regulation, without com promising their ow n intellect ual proper ty r ights or trade secrets.
(89) Third par ties making accessible to the public too ls, ser vices, processes, or AI comp onents other than
g eneral-pur pose AI models, should not be mandat ed to comp ly with requirements targ eting the responsibilities
along the AI value ch ain, in par ticular to wards the provid er that has used or integrat ed them, when those t ools,
ser vices, processes, or AI comp onents are made accessible under a free and open-source licence. Developers of free
and open-source tools, ser vices, processes, or AI comp onents other than g eneral-pur pose AI models should be
encourag ed t o im plement widely adop ted documentation practices, suc h as model cards and data sheets, as a wa y to
accelerat e inf or mation shar ing along the AI value chain, allowi ng the promotion of tr ustwor thy AI systems in the
Uni on.
(90) The Commission could develop and recommend voluntary model contractual term s between providers of high-r isk
AI syste ms and third par ties that supply to ols, ser vices, comp onents or processes that are used or integrat ed in
high-r isk AI systems, to f acilitate the cooperation along the value ch ain. When developing v oluntar y model
contractual te r ms, the Commission should also tak e into account possible contractual requirements applicable in
specific sectors or business cases.
(91) Given the nature of AI systems and the r isk s to safety and fundamental r ights possibly associate d with their use,
including as rega rds the need to ensure proper monitori ng of the perfor mance of an AI syste m in a real-life setting, it
is appropr iate to set specific responsibilities f or deplo y ers. Deplo y ers should in par ticular tak e appropr iat e t echnical
and organisational measures to ensure the y use high-r isk AI syste ms in accordance with the instr uctions of use and
cer tain other obliga tions should be pro vided f or with regard to monitoring of the functioning of the AI systems and
with regard to record-keeping, as appropr iate. Fur ther more, deplo y ers should ensure that the persons assigned to
im plement the instr uctions f or use and human ove rsight as set out in this Regulation hav e the necessar y
EN
OJ L, 12.7.2024
24/144 ELI: http://data.europa.eu/eli/reg/2024/1689/ojcom petence, in par ticular an adequate level of AI literacy , training and author ity to properly fulf il those tasks. Those
oblig ations should be without prejudice to other deplo y er obliga tions in relation to high-r isk AI syste ms under
Uni on or national la w .
(92) This Regulation is without prejudice to obligations f or emplo y ers to inf or m or t o inf or m and consult w orkers or
their representatives under Uni on or national la w and practice, including Directive 2002/14/EC of the European
P arliament and of the Council (
39
), on decisions to put into ser vice or use AI syste ms. It remains necessar y to ensure
inf or mation of w orkers and their representatives on the planned deplo yment of high-r isk AI syste ms at the
w orkplace where the conditions f or those inf or mation or inf or mation and consultation obliga tions in other leg al
instr uments are not fulfilled. Moreo ver , such inf or mation r ight is ancillar y and necessar y to the objective of
prot ecting fundamental r ights that underlies this Regulation. Theref ore, an inf or mation requirement t o that effect
should be laid do wn in this Regulation, without affe cting any existing r ights of work ers.
(93) Whilst r isk s relate d to AI systems can result from the wa y suc h systems are designed, r isks can as well st em from
ho w such AI systems are used. Deplo y ers of high-r isk AI syste m theref ore pla y a cr itical role in ensur ing that
fundamental r ights are protect ed, compl ementing the obligati ons of the provider when developing the AI syste m.
Deplo y ers are best placed t o understand ho w the high-r isk AI syste m will be used concretely and can theref ore
identify potent ial signif icant r isks that were not f oreseen in the development phase, due to a more precise kno wledge
of the context of use, the persons or groups of persons like ly to be affect ed, including vulnerable groups. Deplo yers
of high-r isk AI syste ms list ed in an annex to this Regulation also pla y a cr itical role in inf or ming natural persons and
should, when they mak e decisions or assist in making decisions related to natural persons, where applicable, inf or m
the natural persons that the y are subject to the use of the high-r isk AI system. This inf or mation should include the
inte nded pur pose and the type of decisions it makes. The deplo yer should also inf or m the natural persons about
their r ight to an explanation provided under this Regulation. With regard to high-r isk AI syste ms used f or la w
enf orcement pur poses, that obligation should be imp lemented in accordance with Ar ticle 13 of Directive (EU)
2016/680.
(94) Any processing of biometr ic data in volved in the use of AI systems f or biometr ic identifica tion f or the pur pose of
la w enf orcement needs to comp ly with Ar ticle 10 of Directive (EU) 2016/680, that allo ws suc h processing only
where str ictly necessar y , subject to appropr iate safeguards f or the r ights and freedoms of the data subject, and where
author ised by Union or Member Stat e law . Such use, when author ised, also needs to respect the pr inciples laid do wn
in Ar ticle 4 (1) of Directive (EU) 2016/680 including law fulness, f air ness and transparency , pur pose limitation,
accuracy and storag e limitation.
(95) Without prejudice to applicable Uni on law , in par ticular Regulation (EU) 2016/679 and Directive (EU) 2016/680,
consider ing the intr usive nature of post-remote biometr ic identifica tion systems, the use of post-remote biometr ic
identification syste ms should be subject to saf eguards. P ost-remot e biometr ic identification systems should alwa ys be
used in a wa y that is propor tionate , legitimate and str ictly necessar y , and thus targe te d, in te r ms of the individuals to
be identified, the location, t emporal scope and based on a closed data set of leg ally acquired video f ootage. In any
case, post-remote biometr ic identifica tion syste ms should not be used in the framework of law enf orcement to lead
t o indiscr iminate sur veillance. The conditions f or post-remote biometr ic identifica tion should in any case not
pro vide a basis to circumvent the conditions of the prohibition and str ict excep tions f or real time remote biometr ic
identification.
(96) In order to eff iciently ensure that fundamental r ights are protect ed, deplo y ers of high-r isk AI syste ms that are bodies
gove r ned by public law , or pr ivate entities providing public ser vices and deplo y ers of cer tain high-r isk AI syste ms
list ed in an annex to this Regulation, suc h as banking or insurance entities, should car r y out a fundamental r ights
im pact assessment pr ior to putting it into use. Ser vices imp or tant f or individuals that are of public nature ma y also
be provided by pr ivate entities. Pr ivat e entities pro viding suc h public ser vices are linked t o tasks in the public interest
suc h as in the areas of education, healthcare, social ser vices, housing, administration of justice. The aim of the
fundamental r ights impact assessment is f or the deplo y er to identify the specif ic r isks to the r ights of individuals or
groups of individuals like ly to be affe cted , identify measures to be take n in the case of a mat er ialisation of those r isks.
The impact assessment should be perf or med pr ior to deplo ying the high-r isk AI syste m, and should be updated
OJ L, 12.7.2024
EN
ELI: http://data.europa.eu/eli/reg/2024/1689/oj 25/144
(
39
) Directive 2002/14/EC of the European Parliament and of the Council of 11 March 2002 establishing a ge neral framew ork f or
inf or ming and consulting em plo yee s in the European Community (OJ L 80, 23.3.2002, p. 29).when the deplo y er considers that any of the relevant f actors hav e ch anged. The im pact assessment should identify
the deplo yer ’s relevant processes in which the high-r isk AI system will be used in line with its intended pur pose, and
should include a descr iption of the per iod of time and frequency in which the syste m is intended to be used as well
as of specific categor ies of natural persons and groups who are like ly to be affect ed in the specif ic context of use. The
assessment should also include the identifi cation of specific r isks of har m likel y to hav e an im pact on the
fundamental r ights of those persons or groups. While per f or ming this assessment, the deplo y er should tak e into
account inf or mation relevant to a proper assessment of the imp act, including but not limit ed to the inf or mation
given by the provid er of the high-r isk AI system in the instr uctions f or use. In light of the r isks identified, deplo yers
should det er mine measures to be taken in the case of a materialis ation of those r isks , including f or exam ple
gove r nance ar rangements in that specific context of use, suc h as ar rang ements f or human ove rsight according to the
instr uctions of use or , complai nt handling and redress procedures, as they could be instr umental in mitigating r isks
t o fundamental r ights in concrete use-cases. Af te r perfor ming that impact assessment, the deplo y er should notify the
relevant marke t sur veillance author ity . Where appropr iate, to collect relevant inf or mation necessar y to perfo r m the
im pact assessment, deplo yers of high-r isk AI syste m, in par ticular when AI systems are used in the public sect or ,
could inv olve relevant stakeh olders, including the representatives of groups of persons like ly to be affect ed by the AI
syste m, independent exper ts, and civil society organisations in conducting suc h im pact assessments and designing
measures to be take n in the case of materi alisation of the r isks . The European Ar tif icial Intellig ence Off ice (AI Offi ce)
should develop a t emplat e f or a questionnaire in order to f acilitate comp liance and reduce the administrative burden
f or deplo y ers.
(97) The notion of g eneral-pur pose AI models should be clearly defined and set apar t from the notion of AI systems to
enable legal cer tainty . The def inition should be based on the ke y functional ch aracter istics of a general-pur pose AI
model, in par ticular the generality and the capability t o com petently perf or m a wide rang e of distinct tasks. These
models are typically trained on larg e amounts of data, through var ious methods, suc h as self-super vised,
unsuper vised or reinf orcement lear ning. General-pur pose AI models ma y be placed on the market in var ious wa ys,
including through librar ies, application programming interfaces (APIs), as direct download, or as physica l cop y .
These models ma y be fur ther modif ied or fine -tuned into new models. Although AI models are essential
com ponents of AI systems, they do not constitute AI systems on their own. AI models require the addition of fur ther
com ponents, such as f or exam ple a user inte rface, t o become AI syste ms. AI models are typically integrat ed into and
f or m par t of AI systems. This Regulation provid es specific r ules f or g eneral-pur pose AI models and f or
g eneral-pur pose AI models that pose syste mic r isks , which should apply also when these models are integrat ed or
f or m par t of an AI syste m. It should be understood that the obliga tions f or the providers of gene ral-pur pose AI
models should apply once the g eneral-pur pose AI models are placed on the marke t. When the provid er of
a g eneral-pur pose AI model inte grates an ow n model into its ow n AI syste m that is made ava ilable on the market or
put into ser vice, that model should be considered to be placed on the marke t and, theref ore, the obliga tions in this
Regulation f or models should continue to apply in addition t o those f or AI syste ms. The obligations laid do wn f or
models should in an y case not apply when an own model is used f or purely inter nal processes that are not essential
f or providing a product or a ser vice to third par ties and the r ights of natural persons are not affe cted . Consider ing
their potent ial signifi cantly nega tive effe cts, the g eneral-pur pose AI models with syste mic r isk should alwa ys be
subject to the relevant obliga tions under this Regulation. The definition should not cover AI models used bef ore their
placing on the mark et f or the sole pur pose of researc h, development and prot otyping activities. This is without
prejudice to the obligation to com ply with this Regulation when, f ollo wing suc h activities, a model is placed on the
mark et.
(98) Whereas the g enerality of a model could, inter alia, also be det er mined by a number of paramet ers, models with at
least a billion of paramet ers and trained with a larg e amount of data using self-super vision at scale should be
considered to displa y significant generality and to compet ently perf or m a wide rang e of distinctive tasks .
(99) Larg e g enerative AI models are a typical exam ple f or a g eneral-pur pose AI model, given that they allow f or f lexible
g eneration of cont ent, suc h as in the f or m of te xt, audio, imag es or video, that can readily accommodate a wide
rang e of distinctive tasks.
(100) When a g eneral-pur pose AI model is inte grated into or f or ms par t of an AI syste m, this syste m should be considered
t o be g eneral-pur pose AI syste m when, due to this integration, this system has the capability to ser ve a var iety of
pur poses. A general-purpo se AI system can be used directly , or it ma y be integrat ed into other AI syste ms.
EN
OJ L, 12.7.2024
26/144 ELI: http://data.europa.eu/eli/reg/2024/1689/oj(101) Provi ders of general-purpo se AI models hav e a par ticular role and responsibility along the AI value chain, as the
models the y provide ma y f or m the basis f or a rang e of downstream syste ms, of te n provided b y downstream
pro viders that necessitate a good understanding of the models and their capabilities, both t o enable the integration of
suc h models into their products, and to fulfil their obligations under this or other regulations. Theref ore,
propor tionate transparency measures should be laid down, including the drawi ng up and keeping up to date of
documentation, and the provision of inf or mation on the general-pur pose AI model f or its usage b y the downstream
pro viders. T echnical documentation should be prepared and kep t up to dat e by the g eneral-pur pose AI model
pro vider f or the pur pose of making it ava ilable, upon request, t o the AI Offi ce and the national compet ent
author ities. The minimal set of elements to be included in such documentation should be set out in specific annexe s
t o this Regulation. The Commission should be empo wered to amend those annexe s by means of delegat ed acts in
light of evolving t echnological developments.
(102) Sof tware and data, including models, released under a free and open-source licence that allo ws them to be openly
shared and where users can freely access, use, modify and redistr ibute them or modif ied versions thereof, can
contr ibut e to research and inno vation in the mark et and can pro vide significant growth oppor tunities f or the Uni on
economy . General-pur pose AI models released under free and open-source licences should be considered to ensure
high levels of transparency and openness if their parameter s, including the weights, the inf or mation on the model
arc hitecture, and the inf or mation on model usage are made publicly ava ilable. The licence should be considered to be
free and open-source also when it allows users to r un, cop y , distr ibut e, study , ch ange and im prove sof tware and data,
including models under the condition that the or iginal provider of the model is credited, the identical or compar able
t er ms of distr ibution are respected.
(103) Free and open-source AI comp onents covers the sof tware and data, including models and g eneral-pur pose AI
models, tools, ser vices or processes of an AI syste m. Free and open-source AI components can be provided through
diff erent ch annels, including their development on open repositor ies. For the pur poses of this Regulation, AI
com ponents that are provid ed aga inst a pr ice or other wise monetised, including through the provision of t echnical
suppor t or other ser vices, including through a sof tware platf or m, related to the AI comp onent, or the use of
personal data f or reasons other than exclusively f or imp ro ving the secur ity , compatibility or interoperability of the
sof tware, with the ex ception of transactions between microenter pr ises, should not benef it from the excep tions
pro vided to free and open-source AI components. The fact of making AI components availa ble through open
reposit or ies should not, in itself, constitute a monetisation.
(104) The provid ers of g eneral-pur pose AI models that are released under a free and open-source licence, and whose
paramet ers, including the weights, the inf or mation on the model archit ecture, and the inf or mation on model usage,
are made publicly ava ilable should be subject to ex ceptions as rega rds the transparency-related requirements
im posed on g eneral-pur pose AI models, unless they can be considered to present a systemic r isk, in which case the
circumstance that the model is transparent and accompanied by an open-source license should not be considered to
be a suffi cient reason to exclude compliance with the obligati ons under this Regulation. In any case, given that the
release of general-pur pose AI models under free and open-source licence does not necessar ily reveal substantial
inf or mation on the data set used f or the training or fine -tuning of the model and on how compliance of cop yr ight
la w was thereby ensured, the excep tion provid ed f or g eneral-pur pose AI models from compliance with the
transparency-relate d requirements should not concer n the obligati on to produce a summar y about the cont ent used
f or model training and the obligation to put in place a policy to comply with Union cop yr ight law , in par ticular to
identify and comply with the reser vation of r ights pursuant to Ar ticle 4(3) of Directive (EU) 2019/790 of the
European Pa rliament and of the Council (
40
).
(105) General-pur pose AI models, in par ticular larg e g enerative AI models, capable of g enerating text, imag es, and other
cont ent, present unique innovation oppor tunities but also challeng es to ar tists, authors, and other creators and the
wa y their creative content is create d, distr ibut ed, used and consumed. The development and training of suc h models
require access to vast amounts of te xt, images, videos and other data. T ext and data mining te chniques ma y be used
exte nsively in this context f or the retr ieval and analysis of suc h cont ent, which ma y be protect ed by cop yr ight and
relate d r ights. Any use of cop yr ight protect ed cont ent requires the author isation of the r ightsholder concer ned
unless relevant cop yr ight excep tions and limitations apply . Directive (EU) 2019/790 introduced ex ceptions and
limitations allowing reproductions and extractions of wo rk s or other subject matter , f or the pur pose of te xt and data
OJ L, 12.7.2024
EN
ELI: http://data.europa.eu/eli/reg/2024/1689/oj 27/144
(
40
) Directive (EU) 2019/790 of the European Parliament and of the Council of 17 Apr il 2019 on cop yr ight and relat ed r ights in the
Digital Sing le Market and amending Directives 96/9/EC and 2001/29/EC (OJ L 130, 17.5.2019, p. 92).mining, under cer tain conditions. Und er these r ules, r ightsholders ma y choose to reser ve their r ights over their
w ork s or other subject matter to prevent te xt and data mining, unless this is done f or the pur poses of scientific
researc h. Where the r ights to op t out has been expressly reser ved in an appropr iate manner , provid ers of
g eneral-pur pose AI models need to obtain an author isation from r ightsholders if the y want to car r y out t ext and
data mining ove r suc h w ork s.
(106) Provi ders that place g eneral-pur pose AI models on the Union market should ensure com pliance with the relevant
oblig ations in this Regulation. T o that end, providers of g eneral-pur pose AI models should put in place a policy to
com ply with Uni on law on cop yr ight and related r ights, in par ticular to identify and com ply with the reser vation of
r ights expressed b y r ightsholders pursuant to Ar ticle 4(3) of Directive (EU) 2019/790. Any provider placing
a g eneral-pur pose AI model on the Uni on market should comply with this obligation, regard less of the jur isdiction
in which the cop yr ight-relevant acts under pinning the training of those general-pur pose AI models take place. This
is necessar y to ensure a level pla ying fie ld among provider s of gene ral-pur pose AI models where no provider should
be able t o g ain a comp etitive advantage in the Union market by applying lower cop yr ight standards than those
pro vided in the Uni on.
(107) In order to increase transparency on the data that is used in the pre-training and training of general-pur pose AI
models, including text and data protect ed by cop yr ight law , it is adequate that provid ers of suc h models draw up and
mak e publicly a vailable a sufficiently detailed summar y of the content used f or training the g eneral-pur pose AI
model. While taking into due account the need to protect trade secrets and conf idential business inf or mation, this
summar y should be g enerally comp rehensive in its scope instead of te ch nically detailed to facilitat e par ties with
legitimat e inte rests, including cop yr ight holders, t o ex ercise and enf orce their r ights under Uni on law , f or exam ple by
listing the main data collections or sets that went into training the model, suc h as large pr ivate or public databases or
data archives, and by providing a nar rative explanation about other data sources used. It is appropr iat e f or the AI
Office to provid e a tem plate f or the summar y , which should be sim ple, effe ctive, and allo w the provid er t o provide
the required summar y in nar rative f or m.
(108) With rega rd to the obliga tions imp osed on providers of g eneral-pur pose AI models to put in place a policy to
com ply with Uni on cop yr ight law and make publicly a vailable a summar y of the cont ent used f or the training, the AI
Office should monitor whether the provider has fulf illed those obliga tions without ver ifying or proceeding to
a wo rk -by-w ork assessment of the training data in ter ms of cop yr ight comp liance. This Regulation does not affect
the enf orcement of cop yr ight r ules as provided f or under Uni on law .
(109) Comp liance with the obliga tions applicable to the providers of g eneral-pur pose AI models should be commensurate
and propor tionate to the type of model pro vider , ex cluding the need f or compliance f or persons who develop or use
models f or non-profess ional or scientific research pur poses, who should never theless be encouraged to v oluntar ily
com ply with these requirements. Without prejudice to Uni on cop yr ight law , comp liance with those obligations
should take due account of the size of the provider and allow sim plif ied wa ys of compliance f or SMEs, including
star t-ups, that should not represent an ex cessive cost and not discourage the use of suc h models. In the case of
a modifi cation or fine -tuning of a model, the obligations f or provid ers of g eneral-pur pose AI models should be
limit ed to that modifi cation or fine-tuning, f or exam ple by complementing the already existing t echnical
documentation with inf or mation on the modifi cations, including new training data sources, as a means to com ply
with the value ch ain oblig ations provided in this Regulation.
(110) General-pur pose AI models could pose systemic r isks which include, but are not limit ed t o, any actual or reasonably
f oreseeable nega tive effe cts in relation to major accidents, disr up tions of cr itical sectors and ser ious consequences to
public health and saf ety ; any actual or reasonably f oreseeable negative effe cts on democratic processes, public and
economic secur ity ; the dissemination of illega l, false, or discr iminat or y cont ent. Systemic r isks should be understood
t o increase with model capabilities and model reach, can ar ise along the entire lifecy cle of the model, and are
inf luenced by conditions of misuse, model reliability , model f air ness and model secur ity , the level of autonom y of
EN
OJ L, 12.7.2024
28/144 ELI: http://data.europa.eu/eli/reg/2024/1689/ojthe model, its access to too ls, novel or combined modalities, release and distr ibution strategies, the potential to
remo ve guardrails and other f actor s. In par ticular , inte r national approac hes have so f ar identifie d the need to pa y
atte ntion to r isks from pote ntial intent ional misuse or uninte nded issues of control relating to alignment with
human intent; chemical, biological, radiological, and nuclear r isks , suc h as the wa ys in which bar r iers to entr y can be
lo wered, including f or weapons development, design acquisition, or use; offe nsive cyber capabilities, suc h as the
wa ys in vulnerability discove r y , exploitation, or operational use can be enabled; the eff ects of interac tion and to ol
use, including f or exam ple the capacity to control physical systems and interfere with cr itical infrastr ucture; r isks
from models of making copies of themselves or ‘self-replicating’ or training other models; the wa ys in which models
can give r ise to har mful bias and discr imination with r isks to individuals, communities or societies; the facili tation of
disinf or mation or har ming pr ivacy with threats to democratic values and human r ights; r isk that a par ticular event
could lead to a ch ain reaction with considerable nega tive effe cts that could affe ct up to an entire city , an entire
domain activity or an entire community .
(111) It is appropr iate to establish a methodology f or the classif ication of g eneral-pur pose AI models as g eneral-pur pose
AI model with systemic r isks . Since syste mic r isks result from par ticularly high capabilities, a g eneral-pur pose AI
model should be considered to present systemic r isks if it has high-impact capabilities, evaluate d on the basis of
appropr iat e te chnical to ols and methodologies, or significant impact on the internal market due to its reac h.
High-im pact capabilities in general-pur pose AI models means capabilities that matc h or ex ceed the capabilities
recorded in the most advanced general-pur pose AI models. The full range of capabilities in a model could be bett er
understo od af te r its placing on the marke t or when deplo y ers interact with the model. A ccording to the state of the
ar t at the time of entr y into f orce of this Regulation, the cumulative amount of comp utation used f or the training of
the general-pur pose AI model measured in f loating point operations is one of the relevant appro ximations f or model
capabilities. The cumulative amount of comp utation used f or training includes the computation used across the
activities and methods that are intende d to enhance the capabilities of the model pr ior to deplo yment, suc h as
pre-training, synthetic data generation and fine -tuning. Theref ore, an initial threshold of f loating point operations
should be set, which , if met by a g eneral-pur pose AI model, leads to a presump tion that the model is
a general-pur pose AI model with systemic r isks . This threshold should be adjuste d ove r time to ref lect te chnological
and industr ial chang es, suc h as algor ithmic im provements or increased hardware effi ciency , and should be
supplement ed with bench marks and indicator s f or model capability . T o inf or m this, the AI Offi ce should enga ge
with the scientific community , industr y , civil society and other exper ts. Thresholds, as well as too ls and bench mark s
f or the assessment of high-imp act capabilities, should be strong predict ors of g enerality , its capabilities and
associat ed systemic r isk of general-pur pose AI models, and could take into account the wa y the model will be placed
on the market or the number of users it ma y affect. T o com plement this system, there should be a possibility f or the
Commission to take individual decisions designating a g eneral-pur pose AI model as a g eneral-pur pose AI model
with systemic r isk if it is f ound that suc h model has capabilities or an im pact equivalent to those capture d by the set
threshold. That decision should be take n on the basis of an overa ll assessment of the cr ite r ia f or the designation of
a g eneral-pur pose AI model with systemic r isk set out in an annex to this Regulation, such as quality or size of the
training data set, number of business and end users, its input and output modalities, its level of autonom y and
scalability , or the to ols it has access to. Upon a reasoned request of a provider whose model has been designated as
a g eneral-pur pose AI model with systemic r isk, the Commission should take the request into account and ma y
decide to reassess whether the general-pur pose AI model can still be considered to present syste mic r isks.
(112) It is also necessar y to clar ify a procedure f or the classific ation of a g eneral-pur pose AI model with systemic r isks .
A g eneral-pur pose AI model that meets the applicable threshold f or high-imp act capabilities should be presumed to
be a g eneral-pur pose AI models with syste mic r isk. The provid er should notify the AI Offi ce at the latest tw o weeks
af te r the requirements are met or it becomes kno wn that a general-pur pose AI model will meet the requirements
that lead to the presump tion. This is especially relevant in relation to the threshold of f loating point operations
because training of gene ral-pur pose AI models take s considerable planning which includes the upfront allocation of
com pute resources and, theref ore, provid ers of g eneral-pur pose AI models are able to kno w if their model w ould
meet the threshold bef ore the training is complet ed. In the cont ext of that notif ication, the provid er should be able to
demonstrat e that, because of its specif ic ch aracter istics, a g eneral-pur pose AI model ex ceptionally does not present
syste mic r isks , and that it thus should not be classified as a g eneral-pur pose AI model with syste mic r isk s. That
inf or mation is valuable f or the AI Off ice to anticipate the placing on the market of general-pur pose AI models with
syste mic r isks and the provider s can star t to engag e with the AI Office early on. That inf or mation is especially
OJ L, 12.7.2024
EN
ELI: http://data.europa.eu/eli/reg/2024/1689/oj 29/144im por tant with regard to general-pur pose AI models that are planned t o be released as open-source, given that, af ter
the open-source model release, necessar y measures to ensure compliance with the obliga tions under this Regulation
ma y be more diffic ult to imp lement.
(113) If the Commission becomes awar e of the f act that a general-pur pose AI model meets the requirements to classify as
a g eneral-pur pose AI model with syste mic r isk, which previously had either not been kno wn or of which the
relevant provider has f ailed to notify the Commission, the Commission should be empo wered to designate it so.
A system of qualified aler ts should ensure that the AI Offi ce is made aw are by the scientifi c panel of g eneral-pur pose
AI models that should possibly be classified as g eneral-pur pose AI models with syste mic r isk, in addition to the
monito r ing activities of the AI Off ice.
(114) The pro viders of g eneral-pur pose AI models presenting syste mic r isks should be subject, in addition t o the
oblig ations provid ed f or provid ers of g eneral-pur pose AI models, to oblig ations aimed at identifying and mitig ating
those r isks and ensur ing an adequate level of cybersecur ity protection, regardless of whether it is provided as
a standalone model or embedded in an AI system or a product. T o achi eve those objectives, this Regulation should
require provid ers to perfo r m the necessar y model evaluations, in par ticular pr ior t o its fir st placing on the mark et,
including conducting and documenting adversar ial t esting of models, also, as appropr iate, through inter nal or
independent exte r nal testing. In addition, provider s of g eneral-pur pose AI models with systemic r isks should
continuously assess and mitig ate systemic r isks, including f or exam ple by putting in place r isk -management policies,
suc h as accountability and gover nance processes, imp lementing post-mark et monitoring, taking appropr iate
measures along the entire model’s lif ecy cle and cooperating with relevant actor s along the AI value chai n.
(115) Provi ders of general-pur pose AI models with systemic r isks should assess and mitiga te possible systemic r isks . If,
despite eff or ts t o identify and prevent r isks related to a g eneral-pur pose AI model that ma y present syste mic r isks ,
the development or use of the model causes a ser ious incident, the general-pur pose AI model provid er should
without undue dela y keep trac k of the incident and repor t any relevant inf or mation and possible cor rective measures
t o the Commission and national compet ent author ities. Fur ther more, provider s should ensure an adequate level of
cybersecur ity prot ection f or the model and its physical infrastr ucture, if appropr iate, along the entire model lifecy cle.
Cybersecur ity protection related to systemic r isks associated with malicious use or attacks should duly consider
accidental model leakage, unauthor ised releases, circum vention of saf ety measures, and defe nce aga inst cyberattacks,
unauthor ised access or model thef t. That prot ection could be facili tated by secur ing model weights, algor ithms,
ser vers, and data sets, suc h as through operational secur ity measures f or inf or mation secur ity , specif ic cybersecur ity
policies, adequat e t echnical and established solutions, and cyber and phys ical access controls, appropr iate to the
relevant circumstances and the r isks inv olved.
(116) The AI Office should encourage and f acilitate the drawing up, review and adap tation of codes of practice, taking into
account inter national approaches. All pro viders of gene ral-pur pose AI models could be invit ed to par ticipate. T o
ensure that the codes of practice ref lect the stat e of the ar t and duly take into account a diverse set of perspectives,
the AI Off ice should collaborate with relevant national comp etent author ities, and could, where appropr iate, consult
with civil society org anisations and other relevant stakeholders and exper ts, including the Scientif ic Panel, f or the
dra wing up of suc h codes. Codes of practice should co ver oblig ations f or provider s of g eneral-pur pose AI models
and of g eneral-pur pose AI models presenting systemic r isks. In addition, as regard s systemic r isks , codes of practice
should help to establish a r isk tax onomy of the type and nature of the syste mic r isks at Union level, including their
sources. Codes of practice should also be f ocused on specific r isk assessment and mitiga tion measures.
(117) The codes of practice should represent a central too l f or the proper compliance with the obliga tions provided f or
under this Regulation f or provider s of general-pur pose AI models. Provi ders should be able to rely on codes of
practice to demonstrate compliance with the obliga tions. By means of imp lementing acts, the Commission ma y
decide to approve a code of practice and give it a g eneral validity within the Uni on, or , alt er natively , to provide
common r ules f or the imp lementation of the relevant obliga tions, if, by the time this Regulation becomes applicable,
a code of practice cannot be final ised or is not deemed adequate by the AI Office. Once a har monised standard is
EN
OJ L, 12.7.2024
30/144 ELI: http://data.europa.eu/eli/reg/2024/1689/ojpublished and assessed as suitable to cover the relevant oblig ations by the AI Office, comp liance with a European
har monised standard should grant providers the presump tion of conf or mity . Provi ders of general-pur pose AI
models should fur ther more be able to demonstrate comp liance using alter native adequate means, if codes of practice
or har monised standards are not a vailable, or they choose not to rely on those.
(118) This Regulation regulate s AI systems and AI models b y imposing cer tain requirements and obliga tions f or relevant
mark et actor s that are placing them on the market, putting into ser vice or use in the Union, thereby complementing
oblig ations f or provid ers of inter mediar y ser vices that embed suc h systems or models into their ser vices regulate d by
Regulation (EU) 2022/2065. T o the extent that suc h syste ms or models are embedded into designated ver y larg e
online platf or ms or ver y larg e online search engines, the y are subject t o the r isk -manag ement framew ork provid ed
f or in Regulation (EU) 2022/2065. Consequently , the cor responding obliga tions of this Regulation should be
presumed to be fulfilled, unless significant syste mic r isks not co vered by Regulation (EU) 2022/2065 emerg e and are
identified in suc h models. Within this framework, pro viders of ver y larg e online platf or ms and ver y larg e online
searc h engines are oblige d to assess potential systemic r isks st emming from the design, functioning and use of their
ser vices, including how the design of algor ithmic systems used in the ser vice ma y contr ibut e to such r isks, as well as
syste mic r isks st emming from potent ial misuses. Those provid ers are also oblige d to take appropr iate mitig ating
measures in obser vance of fundamental r ights.
(119) Consider ing the quick pace of inno vation and the te chnologi cal evolution of digital ser vices in scope of different
instr uments of Uni on law in par ticular hav ing in mind the usage and the percep tion of their recipients, the AI
syste ms subject to this Regulation ma y be provid ed as interm ediar y ser vices or par ts thereof within the meaning of
Regulation (EU) 2022/2065, which should be inter prete d in a te chnology-neutral manner . For exam ple, AI syste ms
ma y be used to provide online search engines, in par ticular , to the exte nt that an AI syste m suc h as an online ch atbot
perf or ms search es of, in pr inciple, all websites, then incor porates the results into its existing knowle dge and uses the
updat ed knowledg e to g enerate a sing le output that combines different sources of inf or mation.
(120) Fur ther more, obliga tions placed on provid ers and deplo yers of cer tain AI syste ms in this Regulation to enable the
det ection and disclosure that the outputs of those syste ms are ar tif icially generat ed or manipulated are par ticularly
relevant to f acilitate the effe ctive imp lementation of Regulation (EU) 2022/2065. This applies in par ticular as regard s
the obliga tions of provid ers of ver y larg e online platf or ms or ver y large online search engines to identify and
mitig ate syste mic r isks that ma y ar ise from the dissemination of content that has been ar tif icially g enerated or
manipulat ed, in par ticular r isk of the actual or f oreseeable negati ve effe cts on democratic processes, civic discourse
and elect oral processes, including through disinf or mation.
(121) Standardisation should pla y a k ey role to provid e tec hnical solutions to provider s to ensure comp liance with this
Regulation, in line with the state of the ar t, to promote inno vation as well as comp etitiveness and gro wth in the
sing le market. Comp liance with har monised standards as defined in Ar ticle 2, point (1)(c), of Regulation (EU)
No 1025/2012 of the European Parliament and of the Council (
41
), which are nor mally expected to ref lect the state
of the ar t, should be a means f or providers to demonstrate conf or mity with the requirements of this Regulation.
A balanced representation of inte rests in volving all relevant stak eholders in the development of standards, in
par ticular SMEs, consumer organisations and envir onmental and social stakeholders in accordance with Ar ticles 5
and 6 of Regulation (EU) No 1025/2012 should theref ore be encouraged. In order to f acilitate compliance, the
standardisation requests should be issued by the Commission without undue dela y . When prepar ing the
standardisation request, the Commission should consult the advisor y f or um and the Board in order to collect
relevant exper tise. However , in the absence of relevant refere nces to har monised standards, the Commission should
be able to establish, via implementing acts, and af te r consultation of the advisor y f or um, common specif ications f or
cer tain requirements under this Regulation. The common specif ication should be an ex ceptional f all back solution to
f acilitate the provid er ’s obliga tion to comp ly with the requirements of this Regulation, when the standardisation
request has not been accept ed b y any of the European standardisation org anisations, or when the relevant
har monised standards insufficiently address fundamental r ights concer ns, or when the har monised standards do not
com ply with the request, or when there are dela ys in the adoption of an appropr iate har monised standard. Where
suc h a dela y in the adoption of a har monised standard is due t o the tec hnical complexity of that standard, this should
OJ L, 12.7.2024
EN
ELI: http://data.europa.eu/eli/reg/2024/1689/oj 31/144
(
41
) Regulation (EU) No 1025/2012 of the European Parliament and of the Council of 25 October 2012 on European standardisation,
amending Council Directives 89/686/EEC and 93/15/EEC and Directives 94/9/EC, 94/25/EC, 95/16/EC, 97/23/EC, 98/34/EC,
2004/22/EC, 2007/23/EC, 2009/23/EC and 2009/105/EC of the European Parliament and of the Council and repealing Council
Decision 87/95/EEC and Decision No 1673/2006/EC of the European Parliament and of the Council (OJ L 316, 14.11.2012, p. 12).be considered by the Commission bef ore contem plating the establishment of common specifications. When
developing common specific ations, the Commission is encourage d to cooperate with international par tners and
inte r national standardisation bodies.
(122) It is appropr iate that, without prejudice to the use of har monised standards and common specifications, providers of
a high-r isk AI syste m that has been trained and te sted on data ref lecting the specific geographical, behavi oural,
cont extual or functional setting within which the AI system is inte nded to be used, should be presumed t o comp ly
with the relevant measure provided f or under the requirement on data gove r nance set out in this Regulation.
Without prejudice to the requirements related to robustness and accuracy set out in this Regulation, in accordance
with Ar ticle 54(3) of Regulation (EU) 2019/881, high-r isk AI syste ms that hav e been cer tif ied or f or which
a statement of conf or mity has been issued under a cybersecur ity scheme pursuant to that Regulation and the
refe rences of which hav e been published in the Off icial Jour nal of the Eur opean Union should be presumed to comply
with the cybersecur ity requirement of this Regulation in so f ar as the cybersecur ity cer tif icate or statem ent of
conf or mity or par ts thereof cover the cybersecur ity requirement of this Regulation. This remains without prejudice
t o the v oluntar y nature of that cybersecur ity scheme.
(123) In order to ensure a high level of tr ustw or thiness of high-r isk AI systems, those systems should be subject to
a conf or mity assessment pr ior to their placing on the marke t or putting into ser vice.
(124) It is appropr iate that, in order to minimise the burden on operat ors and av oid an y possible duplication, f or high-r isk
AI syste ms related to products which are cover ed by existing Union har monisation legislation based on the New
Legislative Framewo rk , the compliance of those AI systems with the requirements of this Regulation should be
assessed as par t of the conf or mity assessment already provided f or in that law . The applicability of the requirements
of this Regulation should thus not affect the specif ic logic, methodology or g eneral str ucture of conf or mity
assessment under the relevant Uni on har monisation legislation.
(125) Given the comp lexity of high-r isk AI systems and the r isks that are associate d with them, it is impor tant to develop
an adequate conf or mity assessment procedure f or high-r isk AI syste ms inv olving notified bodies, so-called third
par ty conf or mity assessment. However , given the cur rent exper ience of professional pre-market cer tif iers in the fie ld
of product saf ety and the different nature of r isk s inv olved, it is appropr iate to limit, at least in an initial phase of
application of this Regulation, the scope of application of third-par ty conf or mity assessment f or high-r isk AI
syste ms other than those related to products. Theref ore, the conf or mity assessment of suc h syste ms should be
car r ied out as a g eneral r ule by the provider under its own responsibility , with the only excep tion of AI syste ms
inte nded to be used f or biometr ics.
(126) In order t o car r y out third-par ty conf or mity assessments when so required, notified bodies should be notif ied under
this Regulation by the national comp etent author ities, provided that the y comp ly with a set of requirements, in
par ticular on independence, compet ence, absence of conf licts of interests and suitable cybersecur ity requirements.
Notification of those bodies should be sent by national compet ent author ities t o the Commission and the other
Member Stat es by means of the electronic notif ication to ol developed and managed b y the Commission pursuant to
Ar ticle R23 of Annex I t o Decision No 768/2008/EC.
(127) In line with Uni on commitments under the W orld T rade Organization Agreement on T ec hnical Bar r iers to T rade, it is
adequat e to f acilitate the mutual recognition of conf or mity assessment results produced by comp etent conf or mity
assessment bodies, independent of the ter r itory in which the y are established, provid ed that those conf or mity
assessment bodies established under the law of a third countr y meet the applicable requirements of this Regulation
and the Union has concluded an agreement to that exte nt. In this context, the Commission should actively explore
possible inte r national instr uments f or that pur pose and in par ticular pursue the conclusion of mutual recognition
agreements with third countr ies.
(128) In line with the commonly established notion of substantial modif ication f or products regulate d by Uni on
har monisation legislation, it is appropr iate that whenever a chang e occurs which ma y affe ct the comp liance of
a high-r isk AI system with this Regulation (e.g. ch ange of operating syste m or sof tware arch itec ture), or when the
inte nded pur pose of the syste m ch ange s, that AI syste m should be considered to be a new AI system which should
undergo a new conf or mity assessment. How ever , ch anges occur r ing to the algor ithm and the perf or mance of AI
syste ms which continue to ‘lear n’ af ter being placed on the market or put into ser vice, namely automat ically
adap ting how functions are car r ied out, should not constitute a substantial modifi cation, provid ed that those
ch ange s hav e been pre-dete r mined by the provider and assessed at the moment of the conf or mity assessment.
EN
OJ L, 12.7.2024
32/144 ELI: http://data.europa.eu/eli/reg/2024/1689/oj(129) High-r isk AI systems should bear the CE marking to indicate their conf or mity with this Regulation so that they can
move freely within the intern al mark et. For high-r isk AI systems embedded in a product, a physica l CE marking
should be affixed, and ma y be comp lemented by a digital CE marking. For high-r isk AI systems only provid ed
digitally , a digital CE marking should be used. Member States should not create unjustified obstacles t o the placing
on the marke t or the putting into ser vice of high-r isk AI systems that comp ly with the requirements laid down in
this Regulation and bear the CE marking.
(130) Und er cer tain conditions, rapid a vailability of inno vative tec hnologies ma y be cr ucial f or health and saf ety of
persons, the prot ection of the en vironment and climate ch ange and f or society as a whole. It is thus appropr iat e that
under ex ceptional reasons of public secur ity or prot ection of life and health of natural persons, envir onmental
prot ection and the prot ection of ke y industr ial and infrastr uctural assets, marke t sur veillance author ities could
author ise the placing on the market or the putting into ser vice of AI systems which hav e not undergone
a conf or mity assessment. In duly justified situations, as provided f or in this Regulation, law enf orcement author ities
or civil prot ection author ities ma y put a specific high-r isk AI syste m into ser vice without the author isation of the
mark et sur veillance author ity , provided that suc h author isation is request ed dur ing or af ter the use without undue
dela y .
(131) In order to f acilitate the wo rk of the Commission and the Member States in the AI field as well as to increase the
transparency to wards the public, provider s of high-r isk AI syste ms other than those related to products f alling within
the scope of relevant existing Union har monisation legislation, as well as providers who consider that an AI system
list ed in the high-r isk use cases in an annex to this Regulation is not high-r isk on the basis of a deroga tion, should be
required to register themselves and inf or mation about their AI syste m in an EU database, to be established and
manag ed b y the Commission. Bef ore using an AI syste m listed in the high-r isk use cases in an annex to this
Regulation, deplo y ers of high-r isk AI systems that are public author ities, agencies or bodies, should regist er
themselves in suc h database and select the syste m that they envisag e t o use. Other deplo y ers should be entitled to do
so v oluntar ily . This section of the EU database should be publicly accessible, free of charg e, the inf or mation should
be easily navig able, understandable and machine-r eadable. The EU database should also be user -fr iendly , f or exam ple
b y providing search functionalities, including through ke ywo rds, allowi ng the g eneral public t o find relevant
inf or mation to be submitted upon the registration of high-r isk AI syste ms and on the use case of high-r isk AI
syste ms, set out in an annex to this Regulation, to which the high-r isk AI systems cor respond. Any substantial
modification of high-r isk AI systems should also be registered in the EU database. For high-r isk AI syste ms in the
area of law enf orcement, migration, asylum and border control managem ent, the registration obliga tions should be
fulfilled in a secure non-public section of the EU database. A ccess to the secure non-public section should be str ictly
limit ed to the Commission as well as to market sur veillance author ities with regard to their national section of that
database. High-r isk AI systems in the area of cr itical infrastr ucture should only be register ed at national level. The
Commission should be the controller of the EU database, in accordance with Regulation (EU) 2018/1725. In order
t o ensure the full functionality of the EU database, when deplo yed, the procedure f or setting the database should
include the development of functional specif ications b y the Commission and an independent audit repor t. The
Commission should take into account cybersecur ity r isks when car r ying out its tasks as data controller on the EU
database. In order t o maximise the a vailability and use of the EU database by the public, the EU database, including
the inf or mation made ava ilable through it, should comply with requirements under the Directive (EU) 2019/882.
(132) Cer tain AI systems inte nded to interac t with natural persons or to g enerate cont ent ma y pose specific r isk s of
im personation or decept ion ir respective of whether they qualify as high-r isk or not. In cer tain circumstances, the use
of these systems should theref ore be subject to specif ic transparency obligations without prejudice to the
requirements and obligations f or high-r isk AI systems and subject to target ed ex ceptions to take into account the
special need of la w enf orcement. In par ticular , natural persons should be notified that they are inte racting with an AI
syste m, unless this is obvious from the point of view of a natural person who is reasonably well-inf or med, obser vant
and circumspect taking into account the circumstances and the context of use. When im plementing that obligation,
the ch aracter istics of natural persons belonging to vulnerable groups due to their age or disability should be take n
into account to the exte nt the AI system is intended to interact with those groups as well. Moreove r , natural persons
should be notified when the y are exposed to AI systems that, by processing their biometr ic data, can identify or inf er
the emotions or inte ntions of those persons or assign them to specific cate gor ies. Such specif ic categor ies can relate
t o aspects suc h as sex, age , hair colour , e ye colour , tattoos, personal traits, ethnic or igin, personal pref erences and
inte rests. Such inf or mation and notif ications should be provided in accessible f or mats f or persons with disabilities.
OJ L, 12.7.2024
EN
ELI: http://data.europa.eu/eli/reg/2024/1689/oj 33/144(133) A var iety of AI systems can g enerate larg e quantities of synthetic cont ent that becomes increasing ly hard f or humans
t o distinguish from human-generat ed and authentic cont ent. The wide availa bility and increasing capabilities of
those systems hav e a significant impact on the integr ity and tr ust in the inf or mation ecosystem, raising new r isks of
misinf or mation and manipulation at scale, fraud, im personation and consumer decep tion. In light of those im pacts,
the fast t echnological pace and the need f or new methods and tec hniques t o trace or igin of inf or mation, it is
appropr iat e to require providers of those systems to embed tec hnical solutions that enable marking in a machi ne
readable f or mat and detect ion that the output has been g enerated or manipulate d b y an AI syste m and not a human.
Such t echniques and methods should be sufficiently reliable, inte roperable, effe ctive and robust as far as this is
t echnically feasible, taking into account ava ilable tec hniques or a combination of such te ch niques, suc h as
wat er marks, metadata identifica tions, cr ypt ographic methods f or provin g prove nance and authenticity of cont ent,
logging methods, finger pr ints or other te ch niques, as ma y be appropr iat e. When imp lementing this obligation,
pro viders should also take into account the specif icities and the limitations of the different types of cont ent and the
relevant t echnological and mark et developments in the fie ld, as ref lected in the generally ackno wledged state of the
ar t. Such tec hniques and methods can be im plemented at the level of the AI syste m or at the level of the AI model,
including g eneral-pur pose AI models generating content, thereby facilitating fulfilment of this obliga tion by the
do wnstream provid er of the AI system. T o remain propor tionate , it is appropr iate to en visage that this marking
oblig ation should not cover AI syste ms perf or ming pr imar ily an assistive function f or standard editing or AI syste ms
not substantially alter ing the input data provided b y the deplo y er or the semantics thereof.
(134) Fur ther to the te chnical solutions emp lo y ed b y the provider s of the AI system, deplo yers who use an AI syste m to
g enerate or manipulate imag e, audio or video content that appreciably resembles existing persons, objects, places,
entities or events and would f alsely appear to a person to be authentic or tr uthful (deep f ak es), should also clearly
and distinguishably disclose that the cont ent has been ar tificially create d or manipulated by labelling the AI output
according ly and disclosing its ar tif icial or igin. Compliance with this transparency obligation should not be
inte r preted as indicating that the use of the AI syste m or its output impedes the r ight to freedom of expression and
the r ight to freedom of the ar ts and sciences guarante ed in the Char ter , in par ticular where the content is par t of an
evidently creative, satir ical, ar tistic, fi ctional or analogous work or programme, subject to appropr iate safeguards f or
the r ights and freedoms of third par ties. In those cases, the transparency obliga tion f or deep f ak es set out in this
Regulation is limited to disclosure of the exist ence of suc h g enerated or manipulate d cont ent in an appropr iat e
manner that does not hamper the displa y or enjo yment of the w ork , including its nor mal exploitation and use, while
maintaining the utility and quality of the w ork . In addition, it is also appropr iate t o envisag e a similar disclosure
oblig ation in relation to AI-g enerated or manipulate d te xt to the extent it is published with the pur pose of inf or ming
the public on matters of public interest unless the AI-g enerated content has undergone a process of human review or
edit or ial control and a natural or leg al person holds editor ial responsibility f or the publication of the content.
(135) Without prejudice to the mandat or y nature and full applicability of the transparency oblig ations, the Commission
ma y also encourage and f acilitate the dra wing up of codes of practice at Union level to f acilitate the eff ective
im plementation of the obliga tions regarding the det ection and labelling of ar tif icially g enerated or manipulated
cont ent, including to suppor t practical ar rang ements f or making, as appropr iate, the det ection mec hanisms
accessible and f acilitating cooperation with other actor s along the value chain, disseminating content or chec king its
authenticity and provenance t o enable the public to effe ctively distinguish AI-g enerated cont ent.
(136) The obliga tions placed on provid ers and deplo yers of cer tain AI syste ms in this Regulation t o enable the detection
and disclosure that the outputs of those systems are ar tif icially generat ed or manipulat ed are par ticularly relevant to
f acilitate the eff ective imp lementation of Regulation (EU) 2022/2065. This applies in par ticular as rega rds the
oblig ations of providers of ver y larg e online platf or ms or ver y larg e online search engines to identify and mitigat e
syste mic r isks that ma y ar ise from the dissemination of cont ent that has been ar tif icially g enerated or manipulate d,
in par ticular the r isk of the actual or f oreseeable nega tive effe cts on democratic processes, civic discourse and
elect oral processes, including through disinf or mation. The requirement to label content g enerated by AI syste ms
under this Regulation is without prejudice to the obliga tion in Ar ticle 16(6) of Regulation (EU) 2022/2065 f or
pro viders of hosting ser vices to process notices on illegal content received pursuant to Ar ticle 16(1) of that
Regulation and should not inf luence the assessment and the decision on the illegality of the specif ic content. That
assessment should be perform ed solely with reference to the r ules gover ning the lega lity of the cont ent.
EN
OJ L, 12.7.2024
34/144 ELI: http://data.europa.eu/eli/reg/2024/1689/oj(137) Comp liance with the transparency obliga tions f or the AI syste ms cover ed by this Regulation should not be
inte r preted as indicating that the use of the AI syste m or its output is law ful under this Regulation or other Uni on
and Member Stat e la w and should be without prejudice to other transparency obliga tions f or deplo yers of AI syste ms
laid do wn in Union or national law .
(138) AI is a rapidly developing f amily of tec hnologies that requires regulatory ove rsight and a safe and controlled space
f or exper imentation, while ensur ing responsible innovation and integration of appropr iate safegua rds and r isk
mitig ation measures. T o ensure a lega l framew ork that promotes innovation, is future-proof and resilient to
disr up tion, Member States should ensure that their national compet ent author ities establish at least one AI
regulat or y sandbo x at national level to f acilitate the development and te sting of inno vative AI syste ms under str ict
regulat or y oversight bef ore these systems are placed on the marke t or other wise put into ser vice. Member States
could also fulf il this obligation through par ticipating in already existing regulator y sandbo xe s or establishing jointly
a sandbo x with one or more Member States’ compet ent author ities, insofar as this par ticipation provid es equivalent
level of national coverag e f or the par ticipating Member Stat es. AI regulator y sandbo xe s could be established in
phys ical, digital or hybri d f or m and ma y accommodate physica l as well as digital products. Establishing author ities
should also ensure that the AI regulatory sandbo xe s have the adequate resources f or their functioning, including
financial and human resources.
(139) The objectives of the AI regulato r y sandbo xes should be t o f oster AI inno vation by establishing a controlled
exper imentation and te sting environment in the development and pre-marketing phase with a view to ensur ing
com pliance of the inno vative AI systems with this Regulation and other relevant Union and national law . Moreove r ,
the AI regulato r y sandbo xes should aim t o enhance leg al cer tainty f or inno vators and the compet ent author ities’
o versight and understanding of the oppor tunities, emerging r isks and the im pacts of AI use, to f acilitate regulator y
lear ning f or author ities and under takings, including with a view t o future adap tions of the lega l framew ork, to
suppor t cooperation and the shar ing of best practices with the author ities in volved in the AI regulator y sandbo x,
and t o accelerate access to markets, including by removing bar r iers f or SMEs, including star t-ups. AI regulator y
sandbo xe s should be widely ava ilable throughout the Uni on, and par ticular attention should be given to their
accessibility f or SMEs, including star t-ups. The par ticipation in the AI regulato r y sandbo x should f ocus on issues that
raise lega l uncer tainty f or provid ers and prospective provider s to inno vat e, exper iment with AI in the Uni on and
contr ibut e to evidence-based regulator y lear ning. The super vision of the AI systems in the AI regulator y sandbo x
should theref ore co ver their development, training, testing and validation bef ore the syste ms are placed on the
mark et or put into ser vice, as well as the notion and occur rence of substantial modifi cation that ma y require a new
conf or mity assessment procedure. Any signifi cant r isks identified dur ing the development and te sting of suc h AI
syste ms should result in adequat e mitiga tion and, f ailing that, in the suspension of the development and te sting
process. Where appropr iat e, national com petent author ities establishing AI regulator y sandbo xe s should cooperate
with other relevant author ities, including those super vising the prot ection of fundamental r ights, and could allow f or
the inv olvement of other actor s within the AI ecosystem suc h as national or European standardisation org anisations,
notified bodies, testing and exper imentation f acilities, research and exper imentation labs, European Digital
Inno vation Hubs and relevant stak eholder and civil society organisations. T o ensure unif or m implementation across
the Uni on and economies of scale, it is appropr iate to establish common r ules f or the AI regulator y sandbo xe s ’
im plementation and a framework f or cooperation between the relevant author ities in v olved in the super vision of the
sandbo xe s. AI regulatory sandbo xe s established under this Regulation should be without prejudice to other la w
allo wing f or the establishment of other sandbo xes aiming to ensure comp liance with law other than this Regulation.
Where appropr iate, relevant comp etent author ities in ch arge of those other regulato r y sandbo xes should consider
the benefits of using those sandbo xe s also f or the pur pose of ensur ing comp liance of AI syste ms with this
Regulation. Upon agreement between the national compet ent author ities and the par ticipants in the AI regulator y
sandbo x, te sting in real world conditions ma y also be operated and super vised in the framework of the AI regulator y
sandbo x.
(140) This Regulation should provid e the lega l basis f or the provid ers and prospective provid ers in the AI regulator y
sandbo x to use personal data collect ed f or other pur poses f or developing cer tain AI systems in the public interest
within the AI regulator y sandbo x, only under specif ied conditions, in accordance with Ar ticle 6(4) and Ar ticle 9(2),
point (g), of Regulation (EU) 2016/679, and Ar ticles 5, 6 and 10 of Regulation (EU) 2018/1725, and without
prejudice to Ar ticle 4(2) and Ar ticle 10 of Directive (EU) 2016/680. All other obliga tions of data controllers and
r ights of data subjects under Regulations (EU) 2016/679 and (EU) 2018/1725 and Directive (EU) 2016/680 remain
applicable. In par ticular , this Regulation should not pro vide a lega l basis in the meaning of Ar ticle 22(2), point (b) of
Regulation (EU) 2016/679 and Ar ticle 24(2), point (b) of Regulation (EU) 2018/1725. Providers and prospective
OJ L, 12.7.2024
EN
ELI: http://data.europa.eu/eli/reg/2024/1689/oj 35/144pro viders in the AI regulator y sandbo x should ensure appropr iate saf eguards and cooperate with the compet ent
author ities, including by f ollowing their guidance and acting expeditiously and in good f aith to adequat ely mitigat e
an y identifie d signifi cant r isks to saf ety , health, and fundamental r ights that ma y ar ise dur ing the development,
t esting and exper imentation in that sandbo x.
(141) In order to accelerate the process of development and the placing on the marke t of the high-r isk AI syste ms listed in
an annex to this Regulation, it is impor tant that provid ers or prospective providers of suc h syste ms ma y also benefit
from a specif ic regime f or te sting those systems in real w orld conditions, without par ticipating in an AI regulator y
sandbo x. However , in suc h cases, taking into account the possible consequences of such testing on individuals, it
should be ensured that appropr iate and sufficient guarantees and conditions are introduced by this Regulation f or
pro viders or prospective provider s. Such guarantees should include, inter alia, requesting inf or med consent of
natural persons to par ticipate in te sting in real wo rld conditions, with the ex ception of law enf orcement where the
seeking of inf or med consent would prevent the AI syste m from being te sted. Consent of subjects to par ticipate in
suc h te sting under this Regulation is distinct from, and without prejudice to, consent of data subjects f or the
processing of their personal data under the relevant data protection law . It is also imp or tant to minimise the r isks
and enable ove rsight by compet ent author ities and theref ore require prospective provid ers to hav e a real-wo rld
t esting plan submitted to comp etent mark et sur veillance author ity , regist er the t esting in dedicated sections in the EU
database subject to some limited ex ceptions , set limitations on the per iod f or which the te sting can be done and
require additional safegua rds f or persons belonging to cer tain vulnerable groups, as well as a wr itten agreement
defining the roles and responsibilities of prospective providers and deplo y ers and effe ctive overs ight by compet ent
personnel in volved in the real w orld te sting. Fur ther more, it is appropr iate to envisag e additional saf eguards to
ensure that the predictions, recommendations or decisions of the AI system can be eff ectively reversed and
disreg arded and that personal data is protect ed and is deleted when the subjects hav e withdra wn their consent to
par ticipate in the t esting without prejudice to their r ights as data subjects under the Union data protection law . As
rega rds transfer of data, it is also appropr iate to envis age that data collect ed and processed f or the pur pose of te sting
in real-world conditions should be transf er red to third countr ies only where appropr iat e and applicable saf eguards
under Uni on law are im plemented, in par ticular in accordance with bases f or transf er of personal data under Uni on
la w on data prot ection, while f or non-personal data appropr iate saf eguards are put in place in accordance with
Uni on law , suc h as Regulations (EU) 2022/868 (
42
) and (EU) 2023/2854 (
43
) of the European Parliame nt and of the
Council.
(142) T o ensure that AI leads t o socially and environmentally beneficial outcomes, Member Stat es are encouraged to
suppor t and promote research and development of AI solutions in suppor t of socially and envir onmentally
beneficial outcomes, suc h as AI-based solutions to increase accessibility f or persons with disabilities, tackle
socio-economic inequalities, or meet environmental targets, by allocating sufficient resources, including public and
Uni on funding, and, where appropr iate and pro vided that the eligibility and selection cr ite r ia are fulfilled,
consider ing in par ticular projects which pursue suc h objectives. Such projects should be based on the pr inciple of
inte rdisciplinar y cooperation between AI developers, exper ts on inequality and non-discr imination, accessibility ,
consumer , envir onmental, and digital r ights, as well as academics.
(143) In order to promot e and protect innovation, it is impor tant that the intere sts of SMEs, including star t-ups, that are
pro viders or deplo y ers of AI systems are take n into par ticular account. T o that end, Member Stat es should develop
initiatives, which are targe ted at those operat ors, including on a wareness raising and inf or mation communication.
Member States should provide SMEs, including star t-ups, that hav e a regist ered off ice or a branch in the Uni on, with
pr ior ity access to the AI regulator y sandbo xes provided that they fulf il the eligibility conditions and selection cr ite r ia
and without precluding other providers and prospective providers to access the sandbo xe s provided the same
conditions and cr ite r ia are fulfilled. Member Stat es should utilise existing channels and where appropr iat e, establish
new dedicated channels f or communication with SMEs, including star t-ups, deplo yers, other innovat ors and, as
appropr iat e, local public author ities, to suppor t SMEs throughout their development path by providing guidance
and responding to quer ies about the imp lementation of this Regulation. Where appropr iate, these channels should
w ork tog ether to create synergies and ensure homog eneity in their guidance to SMEs, including star t-ups, and
deplo y ers. Ad ditionally , Member Stat es should f acilitate the par ticipation of SMEs and other relevant stakeholders in
the standardisation development processes. Moreover , the specific inte rests and needs of provid ers that are SMEs,
EN
OJ L, 12.7.2024
36/144 ELI: http://data.europa.eu/eli/reg/2024/1689/oj
(
42
) Regulation (EU) 2022/868 of the European Parl iament and of the Council of 30 Ma y 2022 on European data gover nance and
amending Regulation (EU) 2018/1724 (Data Governanc e A ct) (OJ L 152, 3.6.2022, p. 1).
(
43
) Regulation (EU) 2023/2854 of the European Parl iament and of the Council of 13 December 2023 on har monised r ules on fair
access t o and use of data and amending Regulation (EU) 2017/2394 and Directive (EU) 2020/1828 (Data A ct) (OJ L, 2023/2854,
22.12.2023, ELI: http://data.europa.eu/eli/reg/2023/2854/oj).including star t-ups, should be take n into account when notified bodies set conf or mity assessment f ees. The
Commission should regularly assess the cer tif ication and compliance costs f or SMEs, including star t-ups, through
transparent consultations and should work with Member States to lower suc h costs. For exam ple, translation costs
relate d to mandat or y documentation and communication with author ities ma y constitute a signif icant cost f or
pro viders and other operat ors, in par ticular those of a smaller scale. Member Stat es should possibly ensure that one
of the languag es determin ed and accep ted b y them f or relevant provider s ’ documentation and f or communication
with operat ors is one which is broadly understood by the larg est possible number of cross-border deplo y ers. In order
t o address the specific needs of SMEs, including star t-ups, the Commission should provid e standardised te mplat es f or
the areas covered by this Regulation, upon request of the Board. Additiona lly , the Commission should comp lement
Member States’ eff or ts by providing a sing le inf or mation platf or m with easy-t o-use inf or mation with regard s to this
Regulation f or all provid ers and deplo y ers, by organising appropr iate communication cam paigns to raise a wareness
about the obligations ar ising from this Regulation, and b y evaluating and promoting the conve rgence of best
practices in public procurement procedures in relation to AI systems. Medium-sized ent er pr ises which until recently
qualified as small ent er pr ises within the meaning of the Annex to Commission Recommendation 2003/361/EC (
44
)
should hav e access to those suppor t measures, as those new medium-sized enter pr ises ma y sometimes lack the leg al
resources and training necessar y to ensure proper understanding of, and comp liance with, this Regulation.
(144) In order to promot e and prot ect innovation, the AI-on-demand platf or m, all relevant Uni on funding programmes
and projects, suc h as Digital Europe Programme, Hor izon Europe, im plemented b y the Commission and the Member
Stat es at Uni on or national level should, as appropr iat e, contr ibute to the ac hievement of the objectives of this
Regulation.
(145) In order to minimise the r isks to imp lementation resulting from lack of knowledg e and exper tise in the marke t as
well as to facilitat e comp liance of provid ers, in par ticular SMEs, including star t-ups, and notif ied bodies with their
oblig ations under this Regulation, the AI-on-demand platf or m, the European Digital Inno vation Hubs and the
t esting and exper imentation facilities established by the Commission and the Member States at Uni on or national
level should contr ibute t o the im plementation of this Regulation. Within their respective mission and fie lds of
com petence, the AI-on-demand platf or m, the European Digital Inno vation Hubs and the te sting and
exper imentation F acilities are able t o provide in par ticular tec hnical and scientific suppor t to provider s and
notified bodies.
(146) Moreo ver , in light of the ver y small size of some operat ors and in order to ensure propor tionality regard ing costs of
inno vation, it is appropr iate to allow microenterp r ises to fulf il one of the most costly oblig ations, namely to
establish a quality management syste m, in a sim plif ied manner which would reduce the administrative burden and
the costs f or those ent er pr ises without affe cting the level of prot ection and the need f or comp liance with the
requirements f or high-r isk AI systems. The Commission should develop guidelines to specify the elements of the
quality managem ent syste m t o be fulf illed in this sim plif ied manner b y microent er pr ises.
(147) It is appropr iate that the Commission f acilitates, t o the extent possible, access to testing and exper imentation
f acilities to bodies, groups or laborat or ies established or accredit ed pursuant to an y relevant Union har monisation
legislation and which fulf il tasks in the context of conf or mity assessment of products or devices covered by that
Uni on har monisation legislation. This is, in par ticular , the case as rega rds exper t panels, exper t laborator ies and
refe rence laborator ies in the fie ld of medical devices pursuant to Regulations (EU) 2017/745 and (EU) 2017/746.
(148) This Regulation should establish a gover nance framew ork that both allo ws to coordinate and suppor t the
application of this Regulation at national level, as well as build capabilities at Uni on level and integrat e stakeholders
in the fie ld of AI. The effective im plementation and enf orcement of this Regulation require a gove r nance framework
that allo ws to coordinat e and build up central exper tise at Uni on level. The AI Off ice was established by Commission
Decision (
45
) and has as its mission to develop Union exper tise and capabilities in the fie ld of AI and to contr ibute to
the imp lementation of Uni on law on AI. Member Stat es should f acilitate the tasks of the AI Offi ce with a view to
suppor t the development of Uni on exper tise and capabilities at Uni on level and to strengthen the functioning of the
digital sing le market. Fur ther more, a Board composed of representatives of the Member Stat es, a scientific panel to
inte grate the scientific community and an advisor y f or um to contr ibute stakeh older in put to the imp lementation of
this Regulation, at Uni on and national level, should be established. The development of Uni on exper tise and
OJ L, 12.7.2024
EN
ELI: http://data.europa.eu/eli/reg/2024/1689/oj 37/144
(
44
) Commission Recommendation of 6 Ma y 2003 concer ning the definition of micro, small and medium-sized ent er pr ises (OJ L 124,
20.5.2003, p. 36).
(
45
) Commission Decision of 24.1.2024 establishing the European Ar tificial Intellig ence Office C(2024) 390.capabilities should also include making use of existing resources and exper tise, in par ticular through synergies with
str uctures built up in the cont ext of the Union level enf orcement of other law and synergies with related initiatives at
Uni on level, suc h as the EuroHPC Joint Und er taking and the AI te sting and exper imentation facilities under the
Digital Europe Programme.
(149) In order to f acilitate a smooth, eff ective and har monised implementation of this Regulation a Board should be
established. The Board should ref lect the var ious interests of the AI eco-system and be comp osed of representatives
of the Member Stat es. The Board should be responsible f or a number of advisor y tasks , including issuing opinions,
recommendations, advice or contr ibuting to guidance on matt ers related t o the imp lementation of this Regulation,
including on enf orcement matt ers, tec hnical specif ications or existing standards regarding the requirements
established in this Regulation and pro viding advice t o the Commission and the Member States and their national
com petent author ities on specific questions related to AI. In order to give some f lexibility to Member States in the
designation of their representatives in the Board, suc h representatives ma y be any persons belonging to public
entities who should hav e the relevant compet ences and po wers t o facilitat e coordination at national level and
contr ibut e to the achi evement of the Board’s tasks. The Board should establish two standing sub-groups to provide
a platf or m f or cooperation and exc hang e among market sur veillance author ities and notifying author ities on issues
relate d, respectively , t o marke t sur veillance and notified bodies. The standing subgroup f or marke t sur veillance
should act as the administrative cooperation group (ADCO) f or this Regulation within the meaning of Ar ticle 30 of
Regulation (EU) 2019/1020. In accordance with Ar ticle 33 of that Regulation, the Commission should suppor t the
activities of the standing subgroup f or marke t sur veillance by under taking marke t evaluations or studies, in
par ticular with a view to identifying aspects of this Regulation requir ing specific and urgent coordination among
mark et sur veillance author ities. The Board ma y establish other standing or tem porar y sub-groups as appropr iate f or
the pur pose of examining specific issues. The Board should also cooperate , as appropr iate, with relevant Uni on
bodies, exper ts groups and networks active in the context of relevant Uni on law , including in par ticular those active
under relevant Uni on law on data, digital products and ser vices.
(150) With a view to ensur ing the inv olvement of stakeholders in the imp lementation and application of this Regulation,
an advisor y f or um should be established to advise and pro vide tec hnical exper tise to the Board and the Commission.
T o ensure a var ied and balanced stak eholder representation between commercial and non-commercial interest and,
within the category of commercial interests, with rega rds to SMEs and other under takings, the advisor y f or um
should compr ise inter alia industr y , star t-ups, SMEs, academia, civil society , including the social par tners, as well as
the Fundamental Rights Ag ency , ENISA, the European Committe e f or Standardization (CEN), the European
Committ ee f or Electrotec hnical Standardization (CENELEC) and the European T elecommunications Standards
Institut e (ETSI).
(151) T o suppor t the imp lementation and enf orcement of this Regulation, in par ticular the monitoring activities of the AI
Office as regards g eneral-pur pose AI models, a scientific panel of independent exper ts should be established. The
independent exper ts constituting the scientific panel should be selected on the basis of up-to-da te scientific or
t echnical exper tise in the field of AI and should perf or m their tasks with imp ar tiality , objectivity and ensure the
confidentiality of inf or mation and data obtained in car r ying out their tasks and activities. T o allo w the reinf orcement
of national capacities necessar y f or the effective enf orcement of this Regulation, Member Stat es should be able to
request suppor t from the pool of exper ts constituting the scientific panel f or their enf orcement activities.
(152) In order to suppor t adequate enf orcement as regard s AI systems and reinf orce the capacities of the Member Stat es,
Uni on AI te sting suppor t str uctures should be established and made a vailable to the Member Stat es.
(153) Member States hold a ke y role in the application and enf orcement of this Regulation. In that respect, each Member
Stat e should designate at least one notifying author ity and at least one marke t sur veillance author ity as national
com petent author ities f or the pur pose of super vising the application and implementation of this Regulation.
Member Stat es ma y decide to appoint any kind of public entity to per f or m the tasks of the national compet ent
author ities within the meaning of this Regulation, in accordance with their specific national organisational
ch aracteristics and needs. In order to increase org anisation eff iciency on the side of Member Stat es and to set a sing le
point of contact vis-à-vis the public and other counte r par ts at Member State and Union levels, each Member State
should designate a marke t sur veillance author ity t o act as a sing le point of contact.
EN
OJ L, 12.7.2024
38/144 ELI: http://data.europa.eu/eli/reg/2024/1689/oj(154) The national compet ent author ities should ex ercise their powers independently , imp ar tially and without bias, so as
t o saf eguard the pr inciples of objectivity of their activities and tasks and to ensure the application and
im plementation of this Regulation. The members of these author ities should refrain from any action incom patible
with their duties and should be subject to conf identiality r ules under this Regulation.
(155) In order to ensure that providers of high-r isk AI systems can take into account the exper ience on the use of high-r isk
AI systems f or impro ving their syste ms and the design and development process or can take any possible cor rective
action in a timely manner , all providers should have a post-mark et monitoring syste m in place. Where relevant,
post-mark et monitoring should include an analysis of the inte raction with other AI systems including other devices
and sof tware. P ost-market monito r ing should not cover sensitive operational data of deplo y ers which are la w
enf orcement author ities. This system is also k ey to ensure that the possible r isks emerging from AI systems which
continue t o ‘lear n’ af te r being placed on the marke t or put into ser vice can be more effi ciently and timely addressed.
In this cont ext, providers should also be required to hav e a syste m in place to repor t to the relevant author ities any
ser ious incidents resulting from the use of their AI systems, meaning incident or malfunctioning leading to death or
ser ious damage to health, ser ious and ir reversible disr up tion of the managem ent and operation of cr itical
infrastr ucture, infr ingements of obliga tions under Uni on law intended to prot ect fundamental r ights or ser ious
damage to proper ty or the en vironment.
(156) In order to ensure an appropr iate and effe ctive enf orcement of the requirements and obligations set out by this
Regulation, which is Uni on har monisation legislation, the system of marke t sur veillance and comp liance of products
established by Regulation (EU) 2019/1020 should apply in its entirety . Marke t sur veillance author ities designated
pursuant to this Regulation should hav e all enf orcement po wers laid do wn in this Regulation and in Regulation (EU)
2019/1020 and should ex ercise their powers and car r y out their duties independently , impar tially and without bias.
Although the majorit y of AI syste ms are not subject to specific requirements and obligations under this Regulation,
mark et sur veillance author ities ma y tak e measures in relation to all AI syste ms when the y present a r isk in
accordance with this Regulation. Due to the specif ic nature of Uni on institutions, ag encies and bodies f alling within
the scope of this Regulation, it is appropr iate to designate the European Data Protect ion Super visor as a compet ent
mark et sur veillance author ity f or them. This should be without prejudice to the designation of national compet ent
author ities by the Member Stat es. Market sur veillance activities should not affect the ability of the super vised entities
t o car r y out their tasks independently , when suc h independence is required by Uni on law .
(157) This Regulation is without prejudice to the compet ences, tasks , powers and independence of relevant national public
author ities or bodies which super vise the application of Union law protect ing fundamental r ights, including equality
bodies and data prot ection author ities. Where necessar y f or their mandat e, those national public author ities or
bodies should also hav e access to any documentation create d under this Regulation. A specific saf eguard procedure
should be set f or ensur ing adequat e and timely enf orcement ag ainst AI systems presenting a r isk t o health, saf ety and
fundamental r ights. The procedure f or such AI syste ms presenting a r isk should be applied to high-r isk AI syste ms
presenting a r isk, prohibited systems which hav e been placed on the market, put into ser vice or used in violation of
the prohibited practices laid do wn in this Regulation and AI systems which hav e been made ava ilable in violation of
the transparency requirements laid down in this Regulation and present a r isk.
(158) Uni on financ ial ser vices law includes inte r nal govern ance and r isk -managem ent r ules and requirements which are
applicable to regulate d financial institutions in the course of provision of those ser vices, including when they make
use of AI syste ms. In order to ensure coherent application and enf orcement of the oblig ations under this Regulation
and relevant r ules and requirements of the Uni on financial ser vices leg al acts, the compet ent author ities f or the
super vision and enf orcement of those legal acts, in par ticular compet ent author ities as defined in Regulation (EU)
No 575/2013 of the European Pa rliament and of the Council (
46
) and Directives 2008/48/EC (
47
), 2009/138/EC (
48
),
OJ L, 12.7.2024
EN
ELI: http://data.europa.eu/eli/reg/2024/1689/oj 39/144
(
46
) Regulation (EU) No 575/2013 of the European Parliament and of the Council of 26 June 2013 on pr udential requirements f or credit
institutions and in vestment fir ms and amending Regulation (EU) No 648/2012 (OJ L 176, 27.6.2013, p. 1).
(
47
) Directive 2008/48/EC of the European Parl iament and of the Council of 23 Apr il 2008 on credit agreements f or consumers and
repealing Council Directive 87/102/EEC (OJ L 133, 22.5.2008, p. 66).
(
48
) Directive 2009/138/EC of the European Parl iament and of the Council of 25 No vember 2009 on the taking-up and pursuit of the
business of Insurance and Reinsurance (Solvency II) (OJ L 335, 17.12.2009, p. 1).2013/36/EU (
49
), 2014/17/EU (
50
) and (EU) 2016/97 (
51
) of the European Parliament and of the Council, should be
designate d, within their respective comp etences, as compet ent author ities f or the pur pose of super vising the
im plementation of this Regulation, including f or marke t sur veillance activities, as regard s AI syste ms provid ed or
used by regulated and super vised fi nancial institutions unless Member Stat es decide t o designate another author ity to
fulfil these marke t sur veillance tasks . Those comp etent author ities should hav e all powers under this Regulation and
Regulation (EU) 2019/1020 to enf orce the requirements and obligations of this Regulation, including po wers to
car r y our ex post marke t sur veillance activities that can be integrat ed, as appropr iat e, into their existing super visor y
mec hanisms and procedures under the relevant Union fin ancial ser vices law . It is appropr iate to envisag e that, when
acting as market sur veillance author ities under this Regulation, the national author ities responsible f or the
super vision of credit institutions regulate d under Directive 2013/36/EU, which are par ticipating in the Sing le
Super visor y Mec hanism established by Council Regulation (EU) No 1024/2013 (
52
), should repor t, without dela y , to
the European Central Bank any inf or mation identified in the course of their mark et sur veillance activities that ma y
be of potent ial interest f or the European Central Bank’s pr udential super visor y tasks as specified in that Regulation.
T o fur ther enhance the consistency between this Regulation and the r ules applicable to credit institutions regulated
under Directive 2013/36/EU, it is also appropr iate to inte grate some of the provid ers ’ procedural obligations in
relation to r isk managem ent, post marke ting monitorin g and documentation into the existing obligations and
procedures under Directive 2013/36/EU. In order t o av oid ove rlaps, limit ed deroga tions should also be en visaged in
relation to the quality management system of provid ers and the monitori ng obliga tion placed on deplo yers of
high-r isk AI syste ms to the exte nt that these apply to credit institutions regulated b y Directive 2013/36/EU. The
same regime should apply to insurance and re-insurance under takings and insurance holding companies under
Directive 2009/138/EC and the insurance inter mediar ies under Directive (EU) 2016/97 and other types of financial
institutions subject to requirements regarding inter nal gover nance, ar rang ements or processes established pursuant
t o the relevant Uni on financial ser vices law to ensure consistency and equal treatment in the financ ial sect or .
(159) Each market sur veillance author ity f or high-r isk AI syste ms in the area of biometr ics, as listed in an annex t o this
Regulation insofa r as those syste ms are used f or the pur poses of law enf orcement, migration, asylum and border
control management, or the administration of justice and democratic processes, should hav e effe ctive invest igative
and cor rective powers, including at least the po wer to obtain access to all personal data that are being processed and
t o all inf or mation necessar y f or the perf or mance of its tasks . The market sur veillance author ities should be able to
ex ercise their powers b y acting with complet e independence. Any limitations of their access to sensitive operational
data under this Regulation should be without prejudice to the powers confe r red to them by Directive
(EU) 2016/680. No exclusion on disclosing data t o national data prot ection author ities under this Regulation should
aff ect the cur rent or future powers of those author ities bey ond the scope of this Regulation.
(160) The mark et sur veillance author ities and the Commission should be able to propose joint activities, including joint
in vestig ations, to be conducted by marke t sur veillance author ities or market sur veillance author ities jointly with the
Commission, that hav e the aim of promoting comp liance, identifying non-compliance, raising aw areness and
pro viding guidance in relation to this Regulation with respect t o specific categor ies of high-r isk AI syste ms that are
f ound to present a ser ious r isk across tw o or more Member Stat es. Joint activities to promote compliance should be
car r ied out in accordance with Ar ticle 9 of Regulation (EU) 2019/1020. The AI Office should provid e coordination
suppor t f or joint investig ations.
(161) It is necessar y to clar ify the responsibilities and comp et ences at Uni on and national level as regard s AI syste ms that
are built on g eneral-pur pose AI models. T o av oid ove rlapping comp etences, where an AI syste m is based on
a g eneral-pur pose AI model and the model and system are provid ed b y the same provider , the super vision should
EN
OJ L, 12.7.2024
40/144 ELI: http://data.europa.eu/eli/reg/2024/1689/oj
(
49
) Directive 2013/36/EU of the European Parl iament and of the Council of 26 June 2013 on access to the activity of credit institutions
and the pr udential super vision of credit institutions and in vestment fir ms, amending Directive 2002/87/EC and repealing
Directives 2006/48/EC and 2006/49/EC (OJ L 176, 27.6.2013, p. 338).
(
50
) Directive 2014/17/EU of the European Parl iament and of the Council of 4 Febr uar y 2014 on credit agreements f or consumers
relating to residential immov able proper ty and amending Directives 2008/48/EC and 2013/36/EU and Regulation (EU)
No 1093/2010 (OJ L 60, 28.2.2014, p. 34).
(
51
) Directive (EU) 2016/97 of the European Parliament and of the Council of 20 Januar y 2016 on insurance distr ibution (OJ L 26,
2.2.2016, p. 19).
(
52
) Council Regulation (EU) No 1024/2013 of 15 Oct ober 2013 conf er r ing specific tasks on the European Central Bank concer ning
policies relating t o the pr udential super vision of credit institutions (OJ L 287, 29.10.2013, p. 63).tak e place at Union level through the AI Office, which should have the powers of a market sur veillance author ity
within the meaning of Regulation (EU) 2019/1020 f or this pur pose. In all other cases, national market sur veillance
author ities remain responsible f or the super vision of AI syste ms. How ever , f or g eneral-pur pose AI systems that can
be used directly by deplo yers f or at least one pur pose that is classified as high-r isk, marke t sur veillance author ities
should cooperate with the AI Office to car r y out evaluations of compliance and inf or m the Board and other marke t
sur veillance author ities according ly . Fur ther more, market sur veillance author ities should be able to request
assistance from the AI Off ice where the marke t sur veillance author ity is unable to conclude an invest igation on
a high-r isk AI syste m because of its inability to access cer tain inf or mation related to the g eneral-pur pose AI model
on which the high-r isk AI syste m is built. In suc h cases, the procedure regarding mutual assistance in cross-border
cases in Chapt er VI of Regulation (EU) 2019/1020 should apply mutatis mutandis.
(162) T o mak e best use of the centralised Uni on exper tise and synergies at Uni on level, the po wers of super vision and
enf orcement of the obliga tions on provid ers of g eneral-pur pose AI models should be a comp etence of the
Commission. The AI Office should be able to car r y out all necessar y actions t o monitor the effe ctive im plementation
of this Regulation as rega rds general-pur pose AI models. It should be able to in vestigat e possible infr ingements of
the r ules on providers of general-pur pose AI models both on its ow n initiative, f ollowing the results of its
monito r ing activities, or upon request from market sur veillance author ities in line with the conditions set out in this
Regulation. T o suppor t effe ctive monitoring of the AI Offi ce, it should provide f or the possibility that downstream
pro viders lodg e complaints about possible infr ing ements of the r ules on provid ers of g eneral-pur pose AI models and
syste ms.
(163) With a view to comp lementing the governance systems f or general-pur pose AI models, the scientific panel should
suppor t the monitoring activities of the AI Offi ce and ma y , in cer tain cases, provide qualified aler ts to the AI Off ice
which tr igger f ollow-up s, suc h as invest igations. This should be the case where the scientific panel has reason to
suspect that a general-purpo se AI model poses a concrete and identifi able r isk at Union level. Fur ther more, this
should be the case where the scientific panel has reason to suspect that a g eneral-pur pose AI model meets the cr ite r ia
that wo uld lead to a classif ication as g eneral-pur pose AI model with systemic r isk. T o equip the scientific panel with
the inf or mation necessar y f or the perf or mance of those tasks , there should be a mec hanism whereby the scientific
panel can request the Commission to require documentation or inf or mation from a provid er .
(164) The AI Offi ce should be able t o take the necessar y actions to monitor the eff ective imp lementation of and
com pliance with the obliga tions f or provid ers of g eneral-pur pose AI models laid do wn in this Regulation. The AI
Office should be able to invest igat e possible infr ingements in accordance with the powers provided f or in this
Regulation, including by requesting documentation and inf or mation, b y conducting evaluations, as well as by
requesting measures from provid ers of g eneral-pur pose AI models. When conducting evaluations, in order to make
use of independent exper tise, the AI Off ice should be able to in v olve independent exper ts to car r y out the
evaluations on its behalf. Compliance with the obliga tions should be enf orceable, inter alia, through requests to take
appropr iat e measures, including r isk mitiga tion measures in the case of identifie d syste mic r isks as well as restr icting
the making availa ble on the market, withdra wing or recalling the model. As a saf eguard, where needed bey ond the
procedural r ights provided f or in this Regulation, provid ers of g eneral-pur pose AI models should hav e the
procedural r ights provid ed f or in Ar ticle 18 of Regulation (EU) 2019/1020, which should apply mutatis mutandis,
without prejudice to more specif ic procedural r ights provid ed f or by this Regulation.
(165) The development of AI syste ms other than high-r isk AI systems in accordance with the requirements of this
Regulation ma y lead to a larg er uptak e of ethical and tr ustw or th y AI in the Uni on. Providers of AI syste ms that are
not high-r isk should be encouraged to create codes of conduct, including related gover nance mechanisms, intended
t o f oste r the v oluntar y application of some or all of the mandator y requirements applicable to high-r isk AI systems,
adap te d in light of the intended pur pose of the systems and the low er r isk in volved and taking into account the
a vailable te ch nical solutions and industr y best practices suc h as model and data cards. Provi ders and, as appropr iate,
deplo y ers of all AI systems, high-r isk or not, and AI models should also be encourage d to apply on a v oluntar y basis
additional requirements relate d, f or exam ple, to the elements of the Uni on’s Ethics Guidelines f or T r ustw or th y AI,
OJ L, 12.7.2024
EN
ELI: http://data.europa.eu/eli/reg/2024/1689/oj 41/144en vironmental sustainability , AI literac y measures, inclusive and diverse design and development of AI systems,
including atte ntion to vulnerable persons and accessibility to persons with disability , stakeholders’ par ticipation with
the in volvement, as appropr iate, of relevant stak eholders such as business and civil society organisations, academia,
researc h org anisations, trade unions and consumer prot ection org anisations in the design and development of AI
syste ms, and diversity of the development t eams, including g ender balance. T o ensure that the v oluntar y codes of
conduct are effe ctive, they should be based on clear objectives and ke y perf or mance indicators to measure the
ac hievement of those objectives. They should also be developed in an inclusive wa y , as appropr iate, with the
in v olvement of relevant stakeholders suc h as business and civil society organisations, academia, research
org anisations, trade unions and consumer prot ection organisation. The Commission ma y develop initiatives,
including of a sect oral nature, to f acilitate the lowering of tec hnical bar r iers hinder ing cross-border exc hang e of data
f or AI development, including on data access infrastr ucture, semantic and tec hnical interoperability of different types
of data.
(166) It is im por tant that AI systems relate d to products that are not high-r isk in accordance with this Regulation and thus
are not required to comp ly with the requirements set out f or high-r isk AI systems are never theless safe when placed
on the marke t or put into ser vice. T o contr ibut e to this objective, Regulation (EU) 2023/988 of the European
P arliament and of the Council (
53
) would apply as a saf ety net.
(167) In order to ensure tr ustful and constr uctive cooperation of comp et ent author ities on Uni on and national level, all
par ties inv olved in the application of this Regulation should respect the conf identiality of inf or mation and data
obtained in car r ying out their tasks , in accordance with Uni on or national law . They should car r y out their tasks and
activities in suc h a manner as to prot ect, in par ticular , intellectual proper ty r ights, confi dential business inf or mation
and trade secrets, the effective im plementation of this Regulation, public and national secur ity interests, the integr ity
of cr iminal and administrative proceedings, and the integr ity of classif ied inf or mation.
(168) Comp liance with this Regulation should be enf orceable by means of the imp osition of penalties and other
enf orcement measures. Member States should take all necessar y measures to ensure that the provisions of this
Regulation are implement ed, including b y la ying do wn effe ctive, propor tionate and dissuasive penalties f or their
infr ing ement, and to respect the ne bis in idem pr inciple. In order to strengthen and har monise administrative
penalties f or infr ingem ent of this Regulation, the upper limits f or setting the administrative fine s f or cer tain specific
infr ing ements should be laid do wn. When assessing the amount of the fine s, Member States should, in each
individual case, take into account all relevant circumstances of the specific situation, with due regard in par ticular to
the nature, gra vity and duration of the infr ing ement and of its consequences and to the size of the provid er , in
par ticular if the provider is an SME, including a star t-up. The European Data Protection Super visor should hav e the
po wer to im pose fines on Uni on institutions, agencies and bodies falling within the scope of this Regulation.
(169) Comp liance with the obligations on pro viders of g eneral-pur pose AI models imposed under this Regulation should
be enf orceable, inte r alia, b y means of fines. T o that end, appropr iate levels of fi nes should also be laid do wn f or
infr ing ement of those obligati ons, including the f ailure t o comply with measures request ed by the Commission in
accordance with this Regulation, subject to appropr iate limitation per iods in accordance with the pr inciple of
propor tionality . All decisions taken by the Commission under this Regulation are subject to review by the Cour t of
Justice of the European Uni on in accordance with the TFEU, including the unlimit ed jur isdiction of the Cour t of
Justice with rega rd to penalties pursuant to Ar ticle 261 TFEU.
(170) Uni on and national law already provide eff ective remedies t o natural and leg al persons whose r ights and freedoms
are adversely aff ected by the use of AI syste ms. Without prejudice to those remedies, an y natural or lega l person that
has grounds to consider that there has been an infr ingem ent of this Regulation should be entitled to lodg e
a complaint to the relevant mark et sur veillance author ity .
(171) Aff ecte d persons should hav e the r ight to obtain an explanation where a deplo y er ’s decision is based mainly upon
the output from cer tain high-r isk AI syste ms that f all within the scope of this Regulation and where that decision
produces leg al effe cts or similarly significantly affe cts those persons in a wa y that the y consider to have an adverse
EN
OJ L, 12.7.2024
42/144 ELI: http://data.europa.eu/eli/reg/2024/1689/oj
(
53
) Regulation (EU) 2023/988 of the European Parliament and of the Council of 10 Ma y 2023 on g eneral product safe ty , amending
Regulation (EU) No 1025/2012 of the European Parliament and of the Council and Directive (EU) 2020/1828 of the European
Parl iament and the Council, and repealing Directive 2001/95/EC of the European Parl iament and of the Council and Council
Directive 87/357/EEC (OJ L 135, 23.5.2023, p. 1).im pact on their health, safety or fundamental r ights. That explanation should be clear and meaningful and should
pro vide a basis on which the affe cted persons are able to exercise their r ights. The r ight to obtain an explanation
should not apply to the use of AI syste ms f or which ex ceptions or restr ictions f ollow from Uni on or national law
and should apply only to the extent this r ight is not already provid ed f or under Uni on law .
(172) P ersons acting as whistleblo wers on the infr ing ements of this Regulation should be prot ected under the Union law .
Directive (EU) 2019/1937 of the European Pa rliament and of the Council (
54
) should theref ore apply to the repor ting
of infr ingements of this Regulation and the protect ion of persons repor ting suc h infr ingements.
(173) In order to ensure that the regulato r y framework can be adap te d where necessar y , the power t o adopt acts in
accordance with Ar ticle 290 TFEU should be deleg ated to the Commission t o amend the conditions under which an
AI system is not to be considered to be high-r isk, the list of high-r isk AI syste ms, the provisions regarding te ch nical
documentation, the cont ent of the EU declaration of conf or mity the provisions regard ing the conf or mity assessment
procedures, the provisions establishing the high-r isk AI syste ms t o which the conf or mity assessment procedure
based on assessment of the quality management syste m and assessment of the te ch nical documentation should
apply , the threshold, bench mark s and indicators, including by supplementing those bench mark s and indicator s, in
the r ules f or the classification of general-pur pose AI models with syste mic r isk, the cr ite r ia f or the designation of
g eneral-pur pose AI models with syste mic r isk, the te ch nical documentation f or providers of g eneral-pur pose AI
models and the transparency inf or mation f or providers of g eneral-pur pose AI models. It is of par ticular imp or tance
that the Commission car r y out appropr iate consultations dur ing its preparatory wo rk , including at exper t level, and
that those consultations be conducte d in accordance with the pr inciples laid down in the Inter institutional
Agreement of 13 Apr il 2016 on Better Law -Making (
55
). In par ticular , t o ensure equal par ticipation in the
preparation of delegat ed acts, the European P arliament and the Council receive all documents at the same time as
Member Stat es’ exper ts, and their exper ts syste matically hav e access to meetings of Commission exper t groups
dealing with the preparation of delegat ed acts.
(174) Given the rapid tec hnological developments and the t echnical exper tise required to effe ctively apply this Regulation,
the Commission should evaluate and review this Regulation by 2 Augu st 2029 and ever y f our y ears thereaf te r and
repor t to the European Parliament and the Council. In addition, taking into account the implications f or the scope of
this Regulation, the Commission should car r y out an assessment of the need to amend the list of high-r isk AI
syste ms and the list of prohibited practices once a y ear . Moreove r , by 2 August 2028 and ever y f our y ears thereaf te r ,
the Commission should evaluate and repor t t o the European Parliament and to the Council on the need to amend
the list of high-r isk areas headings in the annex to this Regulation, the AI systems within the scope of the
transparency obligations, the effectiveness of the super vision and gover nance syste m and the progress on the
development of standardisation deliverables on energy eff icient development of g eneral-pur pose AI models,
including the need f or fur ther measures or actions. Finally , by 2 Au gust 2028 and ever y three y ears thereaf te r , the
Commission should evaluate the im pact and effectiveness of v oluntar y codes of conduct to f oste r the application of
the requirements provided f or high-r isk AI systems in the case of AI systems other than high-r isk AI syste ms and
possibly other additional requirements f or suc h AI syste ms.
(175) In order to ensure unif or m conditions f or the imp lementation of this Regulation, impl ementing po wers should be
conf er red on the Commission. Those powers should be ex ercised in accordance with Regulation (EU) No 182/2011
of the European Pa rliament and of the Council (
56
).
(176) Since the objective of this Regulation, namely to imp rove the functioning of the inte r nal mark et and to promot e the
up take of human centr ic and tr ustw or th y AI, while ensur ing a high level of protection of health, saf ety , fundamental
r ights enshr ined in the Char te r , including democracy , the r ule of law and environmental prot ection aga inst har mful
eff ects of AI syste ms in the Uni on and suppor ting inno vation, cannot be sufficiently achieved by the Member Stat es
and can rather , by reason of the scale or effe cts of the action, be bett er ac hieved at Uni on level, the Uni on ma y adop t
OJ L, 12.7.2024
EN
ELI: http://data.europa.eu/eli/reg/2024/1689/oj 43/144
(
54
) Directive (EU) 2019/1937 of the European Parliament and of the Council of 23 Oct ober 2019 on the protection of persons who
repor t breaches of Union la w (OJ L 305, 26.11.2019, p. 17).
(
55
) OJ L 123, 12.5.2016, p. 1.
(
56
) Regulation (EU) No 182/2011 of the European Parliament and of the Council of 16 Febr uar y 2011 la ying down the r ules and
g eneral pr inciples concer ning mechanisms f or control b y Member States of the Commission’s exerci se of im plementing po wers (OJ
L 55, 28.2.2011, p. 13).measures in accordance with the pr inciple of subsidiar ity as set out in Ar ticle 5 TEU. In accordance with the
pr inciple of propor tionality as set out in that Ar ticle, this Regulation does not go bey ond what is necessar y in order
t o achieve that objective.
(177) In order to ensure lega l cer tainty , ensure an appropr iat e adaptation per iod f or operat ors and av oid disr uption to the
mark et, including b y ensur ing continuity of the use of AI syste ms, it is appropr iate that this Regulation applies to the
high-r isk AI syste ms that hav e been placed on the marke t or put into ser vice bef ore the g eneral date of application
thereof, only if, from that date, those systems are subject to signifi cant chang es in their design or intende d pur pose.
It is appropr iate to clar ify that, in this respect, the concept of significant chang e should be understood as equivalent
in substance to the notion of substantial modif ication, which is used with regard only to high-r isk AI syste ms
pursuant t o this Regulation. On an ex ceptiona l basis and in light of public accountability , operat ors of AI syste ms
which are comp onents of the larg e-scale IT systems established b y the lega l acts listed in an annex to this Regulation
and operators of high-r isk AI syste ms that are intended to be used by public author ities should, respectively , take the
necessar y st eps to comp ly with the requirements of this Regulation b y end of 2030 and by 2 August 2030.
(178) Provi ders of high-r isk AI systems are encourage d to star t to comply , on a v oluntar y basis, with the relevant
oblig ations of this Regulation already dur ing the transitional per iod.
(179) This Regulation should apply from 2 August 2026. How ever , taking into account the unaccepta ble r isk associated
with the use of AI in cer tain wa ys, the prohibitions as well as the g eneral provis ions of this Regulation should already
apply from 2 Febr uar y 2025. While the full effe ct of those prohibitions f ollows with the establishment of the
gove r nance and enf orcement of this Regulation, anticipating the application of the prohibitions is imp or tant to take
account of unaccepta ble r isks and t o hav e an effect on other procedures, suc h as in civil law . Moreove r , the
infrastr ucture related to the gove r nance and the conf or mity assessment syste m should be operational bef ore
2 August 2026, theref ore the provisions on notified bodies and governance str ucture should apply from 2 August
2025. Given the rapid pace of t echnological advancements and adop tion of general-pur pose AI models, obligations
f or provid ers of g eneral-pur pose AI models should apply from 2 Au gust 2025. Codes of practice should be ready by
2 Ma y 2025 in view of enabling provid ers to demonstrate com pliance on time. The AI Office should ensure that
classification r ules and procedures are up to date in light of te chnologi cal developments. In addition, Member Stat es
should la y down and notify to the Commission the r ules on penalties, including administrative fines, and ensure that
the y are properly and effe ctively im plemented by the date of application of this Regulation. Theref ore the provis ions
on penalties should apply from 2 Au gust 2025.
(180) The European Data Protection Super visor and the European Data Prot ection Board were consult ed in accordance
with Ar ticle 42(1) and (2) of Regulation (EU) 2018/1725 and delivered their joint opinion on 18 June 2021,
HA VE ADOPTED THIS REGUL A TION:
CHAPTER I
GENERAL PR O VISIONS
Ar ticle 1
Subject matter`
1. The pur pose of this Regulation is to impro ve the functioning of the inter nal marke t and promote the up take of
human-centr ic and tr ustw or thy ar tif icial intellig ence (AI), while ensur ing a high level of prot ection of health, saf ety ,
fundamental r ights enshr ined in the Char te r , including democracy , the r ule of la w and environmental protection, aga inst
the har mful eff ects of AI systems in the Uni on and suppor ting inno vation.
2. This Regulation la ys do wn:
(a) har monised r ules f or the placing on the marke t, the putting into ser vice, and the use of AI systems in the Uni on;
EN
OJ L, 12.7.2024
44/144 ELI: http://data.europa.eu/eli/reg/2024/1689/oj(b) prohibitions of cer tain AI practices;
(c) specific requirements f or high-r isk AI systems and oblig ations f or operators of suc h systems;
(d) har monised transparency r ules f or cer tain AI systems;
(e) har monised r ules f or the placing on the marke t of general-pur pose AI models;
(f) r ules on mark et monitoring, marke t sur veillance, gove r nance and enf orcement ;
(g) measures to suppor t innovation, with a par ticular f ocus on SMEs, including star t-ups.
Ar ticle 2
Scope
1. This Regulation applies to:
(a) provid ers placing on the marke t or putting into ser vice AI syste ms or placing on the market general-pur pose AI models
in the Uni on, ir respective of whether those pro viders are established or locat ed within the Union or in a third countr y ;
(b) deplo yers of AI syste ms that have their place of establishment or are located within the Uni on;
(c) provid ers and deplo y ers of AI syste ms that hav e their place of establishment or are locate d in a third countr y , where the
output produced by the AI system is used in the Union;
(d) imp or te rs and distr ibut ors of AI systems;
(e) product manufacturers placing on the marke t or putting into ser vice an AI system tog ether with their product and
under their o wn name or trademark;
(f) author ised representatives of provider s, which are not established in the Uni on;
(g) affe cted persons that are located in the Union.
2. For AI syste ms classif ied as high-r isk AI systems in accordance with Ar ticle 6(1) related to products covered by the
Uni on har monisation legislation listed in Section B of Annex I, only Ar ticle 6(1), Ar ticles 102 to 109 and Ar ticle 112 apply .
Ar ticle 57 applies only in so f ar as the requirements f or high-r isk AI systems under this Regulation have been integrat ed in
that Union har monisation legislation.
3. This Regulation does not apply to areas outside the scope of Union law , and shall not, in any event, affe ct the
compet ences of the Member States concer ning national secur ity , regardless of the type of entity entr uste d by the Member
Stat es with car r ying out tasks in relation to those comp etences.
This Regulation does not apply to AI syste ms where and in so far they are placed on the marke t, put into ser vice, or used
with or without modif ication exclusively f or militar y , defe nce or national secur ity pur poses, rega rdless of the type of entity
car r ying out those activities.
This Regulation does not apply t o AI systems which are not placed on the market or put into ser vice in the Union, where
the output is used in the Uni on ex clusively f or militar y , defe nce or national secur ity pur poses, regardless of the type of
entity car r ying out those activities.
4. This Regulation applies neither to public author ities in a third countr y nor to inte r national org anisations f alling
within the scope of this Regulation pursuant to paragraph 1, where those author ities or organisations use AI syste ms in the
framew ork of international cooperation or agreements f or law enf orcement and judicial cooperation with the Union or
with one or more Member Stat es, provided that suc h a third countr y or intern ational org anisation provides adequate
saf eguards with respect t o the prot ection of fundamental r ights and freedoms of individuals.
5. This Regulation shall not affect the application of the provisions on the liability of provider s of interm ediar y ser vices
as set out in Chapt er II of Regulation (EU) 2022/2065.
OJ L, 12.7.2024
EN
ELI: http://data.europa.eu/eli/reg/2024/1689/oj 45/1446. This Regulation does not apply to AI systems or AI models, including their output, specif ically developed and put into
ser vice f or the sole pur pose of scientifi c researc h and development.
7. Union law on the prot ection of personal data, pr ivacy and the conf identiality of communications applies to personal
data processed in connection with the r ights and obliga tions laid down in this Regulation. This Regulation shall not affect
Regulation (EU) 2016/679 or (EU) 2018/1725, or Directive 2002/58/EC or (EU) 2016/680, without prejudice to Ar ticle
10(5) and Ar ticle 59 of this Regulation.
8. This Regulation does not apply to any researc h, te sting or development activity regard ing AI syste ms or AI models
pr ior to their being placed on the marke t or put into ser vice. Such activities shall be conducted in accordance with
applicable Union law . T esting in real world conditions shall not be co vered by that ex clusion.
9. This Regulation is without prejudice t o the r ules laid do wn by other Uni on leg al acts related to consumer prot ection
and product safety .
10. This Regulation does not apply t o obligations of deplo y ers who are natural persons using AI syste ms in the course of
a purely personal non-professional activity .
11. This Regulation does not preclude the Uni on or Member States from maintaining or introducing la ws, regulations or
administrative provis ions which are more fa vourable to w orkers in te r ms of protect ing their r ights in respect of the use of
AI systems by em plo y ers, or from encouraging or allowi ng the application of collective agreements which are more
f av ourable to wo rkers.
12. This Regulation does not apply to AI systems released under free and open-source licences, unless they are placed on
the mark et or put into ser vice as high-r isk AI systems or as an AI system that f alls under Ar ticle 5 or 50.
Ar ticle 3
Def initions
For the pur poses of this Regulation, the f ollowing def initions apply:
(1) ‘ AI syste m’ means a machine-based system that is designed to operat e with var ying levels of autonom y and that ma y
exhibit adap tiveness af te r deplo yment, and that, f or explicit or implicit objectives, inf ers, from the input it receives,
ho w to g enerate outputs such as predictions, content, recommendations, or decisions that can inf luence phys ical or
vir tual environments;
(2) ‘r isk’ means the combination of the probability of an occur rence of har m and the sever ity of that har m;
(3) ‘provi der ’ means a natural or lega l person, public author ity , age ncy or other body that develops an AI system or
a general-pur pose AI model or that has an AI syste m or a g eneral-pur pose AI model developed and places it on the
mark et or puts the AI syste m into ser vice under its own name or trademark, whether f or pa yment or free of charg e;
(4) ‘deplo yer ’ means a natural or lega l person, public author ity , age ncy or other body using an AI system under its
author ity ex cept where the AI system is used in the course of a personal non-professional activity ;
(5) ‘author ised representative ’ means a natural or leg al person located or established in the Uni on who has received and
accep te d a wr itten mandate from a provider of an AI system or a g eneral-pur pose AI model to, respectively , perf or m
and car r y out on its behalf the obliga tions and procedures established by this Regulation;
(6) ‘imp or ter ’ means a natural or lega l person locate d or established in the Uni on that places on the market an AI system
that bears the name or trademark of a natural or lega l person established in a third countr y ;
(7) ‘distr ibutor ’ means a natural or leg al person in the supply chain, other than the pro vider or the im por ter , that mak es
an AI syste m available on the Union marke t ;
(8) ‘operat or ’ means a provid er , product manufa cturer , deplo y er , author ised representative, imp or ter or distr ibut or;
EN
OJ L, 12.7.2024
46/144 ELI: http://data.europa.eu/eli/reg/2024/1689/oj(9) ‘placing on the marke t ’ means the fir st making available of an AI system or a general-purpo se AI model on the Uni on
mark et ;
(10) ‘making ava ilable on the market’ means the supply of an AI syste m or a g eneral-pur pose AI model f or distr ibution or
use on the Uni on market in the course of a commercial activity , whether in retur n f or pa yment or free of ch arg e;
(11) ‘putting into ser vice ’ means the supply of an AI system f or first use directly to the deplo y er or f or ow n use in the Uni on
f or its intended pur pose;
(12) ‘int ended pur pose’ means the use f or which an AI syste m is intended by the provider , including the specif ic cont ext
and conditions of use, as specif ied in the inf or mation supplied by the provid er in the instr uctions f or use, promotional
or sales materials and stat ements, as well as in the tec hnical documentation;
(13) ‘reasonably f oreseeable misuse’ means the use of an AI syste m in a wa y that is not in accordance with its intended
pur pose, but which ma y result from reasonably f oreseeable human behavi our or interaction with other systems,
including other AI systems;
(14) ‘saf ety component’ means a component of a product or of an AI system which fulfils a safety function f or that product
or AI system, or the f ailure or malfunctioning of which endang ers the health and safety of persons or proper ty ;
(15) ‘instr uctions f or use ’ means the inf or mation provided by the provider t o inf or m the deplo y er of, in par ticular , an AI
syste m’s intended pur pose and proper use;
(16) ‘recall of an AI syste m’ means an y measure aiming to ac hieve the retur n to the provider or taking out of ser vice or
disabling the use of an AI system made a vailable to deplo yers;
(17) ‘withdra wal of an AI system’ means any measure aiming to prevent an AI system in the supply ch ain being made
a vailable on the mark et ;
(18) ‘perf or mance of an AI syste m’ means the ability of an AI syste m t o achieve its intended pur pose;
(19) ‘notifying author ity’ means the national author ity responsible f or setting up and car r ying out the necessar y procedures
f or the assessment, designation and notification of conf or mity assessment bodies and f or their monito r ing;
(20) ‘conf or mity assessment’ means the process of demonstrating whether the requirements set out in Chapt er III, Section 2
relating to a high-r isk AI system have been fulfilled;
(21) ‘conf or mity assessment body’ means a body that perfo r ms third-par ty conf or mity assessment activities, including
t esting, cer tif ication and inspection;
(22) ‘notified body’ means a conf or mity assessment body notif ied in accordance with this Regulation and other relevant
Uni on har monisation legislation;
(23) ‘substantial modifi cation’ means a ch ange to an AI syste m af te r its placing on the marke t or putting into ser vice which
is not f oreseen or planned in the initial conf or mity assessment car r ied out b y the provid er and as a result of which the
com pliance of the AI system with the requirements set out in Chap ter III, Section 2 is affe cted or results in
a modif ication to the inte nded pur pose f or which the AI syste m has been assessed;
(24) ‘CE marking’ means a marking b y which a provid er indicates that an AI system is in conf or mity with the requirements
set out in Chapt er III, Section 2 and other applicable Uni on har monisation legislation pro viding f or its affixing;
(25) ‘post-mark et monitoring syste m’ means all activities car r ied out by provid ers of AI systems to collect and review
exper ience gained from the use of AI systems they place on the marke t or put into ser vice f or the pur pose of
identifying any need to immediate ly apply any necessar y cor rective or preventive actions;
(26) ‘marke t sur veillance author ity’ means the national author ity car r ying out the activities and taking the measures
pursuant to Regulation (EU) 2019/1020;
OJ L, 12.7.2024
EN
ELI: http://data.europa.eu/eli/reg/2024/1689/oj 47/144(27) ‘har monised standard’ means a har monised standard as def ined in Ar ticle 2(1), point (c), of Regulation (EU)
No 1025/2012;
(28) ‘common specif ication’ means a set of te chnical specif ications as defined in Ar ticle 2, point (4) of Regulation (EU)
No 1025/2012, providing means to compl y with cer tain requirements established under this Regulation;
(29) ‘training data’ means data used f or training an AI system through fi tting its lear nable paramet ers;
(30) ‘validation data’ means data used f or providing an evaluation of the trained AI system and f or tuning its non-lear nable
paramet ers and its lear ning process in order , inter alia, to prevent under fit ting or ove r fitting;
(31) ‘validation data set’ means a separate data set or par t of the training data set, either as a fixed or var iable split ;
(32) ‘te sting data’ means data used f or providing an independent evaluation of the AI system in order t o conf ir m the
expect ed perform ance of that syste m bef ore its placing on the market or putting into ser vice;
(33) ‘in put data’ means data provided to or directly acquired by an AI syste m on the basis of which the syste m produces an
output ;
(34) ‘biometr ic data’ means personal data resulting from specific te chnical processing relating t o the physical , physiolo gical
or behavi oural ch aracter istics of a natural person, such as facial images or dactyloscopic data;
(35) ‘biometr ic identifica tion’ means the automat ed recognition of physical, physiolo gical, behavioural, or psy chological
human features f or the pur pose of establishing the identity of a natural person b y comp ar ing biometr ic data of that
individual to biometr ic data of individuals st ored in a database;
(36) ‘biometr ic ver ificati on’ means the automa ted, one-to-one ver ification, including authentication, of the identity of
natural persons by compari ng their biometr ic data to previously provided biometr ic data;
(37) ‘special categori es of personal data’ means the categori es of personal data refer red to in Ar ticle 9(1) of Regulation (EU)
2016/679, Ar ticle 10 of Directive (EU) 2016/680 and Ar ticle 10(1) of Regulation (EU) 2018/1725;
(38) ‘sensitive operational data’ means operational data related t o activities of prevention, detection, in vestig ation or
prosecution of cr iminal offe nces, the disclosure of which could jeopardise the inte gr ity of cr iminal proceedings;
(39) ‘emotion recognition syste m’ means an AI syste m f or the pur pose of identifying or inferr ing emotions or inte ntions of
natural persons on the basis of their biometr ic data;
(40) ‘biometr ic categor isation syste m’ means an AI system f or the pur pose of assigning natural persons to specif ic
cate gor ies on the basis of their biometr ic data, unless it is ancillar y to another commercial ser vice and str ictly
necessar y f or objective te chnical reasons;
(41) ‘remot e biometr ic identification system’ means an AI system f or the pur pose of identifying natural persons, without
their active in volvement, typically at a distance through the comp ar ison of a person’s biometr ic data with the
biometr ic data contained in a refere nce database;
(42) ‘real-time remote biometr ic identifica tion syste m’ means a remot e biometr ic identification system, whereby the
capt ur ing of biometr ic data, the comp ar ison and the identification all occur without a signifi cant dela y , compr ising
not only instant identifica tion, but also limited shor t dela ys in order to av oid circum vention;
(43) ‘post-remot e biometr ic identifica tion syste m’ means a remot e biometr ic identification system other than a real-time
remot e biometr ic identification system;
(44) ‘publicly accessible space ’ means any publicly or pr ivately owned physica l place accessible t o an undet er mined number
of natural persons, regardless of whether cer tain conditions f or access ma y apply , and regard less of the pote ntial
capacity restr ictions;
EN
OJ L, 12.7.2024
48/144 ELI: http://data.europa.eu/eli/reg/2024/1689/oj(45) ‘law enf orcement author ity’ means:
(a) an y public author ity comp etent f or the prevention, invest igation, detection or prosecution of cr iminal offences or
the ex ecution of cr iminal penalties, including the saf eguarding aga inst and the prevention of threats to public
secur ity ; or
(b) an y other body or entity entr uste d by Member Stat e law to exercise public author ity and public powers f or the
pur poses of the prevention, investig ation, detect ion or prosecution of cr iminal offe nces or the execution of
cr iminal penalties, including the safeguarding against and the prevention of threats to public secur ity ;
(46) ‘law enf orcement’ means activities car r ied out by law enf orcement author ities or on their behalf f or the prevention,
in vestig ation, detection or prosecution of cr iminal offe nces or the execution of cr iminal penalties, including
saf eguarding against and preventing threats to public secur ity ;
(47) ‘ AI Off ice’ means the Commission’s function of contr ibuting to the im plementation, monitorin g and super vision of AI
syste ms and general-pur pose AI models, and AI gover nance, provid ed f or in Commission Decision of 24 Januar y
2024; refe rences in this Regulation to the AI Office shall be constr ued as refe rences to the Commission;
(48) ‘national comp et ent author ity’ means a notifying author ity or a marke t sur veillance author ity ; as regard s AI syste ms
put into ser vice or used by Union institutions, agencies, off ices and bodies, refe rences to national compet ent
author ities or market sur veillance author ities in this Regulation shall be constr ued as refe rences to the European Data
Prot ection Super visor ;
(49) ‘ser ious incident ’ means an incident or malfunctioning of an AI syste m that directly or indirectly leads to any of the
f ollowing:
(a) the death of a person, or ser ious har m to a person’s health;
(b) a ser ious and ir reversible disr uption of the managem ent or operation of cr itical infrastr ucture;
(c) the infr ingement of obliga tions under Uni on law intended to protect fundamental r ights;
(d) ser ious har m to proper ty or the environment ;
(50) ‘personal data’ means personal data as defined in Ar ticle 4, point (1), of Regulation (EU) 2016/679;
(51) ‘non-personal data’ means data other than personal data as defined in Ar ticle 4, point (1), of Regulation (EU)
2016/679;
(52) ‘profiling’ means prof iling as def ined in Ar ticle 4, point (4), of Regulation (EU) 2016/679;
(53) ‘real-w orld te sting plan’ means a document that descr ibes the objectives, methodology , g eographical, population and
t emporal scope, monitorin g, organisation and conduct of t esting in real-world conditions;
(54) ‘sandbo x plan’ means a document agreed between the par ticipating provid er and the comp etent author ity descr ibing
the objectives, conditions, timeframe, methodology and requirements f or the activities car r ied out within the sandbo x;
(55) ‘ AI regulato r y sandbo x’ means a controlled framework set up by a comp etent author ity which offers provid ers or
prospective provider s of AI syste ms the possibility t o develop, train, validate and te st, where appropr iat e in real-wo rld
conditions, an innovative AI system, pursuant to a sandbo x plan f or a limited time under regulator y super vision;
(56) ‘ AI literacy’ means skills, knowledg e and understanding that allow provider s, deplo y ers and affect ed persons, taking
into account their respective r ights and obliga tions in the context of this Regulation, to make an inf or med deplo yment
of AI systems, as well as to g ain awareness about the oppor tunities and r isks of AI and possible har m it can cause;
OJ L, 12.7.2024
EN
ELI: http://data.europa.eu/eli/reg/2024/1689/oj 49/144(57) ‘te sting in real-world conditions’ means the tem porar y t esting of an AI syste m f or its intended pur pose in real-wo rld
conditions outside a laborato r y or other wise simulated envir onment, with a view to g ather ing reliable and robust data
and to assessing and ver ifying the conf or mity of the AI system with the requirements of this Regulation and it does
not qualify as placing the AI system on the marke t or putting it into ser vice within the meaning of this Regulation,
pro vided that all the conditions laid do wn in Ar ticle 57 or 60 are fulf illed;
(58) ‘subject’, f or the pur pose of real-world te sting, means a natural person who par ticipate s in te sting in real-wo rld
conditions;
(59) ‘inf or med consent’ means a subject’s freely given, specific, unambiguous and v oluntar y expression of his or her
willingness to par ticipate in a par ticular testing in real-world conditions, af ter having been inf or med of all aspects of
the te sting that are relevant to the subject ’s decision to par ticipate ;
(60) ‘deep f ak e ’ means AI-g enerated or manipulate d imag e, audio or video cont ent that resembles existing persons, objects,
places, entities or events and would f alsely appear to a person to be authentic or tr uthful;
(61) ‘widespread infr ingement’ means an y act or omission contrar y to Union law prot ecting the inte rest of individuals,
which :
(a) has har med or is like ly to har m the collective interests of individuals residing in at least tw o Member States other
than the Member State in which:
(i) the act or omission or iginated or t ook place;
(ii) the provider concer ned, or , where applicable, its author ised representative is located or established; or
(iii) the deplo y er is established, when the infr ingement is committed b y the deplo y er;
(b) has caused, causes or is likel y to cause har m to the collective intere sts of individuals and has common features,
including the same unlawful practice or the same interest being infr inged, and is occur r ing concur rently ,
committ ed by the same operat or , in at least three Member States;
(62) ‘cr itical infrastr ucture’ means cr itical infrastr ucture as def ined in Ar ticle 2, point (4), of Directive (EU) 2022/2557;
(63) ‘g eneral-pur pose AI model’ means an AI model, including where suc h an AI model is trained with a larg e amount of
data using self-super vision at scale, that displa ys significant g enerality and is capable of compet ently perfo r ming
a wide rang e of distinct tasks regardless of the wa y the model is placed on the market and that can be integrat ed into
a var iety of do wnstream systems or applications, ex cept AI models that are used f or research, development or
prot otyping activities bef ore the y are placed on the mark et ;
(64) ‘high-imp act capabilities ’ means capabilities that matc h or ex ceed the capabilities recorded in the most advanced
g eneral-pur pose AI models;
(65) ‘syst emic r isk’ means a r isk that is specif ic to the high-im pact capabilities of g eneral-pur pose AI models, having
a significant im pact on the Uni on market due t o their reac h, or due to actual or reasonably f oreseeable neg ative effe cts
on public health, safety , public secur ity , fundamental r ights, or the society as a whole, that can be propagat ed at scale
across the value ch ain;
(66) ‘g eneral-pur pose AI syste m’ means an AI syste m which is based on a g eneral-pur pose AI model and which has the
capability to ser ve a var iety of pur poses, both f or direct use as well as f or integrat ion in other AI systems;
(67) ‘f loating-point operation’ means any mathematical operation or assignment in volving f loating-point numbers, which
are a subset of the real numbers typically represente d on comput ers by an integ er of fixed precision scaled by an
inte ger exponent of a fi xe d base;
(68) ‘do wnstream pro vider ’ means a provid er of an AI system, including a g eneral-pur pose AI system, which integrat es an
AI model, rega rdless of whether the AI model is provided by themselves and ver tically integrat ed or provid ed by
another entity based on contractual relations.
EN
OJ L, 12.7.2024
50/144 ELI: http://data.europa.eu/eli/reg/2024/1689/ojAr ticle 4
AI literacy
Provi ders and deplo y ers of AI syste ms shall take measures to ensure, to their best exte nt, a suffi cient level of AI lite racy of
their staff and other persons dealing with the operation and use of AI systems on their behalf, taking into account their
te chnical kno wledge, exper ience, education and training and the context the AI systems are t o be used in, and consider ing
the persons or groups of persons on whom the AI syste ms are to be used.
C HAPTER II
PR OHIBITED AI PRA CTICES
Ar ticle 5
Prohibited AI practices
1. The f ollowi ng AI practices shall be prohibite d:
(a) the placing on the market, the putting into ser vice or the use of an AI syste m that deplo ys subliminal tec hniques bey ond
a person’s consciousness or pur posefully manipulative or decep tive t echniques, with the objective, or the eff ect of
materially distor ting the behaviour of a person or a group of persons by appreciably imp air ing their ability to make an
inf or med decision, thereby causing them to take a decision that the y would not hav e other wise take n in a manner that
causes or is reasonably like ly to cause that person, another person or group of persons significant har m;
(b) the placing on the marke t, the putting into ser vice or the use of an AI system that exploits any of the vulnerabilities of
a natural person or a specif ic group of persons due to their age, disability or a specif ic social or economic situation, with
the objective, or the effe ct, of materi ally distor ting the behavio ur of that person or a person belonging to that group in
a manner that causes or is reasonably like ly to cause that person or another person significant har m;
(c) the placing on the market, the putting into ser vice or the use of AI syste ms f or the evaluation or classif ication of natural
persons or groups of persons o ver a cer tain per iod of time based on their social behavio ur or known, inferr ed or
predicted personal or personality ch aracteristics , with the social score leading to either or both of the f ollo wing:
(i) detr imental or unfa vourable treatment of cer tain natural persons or groups of persons in social contexts that are
unrelat ed to the cont exts in which the data was or iginally generat ed or collect ed;
(ii) detr imental or unfa v ourable treatment of cer tain natural persons or groups of persons that is unjustifie d or
dispropor tionate to their social behavio ur or its gra vity ;
(d) the placing on the marke t, the putting into ser vice f or this specif ic pur pose, or the use of an AI system f or making r isk
assessments of natural persons in order to assess or predict the r isk of a natural person committing a cr iminal offe nce,
based solely on the profiling of a natural person or on assessing their personality traits and ch aracter istics; this
prohibition shall not apply to AI systems used to suppor t the human assessment of the in volvement of a person in
a cr iminal activity , which is already based on objective and ver ifiable f acts directly linke d t o a cr iminal activity ;
(e) the placing on the mark et, the putting into ser vice f or this specif ic pur pose, or the use of AI syste ms that create or
expand f acial recognition databases through the untarget ed scraping of f acial images from the inter net or CCT V f ootage;
(f) the placing on the marke t, the putting into ser vice f or this specific pur pose, or the use of AI syste ms to infer emotions
of a natural person in the areas of wo rkplace and education institutions, ex cept where the use of the AI system is
intende d to be put in place or into the mark et f or medical or saf ety reasons;
OJ L, 12.7.2024
EN
ELI: http://data.europa.eu/eli/reg/2024/1689/oj 51/144(g) the placing on the marke t, the putting into ser vice f or this specific pur pose, or the use of biometr ic categor isation
syste ms that catego r ise individually natural persons based on their biometr ic data to deduce or infer their race, political
opinions, trade union membership, religious or philosophical beliefs, sex life or sexual or ientation; this prohibition does
not cover any labelling or filter ing of law fully acquired biometr ic datasets, suc h as imag es, based on biometr ic data or
categori zing of biometr ic data in the area of law enf orcement ;
(h) the use of ‘real-time’ remote biometr ic identification syste ms in publicly accessible spaces f or the pur poses of la w
enf orcement, unless and in so f ar as suc h use is str ictly necessar y f or one of the f ollo wing objectives:
(i) the targe ted search f or specif ic victims of abduction, trafficking in human beings or sexual exploitation of human
beings, as well as the search f or missing persons;
(ii) the prevention of a specif ic, substantial and imminent threat to the life or physica l saf ety of natural persons or
a genuine and present or g enuine and f oreseeable threat of a te r ror ist attack;
(iii) the localisation or identification of a person suspect ed of having committed a cr iminal offence, f or the pur pose of
conducting a cr iminal invest igation or prosecution or ex ecuting a cr iminal penalty f or offe nces referred to in
Annex II and punishable in the Member Stat e concer ned by a custodial sente nce or a det ention order f or
a maximum per iod of at least f our y ears.
P oint (h) of the fir st subparagraph is without prejudice t o Ar ticle 9 of Regulation (EU) 2016/679 f or the processing of
biometr ic data f or pur poses other than law enf orcement.
2. The use of ‘real-time’ remote biometr ic identifi cation syste ms in publicly accessible spaces f or the pur poses of la w
enf orcement f or any of the objectives refer red to in paragraph 1, fir st subparagraph, point (h), shall be deplo y ed f or the
pur poses set out in that point only to confir m the identity of the specif ically targe ted individual, and it shall take into
account the f ollo wing elements:
(a) the nature of the situation giving r ise to the possible use, in par ticular the ser iousness, probability and scale of the har m
that would be caused if the system were not used;
(b) the consequences of the use of the system f or the r ights and freedoms of all persons concer ned, in par ticular the
ser iousness, probability and scale of those consequences.
In addition, the use of ‘real-time’ remot e biometr ic identification systems in publicly accessible spaces f or the pur poses of
law enf orcement f or any of the objectives referred to in paragraph 1, first subparagraph, point (h), of this Ar ticle shall
comply with necessar y and propor tionate safegua rds and conditions in relation to the use in accordance with the national
law author ising the use thereof, in par ticular as regard s the te mporal, g eographic and personal limitations. The use of the
‘real-time’ remot e biometr ic identifica tion syste m in publicly accessible spaces shall be author ised only if the la w
enf orcement author ity has complet ed a fundamental r ights im pact assessment as provid ed f or in Ar ticle 27 and has
register ed the system in the EU database according to Ar ticle 49. Ho wever , in duly justified cases of urgency , the use of such
syste ms ma y be commenced without the registration in the EU database, provid ed that suc h registration is complet ed
without undue dela y .
3. For the pur poses of paragraph 1, first subparagraph, point (h) and paragraph 2, each use f or the pur poses of la w
enf orcement of a ‘real-time’ remot e biometr ic identifica tion syste m in publicly accessible spaces shall be subject to a pr ior
author isation granted b y a judicial author ity or an independent administrative author ity whose decision is binding of the
Member State in which the use is to take place, issued upon a reasoned request and in accordance with the detailed r ules of
national law referred to in paragraph 5. How ever , in a duly justified situation of urg ency , the use of suc h system ma y be
commenced without an author isation provided that suc h author isation is request ed without undue dela y , at the lat est
within 24 hours. If such author isation is rejected, the use shall be st opped with immediate eff ect and all the data, as well as
the results and outputs of that use shall be immediately discarded and deleted.
The comp etent judicial author ity or an independent administrative author ity whose decision is binding shall grant the
author isation only where it is satisfied, on the basis of objective evidence or clear indications presented to it, that the use of
the ‘real-time’ remote biometr ic identifi cation system concer ned is necessar y f or , and propor tionate to , ac hieving one of the
EN
OJ L, 12.7.2024
52/144 ELI: http://data.europa.eu/eli/reg/2024/1689/ojobjectives specified in paragraph 1, first subparagraph, point (h), as identified in the request and, in par ticular , remains
limited to what is str ictly necessar y concer ning the per iod of time as well as the g eographic and personal scope. In deciding
on the request, that author ity shall tak e into account the elements refe r red to in paragraph 2. No decision that produces an
adverse leg al effe ct on a person ma y be tak en based solely on the output of the ‘real-time’ remote biometr ic identification
syste m.
4. Without prejudice to paragraph 3, each use of a ‘real-time’ remot e biometr ic identifica tion system in publicly
accessible spaces f or law enf orcement pur poses shall be notif ied to the relevant marke t sur veillance author ity and the
national data prot ection author ity in accordance with the national r ules refer red t o in paragraph 5. The notif ication shall, as
a minimum, contain the inf or mation specified under paragraph 6 and shall not include sensitive operational data.
5. A Member Stat e ma y decide t o provide f or the possibility to fully or par tially author ise the use of ‘real-time’ remote
biometr ic identifica tion systems in publicly accessible spaces f or the pur poses of law enf orcement within the limits and
under the conditions listed in paragraph 1, first subparagraph, point (h), and paragraphs 2 and 3. Member Stat es concer ned
shall la y do wn in their national law the necessar y detailed r ules f or the request, issuance and ex ercise of, as well as
super vision and repor ting relating to, the author isations refe r red to in paragraph 3. Those r ules shall also specify in respect
of which of the objectives listed in paragraph 1, fir st subparagraph, point (h), including which of the cr iminal offe nces
referred to in point (h)(iii) thereof, the compet ent author ities ma y be author ised to use those syste ms f or the pur poses of
law enf orcement. Member States shall notify those r ules t o the Commission at the latest 30 da ys f ollo wing the adop tion
thereof. Member States ma y introduce, in accordance with Uni on law , more restr ictive laws on the use of remote biometr ic
identifica tion systems.
6. National market sur veillance author ities and the national data prot ection author ities of Member Stat es that have been
notif ied of the use of ‘real-time’ remote biometr ic identifi cation syste ms in publicly accessible spaces f or law enf orcement
pur poses pursuant to paragraph 4 shall submit to the Commission annual repor ts on suc h use. For that pur pose, the
Commission shall pro vide Member States and national market sur veillance and data protection author ities with a tem plate ,
including inf or mation on the number of the decisions taken by compet ent judicial author ities or an independent
administrative author ity whose decision is binding upon requests f or author isations in accordance with paragraph 3 and
their result.
7. The Commission shall publish annual repor ts on the use of real-time remot e biometr ic identification systems in
publicly accessible spaces f or law enf orcement pur poses, based on aggregat ed data in Member States on the basis of the
annual repor ts refer red to in paragraph 6. Those annual repor ts shall not include sensitive operational data of the related
law enf orcement activities.
8. This Ar ticle shall not affe ct the prohibitions that apply where an AI practice infr ing es other Union law .
CHAPTER III
HIGH-RISK AI SYSTEMS
SECTION 1
Classification of AI sy stems as high-r isk
Ar ticle 6
Classif ication r ules f or high-r isk AI sys tems
1. Ir respective of whether an AI syste m is placed on the marke t or put into ser vice independently of the products
referred to in points (a) and (b), that AI system shall be considered to be high-r isk where both of the f ollo wing conditions
are fulfilled:
(a) the AI syste m is inte nded to be used as a saf ety compo nent of a product, or the AI syste m is itself a product, cover ed by
the Uni on har monisation legislation listed in Annex I;
(b) the product whose safety compo nent pursuant to point (a) is the AI syste m, or the AI system itself as a product, is
required to undergo a third-par ty conf or mity assessment, with a view to the placing on the marke t or the putting into
ser vice of that product pursuant t o the Uni on har monisation legislation listed in Annex I.
2. In addition to the high-r isk AI syste ms refe r red to in paragraph 1, AI syste ms refe r red to in Annex III shall be
considered to be high-r isk.
OJ L, 12.7.2024
EN
ELI: http://data.europa.eu/eli/reg/2024/1689/oj 53/1443. By derogat ion from paragraph 2, an AI system refe r red to in Annex III shall not be considered t o be high-r isk where it
does not pose a significant r isk of har m to the health, safety or fundamental r ights of natural persons, including by not
materially inf luencing the outcome of decision making.
The first subparagraph shall apply where an y of the f ollo wing conditions is fulf illed:
(a) the AI syste m is intende d to per f or m a nar row procedural task;
(b) the AI syste m is intende d to impro ve the result of a previously comp let ed human activity ;
(c) the AI system is intende d to det ect decision-making patt er ns or deviations from pr ior decision-making patterns and is
not meant to replace or inf luence the previously comp leted human assessment, without proper human review ; or
(d) the AI system is intended to perfor m a preparat or y task to an assessment relevant f or the pur poses of the use cases
listed in Annex III.
Notwithstanding the first subparagraph, an AI system refer red to in Annex III shall alwa ys be considered to be high-r isk
where the AI system perfor ms profiling of natural persons.
4. A provid er who considers that an AI syste m refe r red to in Annex III is not high-r isk shall document its assessment
bef ore that system is placed on the market or put into ser vice. Such provid er shall be subject to the registration obliga tion
set out in Ar ticle 49(2). Upon request of national comp et ent author ities, the provider shall provide the documentation of
the assessment.
5. The Commission shall, af te r consulting the European Ar tif icial Inte lligence Board (the ‘Board’), and no later than
2 Febr uar y 2026, provide guidelines specifying the practical im plementation of this Ar ticle in line with Ar ticle 96 to gether
with a comp rehensive list of practical exam ples of use cases of AI systems that are high-r isk and not high-r isk.
6. The Commission is empo wered to adopt deleg ated acts in accordance with Ar ticle 97 in order to amend paragraph 3,
second subparagraph, of this Ar ticle by adding new conditions to those laid do wn therein, or by modifying them, where
there is concrete and reliable evidence of the existe nce of AI syste ms that f all under the scope of Annex III, but do not pose
a significant r isk of har m to the health, saf ety or fundamental r ights of natural persons.
7. The Commission shall adop t delegat ed acts in accordance with Ar ticle 97 in order to amend paragraph 3, second
subparagraph, of this Ar ticle by deleting any of the conditions laid down therein, where there is concrete and reliable
evidence that this is necessar y t o maintain the level of protection of health, safety and fundamental r ights pro vided f or by
this Regulation.
8. Any amendment to the conditions laid do wn in paragraph 3, second subparagraph, adopt ed in accordance with
paragraphs 6 and 7 of this Ar ticle shall not decrease the overall level of protect ion of health, safety and fundamental r ights
provid ed f or b y this Regulation and shall ensure consiste ncy with the deleg ated acts adop ted pursuant to Ar ticle 7(1), and
take account of marke t and tec hnological developments.
Ar ticle 7
Amendments to Annex III
1. The Commission is em powered to adop t deleg ated acts in accordance with Ar ticle 97 t o amend Annex III b y adding
or modifying use-cases of high-r isk AI systems where both of the f ollowing conditions are fulf illed:
(a) the AI syste ms are intende d to be used in any of the areas list ed in Annex III;
(b) the AI systems pose a r isk of har m to health and safety , or an adverse im pact on fundamental r ights, and that r isk is
equivalent to, or great er than, the r isk of har m or of adverse imp act posed by the high-r isk AI syste ms already refe r red
to in Annex III.
EN
OJ L, 12.7.2024
54/144 ELI: http://data.europa.eu/eli/reg/2024/1689/oj2. When assessing the condition under paragraph 1, point (b), the Commission shall take into account the f ollowing
cr ite r ia:
(a) the inte nded pur pose of the AI system;
(b) the exte nt to which an AI syste m has been used or is like ly t o be used;
(c) the nature and amount of the data processed and used b y the AI system, in par ticular whether special categori es of
personal data are processed;
(d) the extent to which the AI system acts autonomously and the possibility f or a human to ove r r ide a decision or
recommendations that ma y lead to pote ntial har m;
(e) the exte nt to which the use of an AI syste m has already caused har m to health and safety , has had an adverse impact on
fundamental r ights or has given r ise to significant concer ns in relation to the like lihood of suc h har m or adverse impact,
as demonstrated, f or exam ple, by repor ts or documente d alleg ations submitted to national compet ent author ities or by
other repor ts, as appropr iate;
(f) the pote ntial exte nt of such har m or suc h adverse impact, in par ticular in ter ms of its intens ity and its ability t o affect
multiple persons or to dispropor tionately affect a par ticular group of persons;
(g) the extent to which persons who are potent ially har med or suffer an adverse impact are dependent on the outcome
produced with an AI system, in par ticular because f or practical or lega l reasons it is not reasonably possible to opt-ou t
from that outcome;
(h) the extent to which there is an imbalance of power , or the persons who are potentially har med or suff er an adverse
imp act are in a vulnerable position in relation to the deplo y er of an AI syste m, in par ticular due to status, author ity ,
kno wledge, economic or social circumstances, or age;
(i) the exte nt to which the outcome produced inv olving an AI system is easily cor r igible or reversible, taking into account
the te chnical solutions available to cor rect or reverse it, whereby outcomes hav ing an adverse imp act on health, saf ety or
fundamental r ights, shall not be considered to be easily cor r igible or reversible;
(j) the magnitude and likelihood of benefit of the deplo yment of the AI system f or individuals, groups, or society at larg e,
including possible im provements in product saf ety ;
(k) the exte nt to which existing Uni on law provid es f or:
(i) eff ective measures of redress in relation t o the r isks posed by an AI syste m, with the ex clusion of claims f or
damage s;
(ii) eff ective measures to prevent or substantially minimise those r isks.
3. The Commission is empo wered to adopt deleg ated acts in accordance with Ar ticle 97 t o amend the list in Annex III
by removi ng high-r isk AI systems where both of the f ollowing conditions are fulf illed:
(a) the high-r isk AI syste m concer ned no longer poses any signifi cant r isks to fundamental r ights, health or safety , taking
into account the cr ite r ia listed in paragraph 2;
(b) the deletion does not decrease the ove rall level of prot ection of health, saf ety and fundamental r ights under Union law .
SECTION 2
Requir ements f or high-r isk AI systems
Ar ticle 8
Compliance with the requirements
1. High-r isk AI systems shall comp ly with the requirements laid do wn in this Section, taking into account their intended
pur pose as well as the g enerally ackno wledged stat e of the ar t on AI and AI-related tec hnologies. The r isk management
syste m refe r red t o in Ar ticle 9 shall be taken into account when ensur ing compliance with those requirements.
OJ L, 12.7.2024
EN
ELI: http://data.europa.eu/eli/reg/2024/1689/oj 55/1442. Where a product contains an AI syste m, to which the requirements of this Regulation as well as requirements of the
Uni on har monisation legislation listed in Section A of Annex I apply , provid ers shall be responsible f or ensur ing that their
product is fully comp liant with all applicable requirements under applicable Uni on har monisation legislation. In ensur ing
the comp liance of high-r isk AI systems refe r red t o in paragraph 1 with the requirements set out in this Section, and in order
to ensure consistency , av oid duplication and minimise additional burdens, provid ers shall have a ch oice of integrat ing, as
appropr iate, the necessar y te sting and repor ting processes, inf or mation and documentation they provide with rega rd to
their product into documentation and procedures that already exist and are required under the Uni on har monisation
legislation listed in Section A of Annex I.
Ar ticle 9
Risk management sys tem
1. A r isk managem ent system shall be established, imp lemented, documented and maintained in relation to high-r isk AI
syste ms.
2. The r isk managem ent system shall be understood as a continuous iterative process planned and r un throughout the
entire lif ecy cle of a high-r isk AI system, requir ing regular systematic review and updating. It shall comp r ise the f ollowing
st eps:
(a) the identification and analysis of the known and the reasonably f oreseeable r isks that the high-r isk AI system can pose
to health, saf ety or fundamental r ights when the high-r isk AI syste m is used in accordance with its intende d pur pose;
(b) the estimation and evaluation of the r isks that ma y emerg e when the high-r isk AI syste m is used in accordance with its
intende d pur pose, and under conditions of reasonably f oreseeable misuse;
(c) the evaluation of other r isks possibly ar ising, based on the analysis of data g athered from the post-mark et monitoring
syste m refe r red t o in Ar ticle 72;
(d) the adoption of appropr iate and target ed r isk management measures designed to address the r isks identified pursuant to
point (a).
3. The r isks referred to in this Ar ticle shall concer n only those which ma y be reasonably mitiga te d or eliminated through
the development or design of the high-r isk AI syste m, or the provision of adequat e te chnical inf or mation.
4. The r isk managem ent measures refe r red to in paragraph 2, point (d), shall give due consideration to the eff ects and
possible inte raction resulting from the combined application of the requirements set out in this Section, with a view to
minimising r isks more effectively while ac hieving an appropr iate balance in implementing the measures to fulfil those
requirements.
5. The r isk managem ent measures refer red to in paragraph 2, point (d), shall be suc h that the relevant residual r isk
associate d with each hazard, as well as the overall residual r isk of the high-r isk AI syste ms is judged to be accep table.
In identifying the most appropr iate r isk managem ent measures, the f ollowing shall be ensured:
(a) elimination or reduction of r isks identifie d and evaluate d pursuant to paragraph 2 in as far as tec hnically f easible
through adequate design and development of the high-r isk AI syste m;
(b) where appropr iate, imp lementation of adequate mitiga tion and control measures addressing r isks that cannot be
eliminated ;
(c) provision of inf or mation required pursuant to Ar ticle 13 and, where appropr iate, training to deplo y ers.
With a view to eliminating or reducing r isks relate d to the use of the high-r isk AI system, due consideration shall be given
to the tec hnical kno wledge, exper ience, education, the training to be expected b y the deplo y er , and the presumable cont ext
in which the system is intended to be used.
EN
OJ L, 12.7.2024
56/144 ELI: http://data.europa.eu/eli/reg/2024/1689/oj6. High-r isk AI systems shall be t ested f or the pur pose of identifying the most appropr iate and target ed r isk management
measures. T esting shall ensure that high-r isk AI systems perfor m consist ently f or their intended pur pose and that they are in
complia nce with the requirements set out in this Section.
7. T esting procedures ma y include t esting in real-world conditions in accordance with Ar ticle 60.
8. The testing of high-r isk AI syste ms shall be perfo r med, as appropr iate, at an y time throughout the development
process, and, in any event, pr ior t o their being placed on the market or put into ser vice. T esting shall be car r ied out agains t
pr ior def ined metr ics and probabilistic thresholds that are appropr iate to the intended pur pose of the high-r isk AI syste m.
9. When implementing the r isk management system as provid ed f or in paragraphs 1 t o 7, provider s shall give
consideration to whether in view of its intende d pur pose the high-r isk AI syste m is like ly to hav e an adverse im pact on
persons under the age of 18 and, as appropr iate, other vulnerable groups.
10. For provider s of high-r isk AI syste ms that are subject t o requirements regard ing inter nal r isk managem ent processes
under other relevant provisions of Union law , the aspects provided in paragraphs 1 to 9 ma y be par t of, or combined with,
the r isk managem ent procedures established pursuant to that law .
Ar ticle 10
Data and dat a go v er nance
1. High-r isk AI syste ms which mak e use of te ch niques in volving the training of AI models with data shall be developed
on the basis of training, validation and testing data sets that meet the quality cr iter ia refe r red t o in paragraphs 2 to 5
whenever suc h data sets are used.
2. T raining, validation and te sting data sets shall be subject to data gove r nance and management practices appropr iate
f or the inte nded pur pose of the high-r isk AI syste m. Those practices shall concer n in par ticular:
(a) the relevant design choices ;
(b) data collection processes and the or igin of data, and in the case of personal data, the or iginal pur pose of the data
collection;
(c) relevant data-preparation processing operations, suc h as annotation, labelling, cleaning, updating, enr ic hment and
aggregati on;
(d) the f or mulation of assum ptions, in par ticular with respect t o the inf or mation that the data are supposed to measure and
represent ;
(e) an assessment of the ava ilability , quantity and suitability of the data sets that are needed;
(f) examination in view of possible biases that are likel y to affect the health and saf ety of persons, hav e a negative im pact
on fundamental r ights or lead t o discr imination prohibited under Uni on law , especially where data outputs inf luence
input s f or future operations;
(g) appropr iate measures t o detect, prevent and mitigat e possible biases identifie d according to point (f);
(h) the identifica tion of relevant data gaps or shor tcomings that prevent compliance with this Regulation, and ho w those
g aps and shor tcomings can be addressed.
3. T raining, validation and te sting data sets shall be relevant, suffici ently representative, and to the best exte nt possible,
free of er rors and comp let e in view of the intended pur pose. They shall hav e the appropr iate statistical proper ties, including,
where applicable, as regards the persons or groups of persons in relation t o whom the high-r isk AI system is intende d to be
used. Those charact er istics of the data sets ma y be met at the level of individual data sets or at the level of a combination
thereof.
4. Data sets shall take into account, to the extent required by the intended pur pose, the char acter istics or elements that
are par ticular to the specific geographical, cont extual, behavi oural or functional setting within which the high-r isk AI
syste m is intende d to be used.
OJ L, 12.7.2024
EN
ELI: http://data.europa.eu/eli/reg/2024/1689/oj 57/1445. T o the exte nt that it is str ictly necessar y f or the pur pose of ensur ing bias detection and cor rection in relation to the
high-r isk AI systems in accordance with paragraph (2), points (f) and (g) of this Ar ticle, the provid ers of suc h systems ma y
ex ceptionally process special categor ies of personal data, subject to appropr iate saf eguards f or the fundamental r ights and
freedoms of natural persons. In addition to the provisions set out in Regulations (EU) 2016/679 and (EU) 2018/1725 and
Directive (EU) 2016/680, all the f ollowing conditions must be met in order f or suc h processing to occur:
(a) the bias det ection and cor rection cannot be effectively fulfilled b y processing other data, including synthetic or
anonymised data;
(b) the special cate gor ies of personal data are subject t o te chnical limitations on the re-use of the personal data, and
state-of-t he-ar t secur ity and pr ivacy-preser ving measures, including pseudon ymisation;
(c) the special cate gor ies of personal data are subject to measures to ensure that the personal data processed are secured,
protect ed, subject t o suitable saf eguards, including str ict controls and documentation of the access, to av oid misuse and
ensure that only author ised persons have access to those personal data with appropr iate conf identiality obliga tions;
(d) the special categori es of personal data are not to be transmitte d, transferre d or other wise accessed by other par ties;
(e) the special categor ies of personal data are delete d once the bias has been cor rect ed or the personal data has reac hed the
end of its retention per iod, whichever comes fi rst ;
(f) the records of processing activities pursuant to Regulations (EU) 2016/679 and (EU) 2018/1725 and Directive (EU)
2016/680 include the reasons why the processing of special categor ies of personal data was str ictly necessar y to det ect
and cor rect biases, and why that objective could not be achi eved b y processing other data.
6. For the development of high-r isk AI syste ms not using t echniques inv olving the training of AI models, paragraphs 2
to 5 apply only to the testing data sets.
Ar ticle 11
T echnical documentation
1. The t echnical documentation of a high-r isk AI syste m shall be drawn up bef ore that syste m is placed on the market or
put into ser vice and shall be kep t up-to date.
The te chnical documentation shall be drawn up in suc h a wa y as to demonstrate that the high-r isk AI system complie s with
the requirements set out in this Section and t o provide national comp etent author ities and notified bodies with the
necessar y inf or mation in a clear and compre hensive f or m t o assess the compliance of the AI syste m with those
requirements. It shall contain, at a minimum, the elements set out in Annex IV . SMEs, including star t-ups, ma y provid e the
elements of the tec hnical documentation specified in Annex IV in a simplified manner . T o that end, the Commission shall
establish a simplified te chnical documentation f or m target ed at the needs of small and microent er pr ises. Where an SME,
including a star t-up, opts to provide the inf or mation required in Annex IV in a sim plif ied manner , it shall use the f or m
referred to in this paragraph. Notified bodies shall accep t the f or m f or the pur poses of the conf or mity assessment.
2. Where a high-r isk AI system related to a product covered b y the Uni on har monisation legislation listed in Section
A of Annex I is placed on the marke t or put into ser vice, a sing le set of te chnical documentation shall be dra wn up
containing all the inf or mation set out in paragraph 1, as well as the inf or mation required under those lega l acts.
3. The Commission is empo wered to adop t delegat ed acts in accordance with Ar ticle 97 in order t o amend Annex IV ,
where necessar y , to ensure that, in light of tec hnical progress, the te chnical documentation provides all the inf or mation
necessar y t o assess the compliance of the system with the requirements set out in this Section.
EN
OJ L, 12.7.2024
58/144 ELI: http://data.europa.eu/eli/reg/2024/1689/ojAr ticle 12
Record-ke eping
1. High-r isk AI syste ms shall te chnically allow f or the auto matic recording of events (logs) over the lifetime of the
syste m.
2. In order to ensure a level of traceability of the functioning of a high-r isk AI system that is appropr iate to the intended
pur pose of the syste m, logging capabilities shall enable the recording of events relevant f or:
(a) identifying situations that ma y result in the high-r isk AI system presenting a r isk within the meaning of Ar ticle 79(1) or
in a substantial modification;
(b) f acilitating the post-market monitoring refer red to in Ar ticle 72; and
(c) monitoring the operation of high-r isk AI systems refe r red to in Ar ticle 26(5).
3. For high-r isk AI systems refe r red to in point 1 (a), of Annex III, the logging capabilities shall provid e, at a minimum:
(a) recording of the per iod of each use of the syste m (star t date and time and end date and time of each use);
(b) the refe rence database aga inst which in put data has been ch eck ed by the syste m;
(c) the in put data f or which the search has led to a matc h;
(d) the identification of the natural persons in v olved in the ver ification of the results, as refer red to in Ar ticle 14(5).
Ar ticle 13
T ransparency and pro vision of inf or mation t o deplo y ers
1. High-r isk AI systems shall be designed and developed in suc h a wa y as to ensure that their operation is suff iciently
transparent to enable deplo y ers to inte r pret a syste m’s output and use it appropr iat ely . An appropr iate type and degree of
transparency shall be ensured with a view to ac hieving comp liance with the relevant obliga tions of the provid er and
deplo yer set out in Section 3.
2. High-r isk AI systems shall be accompanied by instr uctions f or use in an appropr iate digital f or mat or other wise that
include concise, complet e, cor rect and clear inf or mation that is relevant, accessible and comp rehensible to deplo y ers.
3. The instr uctions f or use shall contain at least the f ollo wing inf or mation:
(a) the identity and the contact details of the pro vider and, where applicable, of its author ised representative;
(b) the ch aracter istics, capabilities and limitations of perf or mance of the high-r isk AI syste m, including:
(i) its intended pur pose;
(ii) the level of accuracy , including its metr ics, robustness and cybersecur ity refe r red to in Ar ticle 15 ag ainst which the
high-r isk AI syste m has been test ed and validat ed and which can be expected, and any known and f oreseeable
circumstances that ma y hav e an imp act on that expected level of accuracy , robustness and cybersecur ity ;
(iii) an y kno wn or f oreseeable circumstance, relate d to the use of the high-r isk AI syste m in accordance with its
inte nded pur pose or under conditions of reasonably f oreseeable misuse, which ma y lead t o r isks to the health and
saf ety or fundamental r ights refer red to in Ar ticle 9(2);
(iv) where applicable, the t echnical capabilities and ch aracter istics of the high-r isk AI system to provid e inf or mation
that is relevant t o explain its output ;
OJ L, 12.7.2024
EN
ELI: http://data.europa.eu/eli/reg/2024/1689/oj 59/144(v) when appropr iate, its perform ance rega rding specific persons or groups of persons on which the system is
inte nded to be used;
(vi) when appropr iate, specif ications f or the in put data, or any other relevant inf or mation in ter ms of the training,
validation and testing data sets used, taking into account the intende d pur pose of the high-r isk AI system;
(vii) where applicable, inf or mation to enable deplo y ers to inter pret the output of the high-r isk AI system and use it
appropr iat ely ;
(c) the chang es to the high-r isk AI syste m and its perf or mance which hav e been pre-dete r mined by the pro vider at the
moment of the initial conf or mity assessment, if any ;
(d) the human ove rsight measures referred to in Ar ticle 14, including the te ch nical measures put in place to f acilitate the
inter pretation of the outputs of the high-r isk AI syste ms by the deplo y ers;
(e) the computa tional and hardware resources needed, the expect ed lif etime of the high-r isk AI system and any necessar y
mainte nance and care measures, including their frequency , to ensure the proper functioning of that AI system, including
as regards sof tware update s;
(f) where relevant, a descr iption of the mechanisms included within the high-r isk AI system that allows deplo y ers to
properly collect, st ore and interpret the logs in accordance with Ar ticle 12.
Ar ticle 14
Human o v ersight
1. High-r isk AI systems shall be designed and developed in suc h a wa y , including with appropr iate human-machi ne
interfac e too ls, that they can be effe ctively oversee n b y natural persons dur ing the per iod in which they are in use.
2. Human ove rsight shall aim to prevent or minimise the r isks t o health, saf ety or fundamental r ights that ma y emerge
when a high-r isk AI syste m is used in accordance with its inte nded pur pose or under conditions of reasonably f oreseeable
misuse, in par ticular where suc h r isks persist despite the application of other requirements set out in this Section.
3. The overs ight measures shall be commensurat e with the r isks, level of auto nomy and context of use of the high-r isk
AI system, and shall be ensured through either one or both of the f ollowing types of measures:
(a) measures identified and built, when te chnically feasible, into the high-r isk AI system by the provid er bef ore it is placed
on the mark et or put into ser vice;
(b) measures identifie d by the provider bef ore placing the high-r isk AI syste m on the marke t or putting it into ser vice and
that are appropr iate t o be implement ed b y the deplo y er .
4. For the pur pose of imp lementing paragraphs 1, 2 and 3, the high-r isk AI system shall be provided to the deplo yer in
suc h a wa y that natural persons to whom human oversight is assigned are enabled, as appropr iat e and propor tionate :
(a) to properly understand the relevant capacities and limitations of the high-r isk AI system and be able to duly monitor its
operation, including in view of detecting and addressing anomalies, dysfunctions and unexpect ed perfo r mance;
(b) to remain aw are of the possible tendency of automat ically relying or over -relying on the output produced by a high-r isk
AI syste m (automation bias), in par ticular f or high-r isk AI systems used to provide inf or mation or recommendations f or
decisions to be take n b y natural persons;
(c) to cor rectly inte r pret the high-r isk AI system’s output, taking into account, f or exam ple, the interpretat ion tools and
methods available;
EN
OJ L, 12.7.2024
60/144 ELI: http://data.europa.eu/eli/reg/2024/1689/oj(d) to decide, in any par ticular situation, not to use the high-r isk AI syste m or to other wise disreg ard, overr ide or reverse
the output of the high-r isk AI syste m;
(e) to inter vene in the operation of the high-r isk AI syste m or inter r upt the syste m through a ‘st op’ button or a similar
procedure that allows the syste m to come to a halt in a saf e state.
5. For high-r isk AI syste ms refer red to in point 1(a) of Annex III, the measures referred t o in paragraph 3 of this Ar ticle
shall be suc h as to ensure that, in addition, no action or decision is take n b y the deplo y er on the basis of the identification
resulting from the system unless that identification has been separate ly ver ified and confi r med b y at least tw o natural
persons with the necessar y compet ence, training and author ity .
The requirement f or a separate ver ificati on by at least tw o natural persons shall not apply to high-r isk AI syste ms used f or
the pur poses of law enf orcement, migration, border control or asylum, where Union or national law considers the
application of this requirement to be dispropor tionate .
Ar ticle 15
A ccuracy , robustness and cybersecur ity
1. High-r isk AI syste ms shall be designed and developed in suc h a wa y that the y ac hieve an appropr iate level of accuracy ,
robustness, and cybersecur ity , and that the y perf or m consiste ntly in those respects throughout their lifecy cle.
2. T o address the t echnical aspects of ho w to measure the appropr iate levels of accuracy and robustness set out in
paragraph 1 and any other relevant perform ance metr ics, the Commission shall, in cooperation with relevant stakeholders
and organisations such as metrology and bench marking author ities, encourage , as appropr iate, the development of
bench mark s and measurement methodologies.
3. The levels of accuracy and the relevant accuracy metr ics of high-r isk AI syste ms shall be declared in the accom panying
instr uctions of use.
4. High-r isk AI syste ms shall be as resilient as possible regard ing er rors, f aults or inconsiste ncies that ma y occur within
the syste m or the environment in which the syste m operat es, in par ticular due to their interaction with natural persons or
other syste ms. T echnical and org anisational measures shall be take n in this regard .
The robustness of high-r isk AI syste ms ma y be ac hieved through te chnical redundancy solutions, which ma y include
backu p or f ail-safe plans.
High-r isk AI syste ms that continue to lear n af ter being placed on the market or put into ser vice shall be developed in such
a wa y as to eliminate or reduce as far as possible the r isk of possibly biased outputs inf luencing in put f or future operations
(f eedback loops), and as to ensure that any suc h f eedback loops are duly addressed with appropr iate mitigati on measures.
5. High-r isk AI systems shall be resilient against atte mp ts by unauthor ised third par ties to alter their use, outputs or
perfo r mance b y exploiting syste m vulnerabilities.
The tec hnical solutions aiming to ensure the cybersecur ity of high-r isk AI syste ms shall be appropr iate t o the relevant
circumstances and the r isk s.
The tec hnical solutions to address AI specific vulnerabilities shall include, where appropr iate, measures to prevent, detect,
respond to, resolve and control f or attac ks tr ying to manipulat e the training data set (data poisoning), or pre-trained
compo nents used in training (model poisoning), in puts designed to cause the AI model t o make a mistake (adversar ial
exam ples or model evasion), confi dentiality attac k s or model f la ws.
OJ L, 12.7.2024
EN
ELI: http://data.europa.eu/eli/reg/2024/1689/oj 61/144SECTION 3
Oblig ations of pr o viders and deploye rs of high-r isk AI syste ms and other par ties
Ar ticle 16
Obligations of pro viders of high-r isk AI systems
Provi ders of high-r isk AI syste ms shall:
(a) ensure that their high-r isk AI systems are compliant with the requirements set out in Section 2;
(b) indicate on the high-r isk AI syste m or , where that is not possible, on its packaging or its accom panying documentation,
as applicable, their name, regist ered trade name or register ed trade mark , the address at which the y can be contacted;
(c) hav e a quality management syste m in place which comp lies with Ar ticle 17;
(d) keep the documentation refer red to in Ar ticle 18;
(e) when under their control, keep the logs auto matically g enerated by their high-r isk AI systems as refe r red t o in
Ar ticle 19;
(f) ensure that the high-r isk AI system undergoes the relevant conf or mity assessment procedure as refer red to in Ar ticle 43,
pr ior to its being placed on the mark et or put into ser vice;
(g) dra w up an EU declaration of conf or mity in accordance with Ar ticle 47;
(h) affix the CE marking to the high-r isk AI system or , where that is not possible, on its packag ing or its accom pan ying
documentation, to indicate conf or mity with this Regulation, in accordance with Ar ticle 48;
(i) comply with the registration oblig ations refer red to in Ar ticle 49(1);
(j) take the necessar y cor rective actions and pro vide inf or mation as required in Ar ticle 20;
(k) upon a reasoned request of a national comp etent author ity , demonstrate the conf or mity of the high-r isk AI system with
the requirements set out in Section 2;
(l) ensure that the high-r isk AI system com plies with accessibility requirements in accordance with Directives (EU)
2016/2102 and (EU) 2019/882.
Ar ticle 17
Quality management sys tem
1. Providers of high-r isk AI syste ms shall put a quality manag ement system in place that ensures comp liance with this
Regulation. That system shall be documente d in a syste matic and orderly manner in the f or m of wr itten policies, procedures
and instr uctions, and shall include at least the f ollowing aspects:
(a) a strate gy f or regulator y comp liance, including comp liance with conf or mity assessment procedures and procedures f or
the manag ement of modifications to the high-r isk AI system;
(b) te chniques, procedures and syste matic actions to be used f or the design, design control and design ver ification of the
high-r isk AI syste m;
(c) te chniques, procedures and systematic actions to be used f or the development, quality control and quality assurance of
the high-r isk AI system;
(d) examination, te st and validation procedures to be car r ied out bef ore, dur ing and af te r the development of the high-r isk
AI system, and the frequency with which they hav e to be car r ied out ;
EN
OJ L, 12.7.2024
62/144 ELI: http://data.europa.eu/eli/reg/2024/1689/oj(e) te chnical specifications, including standards, to be applied and, where the relevant har monised standards are not
applied in full or do not cover all of the relevant requirements set out in Section 2, the means to be used t o ensure that
the high-r isk AI system complies with those requirements;
(f) syste ms and procedures f or data managem ent, including data acquisition, data collection, data analysis, data labelling,
data st orage, data filt ration, data mining, data aggregation, data ret ention and any other operation regard ing the data
that is perfo r med bef ore and f or the pur pose of the placing on the mark et or the putting into ser vice of high-r isk AI
syste ms;
(g) the r isk managem ent syste m refe r red to in Ar ticle 9;
(h) the setting-up, imp lementation and maintenance of a post-mark et monitoring syste m, in accordance with Ar ticle 72;
(i) procedures related to the repor ting of a ser ious incident in accordance with Ar ticle 73;
(j) the handling of communication with national compet ent author ities, other relevant author ities, including those
provid ing or suppor ting the access to data, notif ied bodies, other operators, custome rs or other intere sted par ties;
(k) syste ms and procedures f or record-keepi ng of all relevant documentation and inf or mation;
(l) resource manag ement, including secur ity-of-supply related measures;
(m) an accountability framework setting out the responsibilities of the management and other staff with regard to all the
aspects listed in this paragraph.
2. The imp lementation of the aspects refer red to in paragraph 1 shall be propor tionate to the size of the provider ’s
org anisation. Providers shall, in any event, respect the degree of r igour and the level of protection required to ensure the
complia nce of their high-r isk AI systems with this Regulation.
3. Providers of high-r isk AI systems that are subject to obliga tions regard ing quality management systems or an
equivalent function under relevant sectoral Uni on law ma y include the aspects listed in paragraph 1 as par t of the quality
managem ent syste ms pursuant to that law .
4. For provid ers that are fi nancial institutions subject to requirements regard ing their inter nal gove r nance, ar rangements
or processes under Uni on financial ser vices law , the obliga tion to put in place a quality management syste m, with the
ex ception of paragraph 1, points (g), (h) and (i) of this Ar ticle, shall be deemed to be fulfilled by comp lying with the r ules on
inter nal gover nance ar rang ements or processes pursuant to the relevant Uni on financ ial ser vices law . T o that end, any
har monised standards refe r red to in Ar ticle 40 shall be take n into account.
Ar ticle 18
Documentation keep ing
1. The provider shall, f or a per iod ending 10 y ears af te r the high-r isk AI syste m has been placed on the marke t or put
into ser vice, keep at the disposal of the national compet ent author ities:
(a) the t echnical documentation refer red to in Ar ticle 11;
(b) the documentation concer ning the quality management syste m refe r red t o in Ar ticle 17;
(c) the documentation concer ning the ch anges approved by notif ied bodies, where applicable;
(d) the decisions and other documents issued by the notified bodies, where applicable;
(e) the EU declaration of conf or mity referred to in Ar ticle 47.
OJ L, 12.7.2024
EN
ELI: http://data.europa.eu/eli/reg/2024/1689/oj 63/1442. Each Member State shall det er mine conditions under which the documentation referred to in paragraph 1 remains at
the disposal of the national compet ent author ities f or the per iod indicated in that paragraph f or the cases when a provid er
or its author ised representative established on its terr itory goes bankr up t or ceases its activity pr ior to the end of that
per iod.
3. Providers that are financial institutions subject to requirements regard ing their inter nal gover nance, ar rangements or
processes under Union financ ial ser vices law shall maintain the te chnical documentation as par t of the documentation kep t
under the relevant Uni on financial ser vices law .
Ar ticle 19
Aut omatically generated logs
1. Providers of high-r isk AI syste ms shall keep the logs refer red to in Ar ticle 12(1), automat ically g enerated by their
high-r isk AI syste ms, t o the extent suc h logs are under their control. Without prejudice t o applicable Uni on or national law ,
the logs shall be k ept f or a per iod appropr iate to the inte nded pur pose of the high-r isk AI syste m, of at least six months,
unless provided other wise in the applicable Uni on or national la w , in par ticular in Union law on the protect ion of personal
data.
2. Providers that are financial institutions subject to requirements regard ing their inter nal gover nance, ar rangements or
processes under Union fi nancial ser vices law shall maintain the logs automa tically g enerated by their high-r isk AI syste ms
as par t of the documentation kep t under the relevant financial ser vices law .
Ar ticle 20
Cor rectiv e actions and duty of infor mation
1. Providers of high-r isk AI syste ms which consider or hav e reason to consider that a high-r isk AI system that they have
placed on the market or put into ser vice is not in conf or mity with this Regulation shall immediate ly take the necessar y
cor rective actions to br ing that system into conf or mity , to withdraw it, to disable it, or to recall it, as appropr iate. They shall
inf or m the distr ibutors of the high-r isk AI syste m concer ned and, where applicable, the deplo yers, the author ised
representative and imp or te rs according ly .
2. Where the high-r isk AI system presents a r isk within the meaning of Ar ticle 79(1) and the provider becomes aware of
that r isk, it shall immediate ly investig ate the causes, in collaboration with the repor ting deplo y er , where applicable, and
inf or m the marke t sur veillance author ities compet ent f or the high-r isk AI system concer ned and, where applicable, the
notif ied body that issued a cer tificate f or that high-r isk AI system in accordance with Ar ticle 44, in par ticular , of the nature
of the non-comp liance and of any relevant cor rective action taken.
Ar ticle 21
Cooperation with competent author ities
1. Providers of high-r isk AI systems shall, upon a reasoned request by a comp et ent author ity , pro vide that author ity all
the inf or mation and documentation necessar y to demonstrate the conf or mity of the high-r isk AI syste m with the
requirements set out in Section 2, in a languag e which can be easily understood by the author ity in one of the off icial
languag es of the institutions of the Union as indicate d by the Member State concer ned.
2. Upon a reasoned request by a compet ent author ity , providers shall also give the requesting comp et ent author ity , as
applicable, access to the automat ically g enerated logs of the high-r isk AI syste m refer red to in Ar ticle 12(1), to the exte nt
suc h logs are under their control.
3. Any inf or mation obtained by a comp etent author ity pursuant to this Ar ticle shall be treate d in accordance with the
confi dentiality oblig ations set out in Ar ticle 78.
EN
OJ L, 12.7.2024
64/144 ELI: http://data.europa.eu/eli/reg/2024/1689/ojAr ticle 22
Author ised representativ es of pro viders of high-r isk AI systems
1. Pr ior to making their high-r isk AI syste ms ava ilable on the Union mark et, provider s established in third countr ies
shall, by wr itt en mandate, appoint an author ised representative which is established in the Uni on.
2. The provid er shall enable its author ised representative to perf or m the tasks specified in the mandate received from the
provid er .
3. The author ised representative shall perfo r m the tasks specified in the mandate received from the provider . It shall
provid e a cop y of the mandate to the marke t sur veillance author ities upon request, in one of the off icial languages of the
institutions of the Union, as indicated by the compet ent author ity . For the pur poses of this Regulation, the mandate shall
emp ower the author ised representative to car r y out the f ollowing tasks :
(a) ver ify that the EU declaration of conf or mity refer red to in Ar ticle 47 and the te chnical documentation referred to in
Ar ticle 11 hav e been drawn up and that an appropr iat e conf or mity assessment procedure has been car r ied out by the
provid er ;
(b) keep at the disposal of the comp etent author ities and national author ities or bodies refe r red to in Ar ticle 74(10), f or
a per iod of 10 y ears af te r the high-r isk AI syste m has been placed on the marke t or put into ser vice, the contact details
of the provid er that appoint ed the author ised representative, a cop y of the EU declaration of conf or mity refer red to in
Ar ticle 47, the te chnical documentation and, if applicable, the cer tificate issued by the notif ied body ;
(c) provid e a compet ent author ity , upon a reasoned request, with all the inf or mation and documentation, including that
referred to in point (b) of this subparagraph, necessar y to demonstrate the conf or mity of a high-r isk AI system with the
requirements set out in Section 2, including access to the logs, as refe r red to in Ar ticle 12(1), auto matically g enerated by
the high-r isk AI system, to the extent suc h logs are under the control of the provid er ;
(d) cooperate with compet ent author ities, upon a reasoned request, in any action the latter take in relation to the high-r isk
AI system, in par ticular to reduce and mitiga te the r isks posed by the high-r isk AI system;
(e) where applicable, comply with the registration obliga tions refer red to in Ar ticle 49(1), or , if the registration is car r ied
out by the provid er itself, ensure that the inf or mation refer red to in point 3 of Section A of Annex VIII is cor rect.
The mandate shall empo wer the author ised representative to be addressed, in addition t o or instead of the provider , by the
compet ent author ities, on all issues related to ensur ing comp liance with this Regulation.
4. The author ised representative shall te r minate the mandat e if it considers or has reason to consider the provider to be
acting contrar y to its obliga tions pursuant to this Regulation. In suc h a case, it shall immediately inf or m the relevant marke t
sur veillance author ity , as well as, where applicable, the relevant notified body , about the te r mination of the mandat e and the
reasons theref or .
Ar ticle 23
Obligations of impor ters
1. Bef ore placing a high-r isk AI syste m on the marke t, imp or t ers shall ensure that the syste m is in conf or mity with this
Regulation by ver ifying that:
(a) the relevant conf or mity assessment procedure refer red to in Ar ticle 43 has been car r ied out by the provider of the
high-r isk AI syste m;
(b) the provid er has dra wn up the tec hnical documentation in accordance with Ar ticle 11 and Annex IV ;
(c) the system bears the required CE marking and is accompanied b y the EU declaration of conf or mity refer red to in
Ar ticle 47 and instr uctions f or use;
(d) the provid er has appoint ed an author ised representative in accordance with Ar ticle 22(1).
OJ L, 12.7.2024
EN
ELI: http://data.europa.eu/eli/reg/2024/1689/oj 65/1442. Where an impor te r has sufficient reason to consider that a high-r isk AI system is not in conf or mity with this
Regulation, or is f alsifi ed, or accompanie d by f alsifi ed documentation, it shall not place the syste m on the market until it has
been brought into conf or mity . Where the high-r isk AI syste m presents a r isk within the meaning of Ar ticle 79(1), the
imp or te r shall inf or m the provid er of the syste m, the author ised representative and the market sur veillance author ities to
that eff ect.
3. Impor te rs shall indicate their name, regist ered trade name or register ed trade mark, and the address at which they can
be contacted on the high-r isk AI syste m and on its packaging or its accom pan ying documentation, where applicable.
4. Impor te rs shall ensure that, while a high-r isk AI syste m is under their responsibility , st orage or transpor t conditions,
where applicable, do not jeopardise its compliance with the requirements set out in Section 2.
5. Impor te rs shall keep, f or a per iod of 10 y ears af ter the high-r isk AI system has been placed on the mark et or put into
ser vice, a cop y of the cer tif icat e issued by the notif ied body , where applicable, of the instr uctions f or use, and of the EU
declaration of conf or mity refe r red t o in Ar ticle 47.
6. Impor te rs shall provid e the relevant comp etent author ities, upon a reasoned request, with all the necessar y
inf or mation and documentation, including that refer red to in paragraph 5, to demonstrat e the conf or mity of a high-r isk AI
syste m with the requirements set out in Section 2 in a languag e which can be easily understoo d by them. For this pur pose,
they shall also ensure that the tec hnical documentation can be made a vailable to those author ities.
7. Impor te rs shall cooperate with the relevant compet ent author ities in any action those author ities take in relation to
a high-r isk AI syste m placed on the marke t by the im por ters, in par ticular t o reduce and mitig ate the r isks posed by it.
Ar ticle 24
Obligations of distr ibutors
1. Bef ore making a high-r isk AI system a vailable on the marke t, distr ibutors shall ver ify that it bears the required CE
marking, that it is accom panied by a cop y of the EU declaration of conf or mity refe r red to in Ar ticle 47 and instr uctions f or
use, and that the provid er and the imp or te r of that syste m, as applicable, hav e complied with their respective obliga tions as
laid down in Ar ticle 16, points (b) and (c) and Ar ticle 23(3).
2. Where a distr ibutor considers or has reason to consider , on the basis of the inf or mation in its possession, that
a high-r isk AI system is not in conf or mity with the requirements set out in Section 2, it shall not mak e the high-r isk AI
syste m available on the market until the syste m has been brought into conf or mity with those requirements. Fur ther more,
where the high-r isk AI system presents a r isk within the meaning of Ar ticle 79(1), the distr ibutor shall inf or m the provid er
or the imp or te r of the syste m, as applicable, to that effe ct.
3. Distr ibut ors shall ensure that, while a high-r isk AI syste m is under their responsibility , st orage or transpor t
conditions, where applicable, do not jeopardise the compliance of the syste m with the requirements set out in Section 2.
4. A distr ibut or that considers or has reason to consider , on the basis of the inf or mation in its possession, a high-r isk AI
syste m which it has made a vailable on the marke t not t o be in conf or mity with the requirements set out in Section 2, shall
take the cor rective actions necessar y to br ing that system into conf or mity with those requirements, to withdra w it or recall
it, or shall ensure that the provid er , the impor te r or any relevant operat or , as appropr iat e, take s those cor rective actions.
Where the high-r isk AI system presents a r isk within the meaning of Ar ticle 79(1), the distr ibut or shall immediate ly inf or m
the provid er or impor te r of the syste m and the author ities compet ent f or the high-r isk AI system concer ned, giving details,
in par ticular , of the non-com pliance and of any cor rective actions taken.
5. Upon a reasoned request from a relevant comp etent author ity , distr ibut ors of a high-r isk AI syste m shall provide that
author ity with all the inf or mation and documentation regarding their actions pursuant to paragraphs 1 to 4 necessar y to
demonstrate the conf or mity of that system with the requirements set out in Section 2.
6. Distr ibut ors shall cooperate with the relevant compet ent author ities in any action those author ities take in relation to
a high-r isk AI system made a vailable on the market by the distr ibutors, in par ticular to reduce or mitigat e the r isk posed by
it.
EN
OJ L, 12.7.2024
66/144 ELI: http://data.europa.eu/eli/reg/2024/1689/ojAr ticle 25
Responsibilities along the AI value chain
1. Any distr ibut or , imp or te r , deplo y er or other third-par ty shall be considered t o be a provid er of a high-r isk AI system
f or the pur poses of this Regulation and shall be subject to the obliga tions of the provider under Ar ticle 16, in any of the
f ollo wing circumstances:
(a) they put their name or trademark on a high-r isk AI system already placed on the market or put into ser vice, without
prejudice to contractual ar rangements stipulating that the obliga tions are other wise allocat ed;
(b) they mak e a substantial modification to a high-r isk AI system that has already been placed on the market or has already
been put into ser vice in suc h a wa y that it remains a high-r isk AI system pursuant to Ar ticle 6;
(c) they modify the intended pur pose of an AI system, including a g eneral-pur pose AI system, which has not been classif ied
as high-r isk and has already been placed on the marke t or put into ser vice in such a wa y that the AI syste m concer ned
becomes a high-r isk AI system in accordance with Ar ticle 6.
2. Where the circumstances refe r red to in paragraph 1 occur , the provid er that initially placed the AI system on the
marke t or put it into ser vice shall no longe r be considered to be a provider of that specif ic AI syste m f or the pur poses of
this Regulation. That initial provider shall closely cooperate with new provid ers and shall make ava ilable the necessar y
inf or mation and provide the reasonably expected t echnical access and other assistance that are required f or the fulf ilment of
the oblig ations set out in this Regulation, in par ticular rega rding the compliance with the conf or mity assessment of
high-r isk AI syste ms. This paragraph shall not apply in cases where the initial provider has clearly specif ied that its AI
syste m is not to be ch anged into a high-r isk AI system and theref ore does not f all under the obligation to hand over the
documentation.
3. In the case of high-r isk AI syste ms that are saf ety components of products co vered b y the Union har monisation
legislation listed in Section A of Annex I, the product manufa cturer shall be considered to be the provider of the high-r isk
AI system, and shall be subject to the obliga tions under Ar ticle 16 under either of the f ollo wing circumstances:
(a) the high-r isk AI syste m is placed on the marke t to gether with the product under the name or trademark of the product
manufa cturer;
(b) the high-r isk AI system is put into ser vice under the name or trademark of the product manufacturer af te r the product
has been placed on the marke t.
4. The provider of a high-r isk AI system and the third par ty that supplies an AI syste m, tools, ser vices, components, or
processes that are used or integrat ed in a high-r isk AI system shall, by wr itten agreement, specify the necessar y inf or mation,
capabilities, t echnical access and other assistance based on the g enerally ac knowledg ed stat e of the ar t, in order to enable
the provider of the high-r isk AI syste m t o fully comply with the obliga tions set out in this Regulation. This paragraph shall
not apply to third par ties making accessible to the public too ls, ser vices, processes, or comp onents, other than
g eneral-pur pose AI models, under a free and open-source licence.
The AI Office ma y develop and recommend v oluntar y model te r ms f or contracts between provid ers of high-r isk AI syste ms
and third par ties that supply tools, ser vices, comp onents or processes that are used f or or inte grated into high-r isk AI
syste ms. When developing those v oluntar y model ter ms, the AI Office shall take into account possible contractual
requirements applicable in specif ic sect ors or business cases. The v oluntar y model ter ms shall be published and be ava ilable
free of charg e in an easily usable electronic f or mat.
5. Paragraphs 2 and 3 are without prejudice to the need to obser ve and prot ect intellect ual proper ty r ights, confi dential
business inf or mation and trade secrets in accordance with Uni on and national law .
Ar ticle 26
Obligations of deplo y ers of high-r isk AI systems
1. Deplo y ers of high-r isk AI systems shall tak e appropr iate te chnical and org anisational measures to ensure they use
suc h syste ms in accordance with the instr uctions f or use accompan ying the systems, pursuant t o paragraphs 3 and 6.
OJ L, 12.7.2024
EN
ELI: http://data.europa.eu/eli/reg/2024/1689/oj 67/1442. Deplo y ers shall assign human ove rsight to natural persons who hav e the necessar y compet ence, training and
author ity , as well as the necessar y suppor t.
3. The obligations set out in paragraphs 1 and 2, are without prejudice t o other deplo y er obligations under Uni on or
national law and t o the deplo yer ’s freedom t o organise its own resources and activities f or the pur pose of imp lementing the
human overs ight measures indicated by the provid er .
4. Without prejudice to paragraphs 1 and 2, to the extent the deplo y er exercises control over the input data, that
deplo yer shall ensure that in put data is relevant and suffi ciently representative in view of the intende d pur pose of the
high-r isk AI syste m.
5. Deplo y ers shall monitor the operation of the high-r isk AI syste m on the basis of the instr uctions f or use and, where
relevant, inf or m provid ers in accordance with Ar ticle 72. Where deplo yers have reason to consider that the use of the
high-r isk AI system in accordance with the instr uctions ma y result in that AI syste m presenting a r isk within the meaning of
Ar ticle 79(1), they shall, without undue dela y , inf or m the provider or distr ibut or and the relevant market sur veillance
author ity , and shall suspend the use of that system. Where deplo y ers have identifie d a ser ious incident, they shall also
immediately inf or m first the provid er , and then the impor te r or distr ibut or and the relevant market sur veillance author ities
of that incident. If the deplo y er is not able to reac h the provider , Ar ticle 73 shall apply mutatis mutandis. This obliga tion
shall not cover sensitive operational data of deplo yers of AI systems which are law enf orcement author ities.
For deplo y ers that are financ ial institutions subject to requirements regard ing their inter nal gover nance, ar rangements or
processes under Union fi nancial ser vices law , the monitorin g obligation set out in the fir st subparagraph shall be deemed to
be fulf illed by complying with the r ules on inter nal govern ance ar rangements, processes and mechanisms pursuant t o the
relevant financial ser vice la w .
6. Deplo y ers of high-r isk AI syste ms shall k eep the logs auto matically g enerated b y that high-r isk AI system t o the exte nt
suc h logs are under their control, f or a per iod appropr iate to the intended pur pose of the high-r isk AI system, of at least six
months, unless provid ed other wise in applicable Uni on or national law , in par ticular in Union law on the protect ion of
personal data.
Deplo y ers that are financial institutions subject to requirements regard ing their inter nal gove r nance, ar rang ements or
processes under Uni on financial ser vices law shall maintain the logs as par t of the documentation kep t pursuant to the
relevant Uni on financ ial ser vice law .
7. Bef ore putting into ser vice or using a high-r isk AI syste m at the wo rkplace, deplo y ers who are emplo y ers shall inf or m
wo rkers’ representatives and the affect ed work ers that the y will be subject to the use of the high-r isk AI system. This
inf or mation shall be provided, where applicable, in accordance with the r ules and procedures laid do wn in Union and
national law and practice on inf or mation of work ers and their representatives.
8. Deplo y ers of high-r isk AI systems that are public author ities, or Uni on institutions, bodies, offices or age ncies shall
comply with the registration obliga tions refe r red to in Ar ticle 49. When suc h deplo y ers find that the high-r isk AI system
that they en visage using has not been registered in the EU database refer red t o in Ar ticle 71, they shall not use that system
and shall inf or m the provider or the distr ibutor .
9. Where applicable, deplo y ers of high-r isk AI systems shall use the inf or mation provid ed under Ar ticle 13 of this
Regulation to comp ly with their obligation to car r y out a data protection imp act assessment under Ar ticle 35 of Regulation
(EU) 2016/679 or Ar ticle 27 of Directive (EU) 2016/680.
10. Without prejudice to Directive (EU) 2016/680, in the framew ork of an in vestigation f or the targe te d search of
a person suspect ed or convict ed of hav ing committed a cr iminal offe nce, the deplo y er of a high-r isk AI syste m f or
post-remote biometr ic identifica tion shall request an author isation, ex ante, or without undue dela y and no later than 48
hours, by a judicial author ity or an administrative author ity whose decision is binding and subject to judicial review , f or the
use of that syste m, excep t when it is used f or the initial identifica tion of a potential suspect based on objective and ver ifi able
f acts directly linked to the offe nce. Each use shall be limited to what is str ictly necessar y f or the invest igation of a specific
cr iminal offence.
If the author isation requested pursuant to the first subparagraph is rejected, the use of the post-remote biometr ic
identifica tion syste m linked to that requested author isation shall be st opped with immediate eff ect and the personal data
linked to the use of the high-r isk AI system f or which the author isation was request ed shall be delete d.
EN
OJ L, 12.7.2024
68/144 ELI: http://data.europa.eu/eli/reg/2024/1689/ojIn no case shall suc h high-r isk AI syste m f or post-remote biometr ic identifica tion be used f or law enf orcement pur poses in
an untarget ed wa y , without any link to a cr iminal offe nce, a cr iminal proceeding, a g enuine and present or genuine and
f oreseeable threat of a cr iminal offenc e, or the searc h f or a specif ic missing person. It shall be ensured that no decision that
produces an adverse leg al effect on a person ma y be taken by the law enf orcement author ities based solely on the output of
suc h post-remote biometr ic identifi cation syste ms.
This paragraph is without prejudice to Ar ticle 9 of Regulation (EU) 2016/679 and Ar ticle 10 of Directive (EU) 2016/680
f or the processing of biometr ic data.
Reg ardless of the pur pose or deplo y er , each use of such high-r isk AI syste ms shall be documented in the relevant police file
and shall be made a vailable t o the relevant marke t sur veillance author ity and the national data protection author ity upon
request, ex cluding the disclosure of sensitive operational data related to law enf orcement. This subparagraph shall be
without prejudice to the po wers confe r red b y Directive (EU) 2016/680 on super visor y author ities.
Deplo y ers shall submit annual repor ts to the relevant marke t sur veillance and national data prot ection author ities on their
use of post-remote biometr ic identifica tion systems, ex cluding the disclosure of sensitive operational data related to la w
enf orcement. The repor ts ma y be aggrega ted to cover more than one deplo yment.
Member States ma y introduce, in accordance with Uni on law , more restr ictive laws on the use of post-remote biometr ic
identifica tion systems.
11. Without prejudice to Ar ticle 50 of this Regulation, deplo y ers of high-r isk AI systems refe r red t o in Annex III that
make decisions or assist in making decisions relate d to natural persons shall inf or m the natural persons that the y are subject
to the use of the high-r isk AI syste m. For high-r isk AI systems used f or law enf orcement pur poses Ar ticle 13 of Directive
(EU) 2016/680 shall apply .
12. Deplo yers shall cooperate with the relevant compet ent author ities in any action those author ities take in relation to
the high-r isk AI system in order to imp lement this Regulation.
Ar ticle 27
F undament al r ights impact assessment f or high-r isk AI sys tems
1. Pr ior to deplo ying a high-r isk AI syste m refe r red to in Ar ticle 6(2), with the excep tion of high-r isk AI syste ms
intende d to be used in the area listed in point 2 of Annex III, deplo y ers that are bodies gove r ned b y public law , or are pr ivate
entities provid ing public ser vices, and deplo y ers of high-r isk AI syste ms refer red t o in points 5 (b) and (c) of Annex III, shall
perfo r m an assessment of the im pact on fundamental r ights that the use of such syste m ma y produce. For that pur pose,
deplo yers shall perfo r m an assessment consisting of:
(a) a descr iption of the deplo y er ’s processes in which the high-r isk AI system will be used in line with its intende d pur pose;
(b) a descr iption of the per iod of time within which, and the frequency with which, each high-r isk AI system is intended to
be used;
(c) the cate gor ies of natural persons and groups likely to be affe cted b y its use in the specif ic cont ext ;
(d) the specif ic r isk s of har m likely t o hav e an imp act on the categori es of natural persons or groups of persons identifie d
pursuant to point (c) of this paragraph, taking into account the inf or mation given by the provider pursuant to
Ar ticle 13;
(e) a descr ipt ion of the imp lementation of human oversight measures, according to the instr uctions f or use;
(f) the measures to be tak en in the case of the materialis ation of those r isk s, including the ar rang ements f or inte r nal
governance and comp laint mec hanisms.
OJ L, 12.7.2024
EN
ELI: http://data.europa.eu/eli/reg/2024/1689/oj 69/1442. The obligation laid do wn in paragraph 1 applies to the fi rst use of the high-r isk AI system. The deplo y er ma y , in
similar cases, rely on previously conducted fundamental r ights imp act assessments or existing impact assessments car r ied
out by provid er . If, dur ing the use of the high-r isk AI system, the deplo y er considers that any of the elements listed in
paragraph 1 has ch anged or is no longe r up to date, the deplo y er shall take the necessar y st eps to update the inf or mation.
3. Once the assessment refer red t o in paragraph 1 of this Ar ticle has been perf or med, the deplo y er shall notify the
marke t sur veillance author ity of its results, submitting the filled- out tem plate refer red to in paragraph 5 of this Ar ticle as
par t of the notification. In the case refer red to in Ar ticle 46(1), deplo y ers ma y be exem pt from that obligation t o notify .
4. If an y of the obliga tions laid do wn in this Ar ticle is already met through the data protection impact assessment
conducted pursuant t o Ar ticle 35 of Regulation (EU) 2016/679 or Ar ticle 27 of Directive (EU) 2016/680, the fundamental
r ights imp act assessment refe r red to in paragraph 1 of this Ar ticle shall comp lement that data protection im pact
assessment.
5. The AI Office shall develop a te mplat e f or a questionnaire, including through an auto mated t ool, to facilitat e deplo yers
in com plying with their obliga tions under this Ar ticle in a simplified manner .
SECTION 4
Notifying author ities and notified bodies
Ar ticle 28
Notifying author ities
1. Each Member Stat e shall designate or establish at least one notifying author ity responsible f or setting up and car r ying
out the necessar y procedures f or the assessment, designation and notif ication of conf or mity assessment bodies and f or their
monitoring. Those procedures shall be developed in cooperation between the notifying author ities of all Member States.
2. Member States ma y decide that the assessment and monitoring referred to in paragraph 1 is to be car r ied out by
a national accreditation body within the meaning of, and in accordance with, Regulation (EC) No 765/2008.
3. Notifying author ities shall be established, org anised and operated in suc h a wa y that no conf lict of interest ar ises with
conf or mity assessment bodies, and that the objectivity and impar tiality of their activities are saf eguarded.
4. Notifying author ities shall be organi sed in suc h a wa y that decisions relating to the notification of conf or mity
assessment bodies are taken by compet ent persons diffe rent from those who car r ied out the assessment of those bodies.
5. Notifying author ities shall offer or provide neither any activities that conf or mity assessment bodies perform , nor any
consultancy ser vices on a commercial or competitive basis.
6. Notifying author ities shall saf eguard the confidentiality of the inf or mation that they obtain, in accordance with
Ar ticle 78.
7. Notifying author ities shall have an adequate number of com petent personnel at their disposal f or the proper
perfo r mance of their tasks. Comp etent personnel shall hav e the necessar y exper tise, where applicable, f or their function, in
fie lds suc h as inf or mation te chnologi es, AI and la w , including the super vision of fundamental r ights.
Ar ticle 29
Application of a confor mity assessment body for notif ication
1. Conf or mity assessment bodies shall submit an application f or notification to the notifying author ity of the Member
Stat e in which the y are established.
EN
OJ L, 12.7.2024
70/144 ELI: http://data.europa.eu/eli/reg/2024/1689/oj2. The application f or notif ication shall be accompanied b y a descr iption of the conf or mity assessment activities, the
conf or mity assessment module or modules and the types of AI syste ms f or which the conf or mity assessment body claims
to be compet ent, as well as by an accreditation cer tificate, where one exists, issued by a national accreditation body atte sting
that the conf or mity assessment body fulfils the requirements laid do wn in Ar ticle 31.
Any valid document related to existing designations of the applicant notified body under any other Union har monisation
legislation shall be added.
3. Where the conf or mity assessment body concer ned cannot provide an accreditation cer tif icate, it shall provid e the
notifying author ity with all the documentar y evidence necessar y f or the ver ifi cation, recognition and regular monitoring of
its comp liance with the requirements laid do wn in Ar ticle 31.
4. For notif ied bodies which are designate d under any other Union har monisation legislation, all documents and
cer tif icat es linked to those designations ma y be used to suppor t their designation procedure under this Regulation, as
appropr iate. The notified body shall update the documentation refer red to in paragraphs 2 and 3 of this Ar ticle whenever
relevant ch anges occur , in order to enable the author ity responsible f or notified bodies to monitor and ver ify continuous
complia nce with all the requirements laid down in Ar ticle 31.
Ar ticle 30
Notif ication procedure
1. Notifying author ities ma y notify only conf or mity assessment bodies which have satisfied the requirements laid do wn
in Ar ticle 31.
2. Notifying author ities shall notify the Commission and the other Member Stat es, using the electronic notification to ol
developed and managed by the Commission, of each conf or mity assessment body refer red to in paragraph 1.
3. The notification refer red t o in paragraph 2 of this Ar ticle shall include full details of the conf or mity assessment
activities, the conf or mity assessment module or modules, the types of AI syste ms concer ned, and the relevant attestation of
compet ence. Where a notification is not based on an accreditation cer tif icate as refer red to in Ar ticle 29(2), the notifying
author ity shall provide the Commission and the other Member States with documentar y evidence which atte sts to the
compet ence of the conf or mity assessment body and t o the ar rang ements in place t o ensure that that body will be
monitor ed regularly and will continue t o satisfy the requirements laid do wn in Ar ticle 31.
4. The conf or mity assessment body concer ned ma y per f or m the activities of a notified body only where no objections
are raised b y the Commission or the other Member Stat es within tw o weeks of a notification by a notifying author ity where
it includes an accreditation cer tif icate refer red to in Ar ticle 29(2), or within tw o months of a notification by the notifying
author ity where it includes documentar y evidence refer red to in Ar ticle 29(3).
5. Where objections are raised, the Commission shall, without dela y , ent er into consultations with the relevant Member
Stat es and the conf or mity assessment body . In view thereof, the Commission shall decide whether the author isation is
justified. The Commission shall address its decision to the Member State concer ned and to the relevant conf or mity
assessment body .
Ar ticle 31
Requirements relating t o notif ied bodies
1. A notif ied body shall be established under the national law of a Member Stat e and shall hav e legal personality .
2. Notifi ed bodies shall satisfy the org anisational, quality managem ent, resources and process requirements that are
necessar y t o fulf il their tasks, as well as suitable cybersecur ity requirements.
3. The org anisational str ucture, allocation of responsibilities, repor ting lines and operation of notif ied bodies shall
ensure confi dence in their perf or mance, and in the results of the conf or mity assessment activities that the notif ied bodies
conduct.
OJ L, 12.7.2024
EN
ELI: http://data.europa.eu/eli/reg/2024/1689/oj 71/1444. Notifi ed bodies shall be independent of the provid er of a high-r isk AI syste m in relation to which they per f or m
conf or mity assessment activities. Notifi ed bodies shall also be independent of any other operat or having an economic
intere st in high-r isk AI syste ms assessed, as well as of any competit ors of the provider . This shall not preclude the use of
assessed high-r isk AI syste ms that are necessar y f or the operations of the conf or mity assessment body , or the use of such
high-r isk AI syste ms f or personal pur poses.
5. Neither a conf or mity assessment body , its top-level managem ent nor the personnel responsible f or car r ying out its
conf or mity assessment tasks shall be directly in volved in the design, development, marke ting or use of high-r isk AI systems,
nor shall they represent the par ties engag ed in those activities. They shall not enga ge in any activity that might conf lict with
their independence of judgement or inte gr ity in relation to conf or mity assessment activities f or which they are notified.
This shall, in par ticular , apply to consultancy ser vices.
6. Notifi ed bodies shall be org anised and operat ed so as to saf eguard the independence, objectivity and imp ar tiality of
their activities. Notifi ed bodies shall document and im plement a str ucture and procedures to safeguard im par tiality and to
promot e and apply the pr inciples of imp ar tiality throughout their org anisation, personnel and assessment activities.
7. Notifi ed bodies shall have documented procedures in place ensur ing that their personnel, committ ees, subsidiar ies,
subcontractors and any associated body or personnel of exte r nal bodies maintain, in accordance with Ar ticle 78, the
confi dentiality of the inf or mation which comes into their possession dur ing the perf or mance of conf or mity assessment
activities, ex cept when its disclosure is required by law . The staff of notified bodies shall be bound to obser ve profe ssional
secrecy with regard t o all inf or mation obtained in car r ying out their tasks under this Regulation, excep t in relation to the
notifying author ities of the Member Stat e in which their activities are car r ied out.
8. Notifi ed bodies shall have procedures f or the perf or mance of activities which take due account of the size of
a provid er , the sector in which it operat es, its str ucture, and the degree of comp lexity of the AI system concer ned.
9. Notifi ed bodies shall tak e out appropr iat e liability insurance f or their conf or mity assessment activities, unless liability
is assumed b y the Member Stat e in which they are established in accordance with national law or that Member Stat e is itself
directly responsible f or the conf or mity assessment.
10. Notifi ed bodies shall be capable of car r ying out all their tasks under this Regulation with the highest degree of
profe ssional inte gr ity and the requisite comp etence in the specif ic fie ld, whether those tasks are car r ied out by notif ied
bodies themselves or on their behalf and under their responsibility .
11. Notifi ed bodies shall hav e sufficient inte r nal compet ences to be able effe ctively to evaluate the tasks conducted by
external par ties on their behalf. The notif ied body shall hav e per manent ava ilability of sufficient administrative, te chnical,
lega l and scientific personnel who possess exper ience and knowledg e relating t o the relevant types of AI syste ms, data and
data computing , and relating to the requirements set out in Section 2.
12. Notifi ed bodies shall par ticipate in coordination activities as refer red to in Ar ticle 38. They shall also take par t
directly , or be represente d in, European standardisation organi sations, or ensure that they are aw are and up to dat e in
respect of relevant standards.
Ar ticle 32
Presumption of confor mity with requirements relating to notif ied bodies
Where a conf or mity assessment body demonstrates its conf or mity with the cr iter ia laid do wn in the relevant har monised
standards or par ts thereof, the refe rences of which hav e been published in the Off icial Jour nal of the European Union, it shall
be presumed to comply with the requirements set out in Ar ticle 31 in so f ar as the applicable har monised standards cover
those requirements.
EN
OJ L, 12.7.2024
72/144 ELI: http://data.europa.eu/eli/reg/2024/1689/ojAr ticle 33
Subsidiar ies of notif ied bodies and subcontracting
1. Where a notif ied body subcontracts specif ic tasks connect ed with the conf or mity assessment or has recourse to
a subsidiar y , it shall ensure that the subcontractor or the subsidiar y meets the requirements laid do wn in Ar ticle 31, and
shall inf or m the notifying author ity according ly .
2. Notifi ed bodies shall take full responsibility f or the tasks perf or med by any subcontract ors or subsidiar ies.
3. A ctivities ma y be subcontract ed or car r ied out by a subsidiar y only with the agreement of the provider . Notified
bodies shall mak e a list of their subsidiar ies publicly available.
4. The relevant documents concer ning the assessment of the qualific ations of the subcontract or or the subsidiar y and
the w ork car r ied out by them under this Regulation shall be kep t at the disposal of the notifying author ity f or a per iod of
five y ears from the t er mination dat e of the subcontracting.
Ar ticle 34
Operational obligations of notif ied bodies
1. Notifi ed bodies shall ver ify the conf or mity of high-r isk AI systems in accordance with the conf or mity assessment
procedures set out in Ar ticle 43.
2. Notifi ed bodies shall av oid unnecessar y burdens f or pro viders when perf or ming their activities, and take due account
of the size of the provid er , the secto r in which it operat es, its str ucture and the degree of complexity of the high-r isk AI
syste m concer ned, in par ticular in view of minimising administrative burdens and compliance costs f or micro- and small
ent er pr ises within the meaning of Recommendation 2003/361/EC. The notif ied body shall, never theless, respect the degree
of r igour and the level of prot ection required f or the comp liance of the high-r isk AI system with the requirements of this
Regulation.
3. Notifi ed bodies shall make ava ilable and submit upon request all relevant documentation, including the provider s ’
documentation, to the notifying author ity refe r red to in Ar ticle 28 to allo w that author ity to conduct its assessment,
designation, notification and monitoring activities, and to f acilitate the assessment outlined in this Section.
Ar ticle 35
Identif ication numbers and lists of notif ied bodies
1. The Commission shall assign a sing le identification number to each notif ied body , even where a body is notified under
more than one Uni on act.
2. The Commission shall mak e publicly available the list of the bodies notif ied under this Regulation, including their
identifica tion numbers and the activities f or which they have been notif ied. The Commission shall ensure that the list is k ep t
up t o date.
Ar ticle 36
Changes to notif ications
1. The notifying author ity shall notify the Commission and the other Member States of an y relevant ch anges t o the
notif ication of a notif ied body via the electronic notification to ol refer red to in Ar ticle 30(2).
2. The procedures laid do wn in Ar ticles 29 and 30 shall apply to extensi ons of the scope of the notif ication.
For ch anges to the notif ication other than extensi ons of its scope, the procedures laid do wn in paragraphs (3) to (9) shall
apply .
OJ L, 12.7.2024
EN
ELI: http://data.europa.eu/eli/reg/2024/1689/oj 73/1443. Where a notif ied body decides to cease its conf or mity assessment activities, it shall inf or m the notifying author ity and
the provid ers concer ned as soon as possible and, in the case of a planned cessation, at least one y ear bef ore ceasing its
activities. The cer tif icates of the notified body ma y remain valid f or a per iod of nine months af ter cessation of the notified
body’s activities, on condition that another notified body has confi r med in wr iting that it will assume responsibilities f or the
high-r isk AI syste ms covered by those cer tif icat es. The latte r notified body shall complet e a full assessment of the high-r isk
AI syste ms affect ed by the end of that nine-month-per iod bef ore issuing new cer tif icat es f or those systems. Where the
notif ied body has ceased its activity , the notifying author ity shall withdra w the designation.
4. Where a notifying author ity has suffici ent reason to consider that a notified body no longer meets the requirements
laid down in Ar ticle 31, or that it is f ailing to fulf il its obliga tions, the notifying author ity shall without dela y in vestig ate the
matt er with the utmost diligence. In that context, it shall inf or m the notif ied body concer ned about the objections raised
and give it the possibility to mak e its views known. If the notifying author ity comes to the conclusion that the notified body
no longer meets the requirements laid do wn in Ar ticle 31 or that it is f ailing to fulfil its obligations, it shall restr ict, suspend
or withdra w the designation as appropr iat e, depending on the ser iousness of the f ailure to meet those requirements or fulf il
those obliga tions. It shall immediately inf or m the Commission and the other Member States according ly .
5. Where its designation has been suspended, restr icted, or fully or par tially withdra wn, the notif ied body shall inf or m
the provid ers concer ned within 10 da ys.
6. In the event of the restr iction, suspension or withdrawal of a designation, the notifying author ity shall take
appropr iate steps to ensure that the fi les of the notif ied body concer ned are k ept, and to make them a vailable t o notifying
author ities in other Member States and to mark et sur veillance author ities at their request.
7. In the event of the restr iction, suspension or withdra wal of a designation, the notifying author ity shall:
(a) assess the imp act on the cer tif icates issued b y the notif ied body ;
(b) submit a repor t on its fi ndings to the Commission and the other Member Stat es within three months of hav ing notified
the ch ange s to the designation;
(c) require the notif ied body to suspend or withdra w , within a reasonable per iod of time det er mined by the author ity , any
cer tif icat es which were unduly issued, in order to ensure the continuing conf or mity of high-r isk AI systems on the
marke t ;
(d) inf or m the Commission and the Member States about cer tif icat es the suspension or withdra wal of which it has required;
(e) provid e the national compet ent author ities of the Member Stat e in which the provider has its regist ered place of
business with all relevant inf or mation about the cer tificates of which it has required the suspension or withdra wal; that
author ity shall take the appropr iate measures, where necessar y , t o av oid a pote ntial r isk to health, safety or fundamental
r ights.
8. With the ex ception of cer tif icat es unduly issued, and where a designation has been suspended or restr icted, the
cer tif icat es shall remain valid in one of the f ollowing circumstances:
(a) the notifying author ity has conf ir med, within one month of the suspension or restr iction, that there is no r isk to health,
saf ety or fundamental r ights in relation to cer tif icat es affect ed by the suspension or restr iction, and the notifying
author ity has outlined a timeline f or actions to remedy the suspension or restr iction; or
(b) the notifying author ity has confir med that no cer tif icat es relevant to the suspension will be issued, amended or re-issued
dur ing the course of the suspension or restr iction, and states whether the notif ied body has the capability of continuing
to monitor and remain responsible f or existing cer tif icat es issued f or the per iod of the suspension or restr iction; in the
event that the notifying author ity determ ines that the notif ied body does not have the capability to suppor t existing
cer tif icat es issued, the provider of the system cover ed by the cer tif icate shall conf ir m in wr iting t o the national
compet ent author ities of the Member Stat e in which it has its registered place of business, within three months of the
suspension or restr iction, that another qualif ied notif ied body is te mporar ily assuming the functions of the notif ied
body to monitor and remain responsible f or the cer tif icates dur ing the per iod of suspension or restr iction.
EN
OJ L, 12.7.2024
74/144 ELI: http://data.europa.eu/eli/reg/2024/1689/oj9. With the excep tion of cer tif icates unduly issued, and where a designation has been withdra wn, the cer tif icat es shall
remain valid f or a per iod of nine months under the f ollo wing circumstances:
(a) the national compet ent author ity of the Member State in which the provider of the high-r isk AI syste m cover ed by the
cer tif icat e has its registered place of business has confi r med that there is no r isk to health, safety or fundamental r ights
associate d with the high-r isk AI systems concer ned; and
(b) another notified body has confir med in wr iting that it will assume immediate responsibility f or those AI syste ms and
complet es its assessment within 12 months of the withdra wal of the designation.
In the circumstances refe r red to in the fir st subparagraph, the national comp etent author ity of the Member Stat e in which
the provid er of the syste m covered by the cer tificate has its place of business ma y exte nd the provisional validity of the
cer tif icat es f or additional per iods of three months, which shall not exceed 12 months in total.
The national compet ent author ity or the notif ied body assuming the functions of the notified body affect ed by the chang e
of designation shall immediate ly inf or m the Commission, the other Member States and the other notif ied bodies thereof.
Ar ticle 37
Challenge to the competence of notif ied bodies
1. The Commission shall, where necessar y , in vestigat e all cases where there are reasons to doubt the comp et ence of
a notified body or the continued fulfilment by a notif ied body of the requirements laid down in Ar ticle 31 and of its
applicable responsibilities.
2. The notifying author ity shall provid e the Commission, on request, with all relevant inf or mation relating to the
notif ication or the maintenance of the comp etence of the notified body concer ned.
3. The Commission shall ensure that all sensitive inf or mation obtained in the course of its in vestigations pursuant t o this
Ar ticle is treate d confidentially in accordance with Ar ticle 78.
4. Where the Commission ascer tains that a notif ied body does not meet or no longer meets the requirements f or its
notif ication, it shall inf or m the notifying Member Stat e according ly and request it to take the necessar y cor rective measures,
including the suspension or withdra wal of the notif ication if necessar y . Where the Member Stat e f ails to take the necessar y
cor rective measures, the Commission ma y , by means of an imp lementing act, suspend, restr ict or withdraw the designation.
That im plementing act shall be adop te d in accordance with the examination procedure refer red to in Ar ticle 98(2).
Ar ticle 38
Coordination of notif ied bodies
1. The Commission shall ensure that, with regard to high-r isk AI systems, appropr iate coordination and cooperation
between notified bodies active in the conf or mity assessment procedures pursuant to this Regulation are put in place and
properly operat ed in the f or m of a sect oral group of notified bodies.
2. Each notifying author ity shall ensure that the bodies notified by it par ticipate in the wo rk of a group refer red to in
paragraph 1, directly or through designated representatives.
3. The Commission shall provid e f or the ex ch ange of kno wledge and best practices between notifying author ities.
OJ L, 12.7.2024
EN
ELI: http://data.europa.eu/eli/reg/2024/1689/oj 75/144Ar ticle 39
Conf or mity assessment bodies of third countr ies
Conf or mity assessment bodies established under the law of a third countr y with which the Union has concluded an
agreement ma y be author ised to car r y out the activities of notified bodies under this Regulation, provided that they meet
the requirements laid do wn in Ar ticle 31 or they ensure an equivalent level of compliance.
SECTION 5
Standar ds, conf or mity assessment, cer tificates, r egistr ation
Ar ticle 40
Har monised s tandards and s tandardisation deliv erables
1. High-r isk AI syste ms or g eneral-pur pose AI models which are in conf or mity with har monised standards or par ts
thereof the references of which hav e been published in the Off icial Jour nal of the Eur opean Union in accordance with
Regulation (EU) No 1025/2012 shall be presumed to be in conf or mity with the requirements set out in Section 2 of this
Chapt er or , as applicable, with the obliga tions set out in of Chapt er V , Sections 2 and 3, of this Regulation, to the extent that
those standards cover those requirements or obliga tions.
2. In accordance with Ar ticle 10 of Regulation (EU) No 1025/2012, the Commission shall issue, without undue dela y ,
standardisation requests coveri ng all requirements set out in Section 2 of this Chap ter and, as applicable, standardisation
requests coverin g obliga tions set out in Chapt er V , Sections 2 and 3, of this Regulation. The standardisation request shall
also ask f or deliverables on repor ting and documentation processes to impro ve AI systems’ resource perf or mance, such as
reducing the high-r isk AI system’s consump tion of energy and of other resources dur ing its lifecy cle, and on the
energy-eff icient development of g eneral-pur pose AI models. When prepar ing a standardisation request, the Commission
shall consult the Board and relevant stakeh olders, including the advisor y f or um.
When issuing a standardisation request t o European standardisation organisations, the Commission shall specify that
standards hav e to be clear , consiste nt, including with the standards developed in the var ious sect ors f or products covered by
the existing Uni on har monisation legislation listed in Annex I, and aiming to ensure that high-r isk AI syste ms or
g eneral-pur pose AI models placed on the marke t or put into ser vice in the Union meet the relevant requirements or
obliga tions laid do wn in this Regulation.
The Commission shall request the European standardisation organisations to provide evidence of their best eff or ts to fulf il
the objectives refer red to in the first and the second subparagraph of this paragraph in accordance with Ar ticle 24 of
Regulation (EU) No 1025/2012.
3. The par ticipants in the standardisation process shall seek to promot e invest ment and inno vation in AI, including
through increasing legal cer tainty , as well as the comp etitiveness and gro wth of the Uni on marke t, to contr ibut e to
strengthening g lobal cooperation on standardisation and taking into account existing intern ational standards in the field of
AI that are consiste nt with Union values, fundamental r ights and interests, and to enhance multi-stake holder gove r nance
ensur ing a balanced representation of inte rests and the eff ective par ticipation of all relevant stak eholders in accordance with
Ar ticles 5, 6, and 7 of Regulation (EU) No 1025/2012.
Ar ticle 41
Common specif ications
1. The Commission ma y adopt, imp lementing acts establishing common specifications f or the requirements set out in
Section 2 of this Chapt er or , as applicable, f or the obligations set out in Sections 2 and 3 of Chapt er V where the f ollowing
conditions hav e been fulf illed:
(a) the Commission has requested, pursuant to Ar ticle 10(1) of Regulation (EU) No 1025/2012, one or more European
standardisation organisations to draf t a har monised standard f or the requirements set out in Section 2 of this Chapt er ,
or , as applicable, f or the obliga tions set out in Sections 2 and 3 of Chap ter V , and:
(i) the request has not been accept ed b y an y of the European standardisation org anisations; or
EN
OJ L, 12.7.2024
76/144 ELI: http://data.europa.eu/eli/reg/2024/1689/oj(ii) the har monised standards addressing that request are not delivered within the deadline set in accordance with
Ar ticle 10(1) of Regulation (EU) No 1025/2012; or
(iii) the relevant har monised standards insuff iciently address fundamental r ights concer ns; or
(iv) the har monised standards do not com ply with the request ; and
(b) no refere nce to har monised standards covering the requirements refer red to in Section 2 of this Chapt er or , as
applicable, the oblig ations refe r red to in Sections 2 and 3 of Chap ter V has been published in the Off icial Jour nal of the
Europe an Union in accordance with Regulation (EU) No 1025/2012, and no suc h reference is expect ed to be published
within a reasonable per iod.
When draf ting the common specif ications, the Commission shall consult the advisor y f or um refer red to in Ar ticle 67.
The implementing acts refe r red t o in the fi rst subparagraph of this paragraph shall be adop ted in accordance with the
examination procedure refe r red t o in Ar ticle 98(2).
2. Bef ore prepar ing a draf t implementing act, the Commission shall inf or m the committee referred to in Ar ticle 22 of
Regulation (EU) No 1025/2012 that it considers the conditions laid do wn in paragraph 1 of this Ar ticle to be fulf illed.
3. High-r isk AI syste ms or g eneral-pur pose AI models which are in conf or mity with the common specific ations refe r red
to in paragraph 1, or par ts of those specifications, shall be presumed to be in conf or mity with the requirements set out in
Section 2 of this Chapt er or , as applicable, to comp ly with the oblig ations refer red to in Sections 2 and 3 of Chapt er V , to
the exte nt those common specifications cover those requirements or those obligations.
4. Where a har monised standard is adop ted b y a European standardisation org anisation and proposed to the
Commission f or the publication of its reference in the Off icial Jour nal of the Eur opean Union, the Commission shall assess the
har monised standard in accordance with Regulation (EU) No 1025/2012. When refe rence to a har monised standard is
published in the Off icial Jour nal of the European Union, the Commission shall repeal the imp lementing acts refer red to in
paragraph 1, or par ts thereof which cover the same requirements set out in Section 2 of this Chap ter or , as applicable, the
same obliga tions set out in Sections 2 and 3 of Chap ter V .
5. Where provider s of high-r isk AI syste ms or g eneral-pur pose AI models do not comply with the common
specific ations refe r red to in paragraph 1, they shall duly justify that the y hav e adopt ed tec hnical solutions that meet the
requirements refer red to in Section 2 of this Chapt er or , as applicable, comply with the obligations set out in Sections 2 and
3 of Chapt er V to a level at least equivalent thereto.
6. Where a Member Stat e considers that a common specification does not entirely meet the requirements set out in
Section 2 or , as applicable, com ply with obliga tions set out in Sections 2 and 3 of Chapt er V , it shall inf or m the
Commission thereof with a detailed explanation. The Commission shall assess that inf or mation and, if appropr iate, amend
the im plementing act establishing the common specif ication concer ned.
Ar ticle 42
Presumption of confor mity with cer tain requirements
1. High-r isk AI systems that hav e been trained and test ed on data ref lecting the specif ic g eographical, behavi oural,
cont extual or functional setting within which they are intended to be used shall be presumed to comply with the relevant
requirements laid do wn in Ar ticle 10(4).
2. High-r isk AI syste ms that hav e been cer tif ied or f or which a stat ement of conf or mity has been issued under
a cybersecur ity scheme pursuant to Regulation (EU) 2019/881 and the references of which hav e been published in the
Off icial Jour nal of the European Union shall be presumed t o comply with the cybersecur ity requirements set out in Ar ticle 15
of this Regulation in so f ar as the cybersecur ity cer tif icate or statem ent of conf or mity or par ts thereof cover those
requirements.
OJ L, 12.7.2024
EN
ELI: http://data.europa.eu/eli/reg/2024/1689/oj 77/144Ar ticle 43
Conformit y assessment
1. For high-r isk AI systems listed in point 1 of Annex III, where, in demonstrating the complia nce of a high-r isk AI
syste m with the requirements set out in Section 2, the provid er has applied har monised standards refer red t o in Ar ticle 40,
or , where applicable, common specifications refer red to in Ar ticle 41, the provider shall opt f or one of the f ollowing
conf or mity assessment procedures based on:
(a) the inte r nal control refer red to in Annex VI; or
(b) the assessment of the quality management syste m and the assessment of the te ch nical documentation, with the
inv olvement of a notified body , refe r red t o in Annex VII.
In demonstrating the compliance of a high-r isk AI system with the requirements set out in Section 2, the provid er shall
f ollo w the conf or mity assessment procedure set out in Annex VII where:
(a) har monised standards refer red t o in Ar ticle 40 do not exist, and common specifications refe r red to in Ar ticle 41 are not
ava ilable;
(b) the provid er has not applied, or has applied only par t of, the har monised standard;
(c) the common specifications refer red to in point (a) exist, but the provider has not applied them;
(d) one or more of the har monised standards refe r red to in point (a) has been published with a restr iction, and only on the
par t of the standard that was restr icted.
For the pur poses of the conf or mity assessment procedure refer red t o in Annex VII, the provider ma y ch oose an y of the
notif ied bodies. How ever , where the high-r isk AI syste m is intended to be put into ser vice by law enf orcement, immigration
or asylum author ities or b y Uni on institutions, bodies, off ices or age ncies, the mark et sur veillance author ity refer red t o in
Ar ticle 74(8) or (9), as applicable, shall act as a notified body .
2. For high-r isk AI systems refer red to in points 2 to 8 of Annex III, provider s shall f ollow the conf or mity assessment
procedure based on inter nal control as refe r red to in Annex VI, which does not provide f or the inv olvement of a notif ied
body .
3. For high-r isk AI systems cover ed b y the Uni on har monisation legislation listed in Section A of Annex I, the provid er
shall f ollo w the relevant conf or mity assessment procedure as required under those legal acts. The requirements set out in
Section 2 of this Chapt er shall apply to those high-r isk AI syste ms and shall be par t of that assessment. P oints 4.3., 4.4., 4.5.
and the fif th paragraph of point 4.6 of Annex VII shall also apply .
For the pur poses of that assessment, notif ied bodies which hav e been notif ied under those legal acts shall be entitled to
control the conf or mity of the high-r isk AI syste ms with the requirements set out in Section 2, pro vided that the compliance
of those notif ied bodies with requirements laid do wn in Ar ticle 31(4), (5), (10) and (11) has been assessed in the context of
the notification procedure under those lega l acts.
Where a legal act listed in Section A of Annex I enables the product manufa cturer to opt out from a third-par ty conf or mity
assessment, provided that that manufacture r has applied all har monised standards coveri ng all the relevant requirements,
that manufa cturer ma y use that option only if it has also applied har monised standards or , where applicable, common
specific ations refe r red to in Ar ticle 41, covering all requirements set out in Section 2 of this Chapt er .
4. High-r isk AI systems that hav e already been subject to a conf or mity assessment procedure shall undergo a new
conf or mity assessment procedure in the event of a substantial modification, regardless of whether the modified syste m is
intende d to be fur ther distr ibut ed or continues t o be used b y the cur rent deplo y er .
For high-r isk AI systems that continue t o lear n af te r being placed on the marke t or put into ser vice, chang es to the high-r isk
AI syste m and its perfo r mance that have been pre-dete r mined b y the provid er at the moment of the initial conf or mity
assessment and are par t of the inf or mation contained in the tec hnical documentation refer red to in point 2(f) of Annex IV ,
shall not constitute a substantial modifi cation.
5. The Commission is empo wered to adop t deleg ated acts in accordance with Ar ticle 97 in order to amend Annex es VI
and VII b y updating them in light of tec hnical progress.
EN
OJ L, 12.7.2024
78/144 ELI: http://data.europa.eu/eli/reg/2024/1689/oj6. The Commission is empo wered to adop t deleg ated acts in accordance with Ar ticle 97 in order to amend paragraphs 1
and 2 of this Ar ticle in order to subject high-r isk AI syste ms refe r red t o in points 2 to 8 of Annex III to the conf or mity
assessment procedure refe r red to in Annex VII or par ts thereof. The Commission shall adopt such delegat ed acts taking into
account the effectiveness of the conf or mity assessment procedure based on inte r nal control refe r red to in Annex VI in
preventing or minimising the r isks t o health and saf ety and prot ection of fundamental r ights posed by suc h syste ms, as well
as the ava ilability of adequat e capacities and resources among notified bodies.
Ar ticle 44
Cer tif icates
1. Cer tif icates issued by notif ied bodies in accordance with Annex VII shall be dra wn-up in a languag e which can be
easily understood by the relevant author ities in the Member State in which the notified body is established.
2. Cer tif icates shall be valid f or the per iod they indicate , which shall not ex ceed five y ears f or AI systems covered by
Annex I, and f our y ears f or AI syste ms co vered b y Annex III. At the request of the provider , the validity of a cer tif icate ma y
be exte nded f or fur ther per iods, each not ex ceeding fiv e y ears f or AI systems co vered b y Annex I, and f our years f or AI
syste ms cover ed b y Annex III, based on a re-assessment in accordance with the applicable conf or mity assessment
procedures. Any supplement to a cer tif icate shall remain valid, provided that the cer tif icat e which it supplements is valid.
3. Where a notified body finds that an AI syste m no longer meets the requirements set out in Section 2, it shall, taking
account of the pr inciple of propor tionality , suspend or withdra w the cer tif icat e issued or impose restr ictions on it, unless
complia nce with those requirements is ensured by appropr iat e cor rective action take n by the provider of the system within
an appropr iate deadline set b y the notif ied body . The notif ied body shall give reasons f or its decision.
An appeal procedure aga inst decisions of the notified bodies, including on conf or mity cer tif icat es issued, shall be available .
Ar ticle 45
Infor mation obligations of notif ied bodies
1. Notifi ed bodies shall inf or m the notifying author ity of the f ollo wing:
(a) any Uni on tec hnical documentation assessment cer tificates, an y supplements to those cer tificates, and any quality
managem ent syste m approva ls issued in accordance with the requirements of Annex VII;
(b) any refusal, restr iction, suspension or withdra wal of a Union te chnical documentation assessment cer tif icat e or a quality
managem ent syste m approva l issued in accordance with the requirements of Annex VII;
(c) any circumstances affecting the scope of or conditions f or notif ication;
(d) any request f or inf or mation which the y have received from marke t sur veillance author ities regarding conf or mity
assessment activities;
(e) on request, conf or mity assessment activities perf or med within the scope of their notif ication and any other activity
perfo r med, including cross-border activities and subcontracting.
2. Each notif ied body shall inf or m the other notif ied bodies of:
(a) quality managem ent system approva ls which it has refused, suspended or withdra wn, and, upon request, of quality
syste m approva ls which it has issued;
(b) Uni on tec hnical documentation assessment cer tificates or any supplements thereto which it has refused, withdra wn,
suspended or other wise restr icte d, and, upon request, of the cer tif icates and/or supplements thereto which it has issued.
OJ L, 12.7.2024
EN
ELI: http://data.europa.eu/eli/reg/2024/1689/oj 79/1443. Each notif ied body shall provide the other notified bodies car r ying out similar conf or mity assessment activities
covering the same types of AI systems with relevant inf or mation on issues relating to negative and, on request, positive
conf or mity assessment results.
4. Notifi ed bodies shall safegua rd the confi dentiality of the inf or mation that they obtain, in accordance with Ar ticle 78.
Ar ticle 46
Derogation from confor mity assessment procedure
1. By wa y of derogat ion from Ar ticle 43 and upon a duly justified request, any market sur veillance author ity ma y
author ise the placing on the marke t or the putting into ser vice of specific high-r isk AI systems within the te r r itory of the
Member Stat e concer ned, f or ex ceptiona l reasons of public secur ity or the prot ection of life and health of persons,
envir onmental protection or the prot ection of k ey industr ial and infrastr uctural assets. That author isation shall be f or
a limited per iod while the necessar y conf or mity assessment procedures are being car r ied out, taking into account the
ex ceptional reasons justifying the deroga tion. The completion of those procedures shall be under taken without undue dela y .
2. In a duly justifi ed situation of urg ency f or excep tional reasons of public secur ity or in the case of specif ic, substantial
and imminent threat to the life or physica l safety of natural persons, law -enf orcement author ities or civil prot ection
author ities ma y put a specif ic high-r isk AI system into ser vice without the author isation refe r red t o in paragraph 1,
provid ed that suc h author isation is request ed dur ing or af ter the use without undue dela y . If the author isation referred to in
paragraph 1 is refused, the use of the high-r isk AI system shall be stopp ed with immediate eff ect and all the results and
outputs of suc h use shall be immediately discarded.
3. The author isation refe r red to in paragraph 1 shall be issued only if the marke t sur veillance author ity concludes that
the high-r isk AI syste m comp lies with the requirements of Section 2. The marke t sur veillance author ity shall inf or m the
Commission and the other Member Stat es of any author isation issued pursuant to paragraphs 1 and 2. This obligation shall
not co ver sensitive operational data in relation to the activities of la w-enf orcement author ities.
4. Where, within 15 calendar da ys of receipt of the inf or mation refer red to in paragraph 3, no objection has been raised
by either a Member State or the Commission in respect of an author isation issued by a marke t sur veillance author ity of
a Member State in accordance with paragraph 1, that author isation shall be deemed justified.
5. Where, within 15 calendar da ys of receipt of the notification referred to in paragraph 3, objections are raised by
a Member Stat e against an author isation issued by a mark et sur veillance author ity of another Member State, or where the
Commission considers the author isation to be contrar y to Uni on law , or the conclusion of the Member Stat es rega rding the
complia nce of the syste m as refe r red to in paragraph 3 to be unf ounded, the Commission shall, without dela y , ent er into
consultations with the relevant Member Stat e. The operat ors concer ned shall be consulted and hav e the possibility to
present their views. Ha ving regard thereto, the Commission shall decide whether the author isation is justified. The
Commission shall address its decision to the Member Stat e concer ned and to the relevant operato rs.
6. Where the Commission considers the author isation unjustified, it shall be withdra wn b y the marke t sur veillance
author ity of the Member State concer ned.
7. For high-r isk AI syste ms related t o products covered by Union har monisation legislation listed in Section A of
Annex I, only the derogat ions from the conf or mity assessment established in that Uni on har monisation legislation shall
apply .
Ar ticle 47
EU declaration of confor mity
1. The provider shall dra w up a wr itt en machine readable, physical or electronically signed EU declaration of conf or mity
f or each high-r isk AI system, and k eep it at the disposal of the national comp etent author ities f or 10 y ears af ter the
high-r isk AI syste m has been placed on the mark et or put into ser vice. The EU declaration of conf or mity shall identify the
high-r isk AI system f or which it has been drawn up. A cop y of the EU declaration of conf or mity shall be submitt ed t o the
relevant national comp etent author ities upon request.
EN
OJ L, 12.7.2024
80/144 ELI: http://data.europa.eu/eli/reg/2024/1689/oj2. The EU declaration of conf or mity shall state that the high-r isk AI system concer ned meets the requirements set out in
Section 2. The EU declaration of conf or mity shall contain the inf or mation set out in Annex V , and shall be translated into
a language that can be easily understoo d by the national compet ent author ities of the Member States in which the high-r isk
AI system is placed on the mark et or made available.
3. Where high-r isk AI syste ms are subject t o other Union har monisation legislation which also requires an EU
declaration of conf or mity , a sing le EU declaration of conf or mity shall be drawn up in respect of all Uni on law applicable to
the high-r isk AI system. The declaration shall contain all the inf or mation required t o identify the Uni on har monisation
legislation to which the declaration relates .
4. By drawing up the EU declaration of conf or mity , the provid er shall assume responsibility f or comp liance with the
requirements set out in Section 2. The pro vider shall k eep the EU declaration of conf or mity up-to-dat e as appropr iate.
5. The Commission is emp owered t o adopt delegat ed acts in accordance with Ar ticle 97 in order t o amend Annex V by
updating the cont ent of the EU declaration of conf or mity set out in that Annex, in order to introduce elements that become
necessar y in light of tec hnical progress.
Ar ticle 48
CE marking
1. The CE marking shall be subject to the general pr inciples set out in Ar ticle 30 of Regulation (EC) No 765/2008.
2. For high-r isk AI syste ms provided digitally , a digital CE marking shall be used, only if it can easily be accessed via the
interfac e from which that system is accessed or via an easily accessible machi ne-readable code or other electronic means.
3. The CE marking shall be aff ix ed visibly , legibly and indelibly f or high-r isk AI syste ms. Where that is not possible or
not war rant ed on account of the nature of the high-r isk AI syste m, it shall be affixed to the packag ing or to the
accom panying documentation, as appropr iat e.
4. Where applicable, the CE marking shall be f ollowed b y the identifica tion number of the notified body responsible f or
the conf or mity assessment procedures set out in Ar ticle 43. The identifica tion number of the notif ied body shall be affixed
by the body itself or , under its instr uctions, b y the provid er or by the provid er ’s author ised representative. The identification
number shall also be indicated in any promotional material which mentions that the high-r isk AI system fulf ils the
requirements f or CE marking.
5. Where high-r isk AI systems are subject to other Uni on law which also provid es f or the affixing of the CE marking, the
CE marking shall indicate that the high-r isk AI system also fulf il the requirements of that other law .
Ar ticle 49
Regis tration
1. Bef ore placing on the marke t or putting into ser vice a high-r isk AI system listed in Annex III, with the excep tion of
high-r isk AI syste ms refe r red to in point 2 of Annex III, the provid er or , where applicable, the author ised representative
shall regist er themselves and their syste m in the EU database refe r red to in Ar ticle 71.
2. Bef ore placing on the market or putting into ser vice an AI system f or which the pro vider has concluded that it is not
high-r isk according to Ar ticle 6(3), that provider or , where applicable, the author ised representative shall regist er
themselves and that system in the EU database refer red to in Ar ticle 71.
3. Bef ore putting into ser vice or using a high-r isk AI syste m listed in Annex III, with the excep tion of high-r isk AI
syste ms listed in point 2 of Annex III, deplo y ers that are public author ities, Uni on institutions, bodies, off ices or agencies or
persons acting on their behalf shall regist er themselves, select the syste m and register its use in the EU database referred to
in Ar ticle 71.
OJ L, 12.7.2024
EN
ELI: http://data.europa.eu/eli/reg/2024/1689/oj 81/1444. For high-r isk AI systems refer red to in points 1, 6 and 7 of Annex III, in the areas of law enf orcement, migration,
asylum and border control management, the registration referred t o in paragraphs 1, 2 and 3 of this Ar ticle shall be in
a secure non-public section of the EU database refer red to in Ar ticle 71 and shall include only the f ollo wing inf or mation, as
applicable, referred to in:
(a) Section A, points 1 to 10, of Annex VIII, with the excep tion of points 6, 8 and 9;
(b) Section B, points 1 to 5, and points 8 and 9 of Annex VIII;
(c) Section C, points 1 to 3, of Annex VIII;
(d) points 1, 2, 3 and 5, of Annex IX.
Only the Commission and national author ities refe r red t o in Ar ticle 74(8) shall hav e access to the respective restr icted
sections of the EU database listed in the first subparagraph of this paragraph.
5. High-r isk AI syste ms refer red to in point 2 of Annex III shall be regist ered at national level.
C HAPTER IV
TRANSP ARENCY OBLIGA TIONS F OR PR O VIDERS AND DEPLO YERS OF CERT AIN AI SYSTEMS
Ar ticle 50
T ransparency obligations f or pro viders and deplo y ers of cer ta in AI sys tems
1. Providers shall ensure that AI syste ms intended to interac t directly with natural persons are designed and developed in
suc h a wa y that the natural persons concer ned are inf or med that they are interacting with an AI system, unless this is
obvious from the point of view of a natural person who is reasonably well-inf or med, obser vant and circumspect, taking
into account the circumstances and the cont ext of use. This obligation shall not apply t o AI systems author ised b y law to
detect , prevent, invest igat e or prosecute cr iminal offe nces, subject t o appropr iate saf eguards f or the r ights and freedoms of
third par ties, unless those syste ms are ava ilable f or the public to repor t a cr iminal offence.
2. Providers of AI syste ms, including g eneral-pur pose AI systems, g enerating synthetic audio, image, video or text
cont ent, shall ensure that the outputs of the AI system are marke d in a machine-readable f or mat and detectable as
ar tif icially generat ed or manipulate d. Providers shall ensure their t echnical solutions are effe ctive, inte roperable, robust and
reliable as far as this is t echnically f easible, taking into account the specif icities and limitations of var ious types of cont ent,
the costs of imp lementation and the g enerally ackno wledged state of the ar t, as ma y be ref lected in relevant t echnical
standards. This obligation shall not apply t o the extent the AI systems perfo r m an assistive function f or standard editing or
do not substantially alt er the input data provid ed by the deplo y er or the semantics thereof, or where author ised b y law to
detect , prevent, investig ate or prosecute cr iminal offe nces.
3. Deplo y ers of an emotion recognition syste m or a biometr ic catego r isation syste m shall inf or m the natural persons
exposed thereto of the operation of the syste m, and shall process the personal data in accordance with Regulations (EU)
2016/679 and (EU) 2018/1725 and Directive (EU) 2016/680, as applicable. This obliga tion shall not apply to AI syste ms
used f or biometr ic categor isation and emotion recognition, which are per mitted b y law to detect , prevent or invest igat e
cr iminal offe nces, subject to appropr iate saf eguards f or the r ights and freedoms of third par ties, and in accordance with
Uni on law .
4. Deplo y ers of an AI syste m that gene rates or manipulat es image, audio or video cont ent constituting a deep f ake, shall
disclose that the content has been ar tificially g enerated or manipulated. This obliga tion shall not apply where the use is
author ised by law to detect, prevent, invest igat e or prosecute cr iminal offence. Where the cont ent f or ms par t of an evidently
ar tistic, creative, satir ical, fictional or analogous work or programme, the transparency obligations set out in this paragraph
are limit ed to disclosure of the existe nce of suc h g enerated or manipulate d cont ent in an appropr iate manner that does not
ham per the displa y or enjo yment of the work.
Deplo y ers of an AI system that generat es or manipulate s te xt which is published with the pur pose of inf or ming the public
on matt ers of public intere st shall disclose that the text has been ar tif icially g enerated or manipulate d. This obligation shall
not apply where the use is author ised by law to detect, prevent, investig ate or prosecute cr iminal offences or where the
AI-g enerated cont ent has undergone a process of human review or edito r ial control and where a natural or legal person
holds editor ial responsibility f or the publication of the content.
EN
OJ L, 12.7.2024
82/144 ELI: http://data.europa.eu/eli/reg/2024/1689/oj5. The inf or mation refe r red t o in paragraphs 1 to 4 shall be pro vided to the natural persons concer ned in a clear and
distinguishable manner at the latest at the time of the first inte raction or exposure. The inf or mation shall conf or m t o the
applicable accessibility requirements.
6. Paragraphs 1 to 4 shall not affect the requirements and obligations set out in Chapt er III, and shall be without
prejudice to other transparency oblig ations laid do wn in Union or national law f or deplo y ers of AI systems.
7. The AI Office shall encourage and f acilitate the dra wing up of codes of practice at Uni on level to f acilitate the eff ective
imp lementation of the obligations regard ing the det ection and labelling of ar tif icially g enerated or manipulat ed cont ent.
The Commission ma y adop t imp lementing acts to approve those codes of practice in accordance with the procedure laid
do wn in Ar ticle 56 (6). If it deems the code is not adequat e, the Commission ma y adopt an imp lementing act specifying
common r ules f or the implementation of those obliga tions in accordance with the examination procedure laid do wn in
Ar ticle 98(2).
C HAPTER V
GENERAL -PURPOSE AI MODELS
SECTION 1
Classification r ules
Ar ticle 51
Classif ication of general-pur pose AI models as general-pur pose AI models with sys temic r isk
1. A g eneral-pur pose AI model shall be classified as a g eneral-pur pose AI model with systemic r isk if it meets any of the
f ollo wing conditions:
(a) it has high imp act capabilities evaluated on the basis of appropr iate tec hnical tools and methodologies, including
indicator s and benchmarks;
(b) based on a decision of the Commission, ex of ficio or f ollowi ng a qualif ied aler t from the scientific panel, it has
capabilities or an imp act equivalent to those set out in point (a) hav ing rega rd to the cr iter ia set out in Annex XIII.
2. A g eneral-pur pose AI model shall be presumed t o hav e high im pact capabilities pursuant to paragraph 1, point (a),
when the cumulative amount of computation used f or its training measured in f loating point operations is great er than
10
25
.
3. The Commission shall adopt delegat ed acts in accordance with Ar ticle 97 t o amend the thresholds listed in
paragraphs 1 and 2 of this Ar ticle, as well as to supplement benchmarks and indicator s in light of evolving te chnological
developments, suc h as algor ithmic im provements or increased hardware efficiency , when necessar y , f or these thresholds to
ref lect the state of the ar t.
Ar ticle 52
Procedure
1. Where a gene ral-pur pose AI model meets the condition refe r red to in Ar ticle 51(1), point (a), the relevant provid er
shall notify the Commission without dela y and in any event within two week s af te r that requirement is met or it becomes
kno wn that it will be met. That notif ication shall include the inf or mation necessar y to demonstrate that the relevant
requirement has been met. If the Commission becomes aw are of a g eneral-pur pose AI model presenting syste mic r isk s of
which it has not been notified, it ma y decide to designate it as a model with syste mic r isk.
2. The provid er of a general-pur pose AI model that meets the condition refer red t o in Ar ticle 51(1), point (a), ma y
present, with its notif ication, suff iciently substantiate d arguments to demonstrate that, ex ceptiona lly , although it meets that
requirement, the g eneral-pur pose AI model does not present, due to its specif ic ch aracteristics , syste mic r isk s and theref ore
should not be classif ied as a general-pur pose AI model with systemic r isk.
OJ L, 12.7.2024
EN
ELI: http://data.europa.eu/eli/reg/2024/1689/oj 83/1443. Where the Commission concludes that the arguments submitted pursuant to paragraph 2 are not suffi ciently
substantiate d and the relevant provid er was not able to demonstrate that the general-purpo se AI model does not present,
due t o its specif ic ch aracter istics, syste mic r isks , it shall reject those arguments, and the g eneral-pur pose AI model shall be
considered to be a g eneral-pur pose AI model with syste mic r isk.
4. The Commission ma y designate a g eneral-pur pose AI model as presenting systemic r isks, ex of ficio or f ollowing
a qualif ied aler t from the scientific panel pursuant to Ar ticle 90(1), point (a), on the basis of cr iter ia set out in Annex XIII.
The Commission is emp owered to adop t delegat ed acts in accordance with Ar ticle 97 in order to amend Annex XIII by
specifying and updating the cr iter ia set out in that Annex.
5. Upon a reasoned request of a provid er whose model has been designate d as a general-pur pose AI model with systemic
r isk pursuant to paragraph 4, the Commission shall take the request into account and ma y decide to reassess whether the
g eneral-pur pose AI model can still be considered to present systemic r isks on the basis of the cr ite r ia set out in Annex XIII.
Such a request shall contain objective, detailed and new reasons that hav e ar isen since the designation decision. Provi ders
ma y request reassessment at the earliest six months af t er the designation decision. Where the Commission, f ollowing its
reassessment, decides to maintain the designation as a general-pur pose AI model with syste mic r isk, providers ma y request
reassessment at the earliest six months af te r that decision.
6. The Commission shall ensure that a list of g eneral-pur pose AI models with systemic r isk is published and shall kee p
that list up t o dat e, without prejudice to the need to obser ve and prot ect inte llectual proper ty r ights and confi dential
business inf or mation or trade secrets in accordance with Uni on and national law .
SECTION 2
Oblig ations f or providers of g ener al-pur pose AI models
Ar ticle 53
Obligations for pro viders of general-pur pose AI models
1. Providers of g eneral-pur pose AI models shall:
(a) dra w up and keep up-to-da te the te chnical documentation of the model, including its training and testing process and
the results of its evaluation, which shall contain, at a minimum, the inf or mation set out in Annex XI f or the pur pose of
provid ing it, upon request, to the AI Office and the national compet ent author ities;
(b) dra w up, keep up-to-da te and mak e a vailable inf or mation and documentation t o provid ers of AI syste ms who inte nd to
integrat e the g eneral-pur pose AI model into their AI systems. Without prejudice to the need t o obser ve and prot ect
intellect ual proper ty r ights and confidential business inf or mation or trade secrets in accordance with Union and
national law , the inf or mation and documentation shall:
(i) enable provider s of AI syste ms to hav e a good understanding of the capabilities and limitations of the
g eneral-pur pose AI model and to comply with their obligations pursuant to this Regulation; and
(ii) contain, at a minimum, the elements set out in Annex XII;
(c) put in place a policy to comply with Uni on law on cop yr ight and related r ights, and in par ticular to identify and comply
with, including through state-of-t he-ar t te chnologi es, a reser vation of r ights expressed pursuant to Ar ticle 4(3) of
Directive (EU) 2019/790;
(d) dra w up and make publicly availa ble a sufficiently detailed summar y about the content used f or training of the
g eneral-pur pose AI model, according to a te mplat e provided by the AI Office.
EN
OJ L, 12.7.2024
84/144 ELI: http://data.europa.eu/eli/reg/2024/1689/oj2. The obliga tions set out in paragraph 1, points (a) and (b), shall not apply to providers of AI models that are released
under a free and open-source licence that allo ws f or the access, usage , modif ication, and distr ibution of the model, and
whose paramet ers, including the weights, the inf or mation on the model archit ecture, and the inf or mation on model usage,
are made publicly available . This ex ception shall not apply to g eneral-pur pose AI models with systemic r isks .
3. Providers of g eneral-pur pose AI models shall cooperate as necessar y with the Commission and the national
compet ent author ities in the ex ercise of their comp etences and powers pursuant to this Regulation.
4. Providers of g eneral-pur pose AI models ma y rely on codes of practice within the meaning of Ar ticle 56 to
demonstrate complia nce with the obliga tions set out in paragraph 1 of this Ar ticle, until a har monised standard is
published. Compliance with European har monised standards grants providers the presump tion of conf or mity t o the exte nt
that those standards cover those obliga tions. Providers of g eneral-pur pose AI models who do not adhere to an approved
code of practice or do not comply with a European har monised standard shall demonstrate alt er native adequat e means of
complia nce f or assessment by the Commission.
5. For the pur pose of facilitating compliance with Annex XI, in par ticular points 2 (d) and (e) thereof, the Commission is
emp owered to adop t deleg ated acts in accordance with Ar ticle 97 to detail measurement and calculation methodologies
with a view to allowing f or comp arable and ver ifi able documentation.
6. The Commission is empo wered to adop t deleg ated acts in accordance with Ar ticle 97(2) to amend Annexes XI and XII
in light of ev olving t echnological developments.
7. Any inf or mation or documentation obtained pursuant to this Ar ticle, including trade secrets, shall be treate d in
accordance with the conf identiality obliga tions set out in Ar ticle 78.
Ar ticle 54
Author ised represent ativ es of pro viders of general-pur pose AI models
1. Pr ior to placing a general-pur pose AI model on the Uni on market, provider s established in third countr ies shall, by
wr itten mandat e, appoint an author ised representative which is established in the Union.
2. The provid er shall enable its author ised representative to perf or m the tasks specified in the mandate received from the
provid er .
3. The author ised representative shall perfo r m the tasks specified in the mandate received from the provider . It shall
provid e a cop y of the mandate to the AI Offi ce upon request, in one of the off icial languag es of the institutions of the
Uni on. For the pur poses of this Regulation, the mandate shall empo wer the author ised representative to car r y out the
f ollo wing tasks :
(a) ver ify that the tec hnical documentation specif ied in Annex XI has been drawn up and all obliga tions refe r red to in
Ar ticle 53 and, where applicable, Ar ticle 55 have been fulfilled b y the provider;
(b) keep a cop y of the te ch nical documentation specified in Annex XI at the disposal of the AI Offi ce and national
compet ent author ities, f or a per iod of 10 y ears af ter the g eneral-pur pose AI model has been placed on the mark et, and
the contact details of the provid er that appointed the author ised representative;
(c) provid e the AI Office, upon a reasoned request, with all the inf or mation and documentation, including that refe r red to
in point (b), necessar y to demonstrat e complia nce with the oblig ations in this Chapt er;
(d) cooperate with the AI Office and compet ent author ities, upon a reasoned request, in any action they take in relation to
the g eneral-pur pose AI model, including when the model is integrat ed into AI systems placed on the marke t or put into
ser vice in the Uni on.
4. The mandate shall em power the author ised representative to be addressed, in addition to or inst ead of the provider ,
by the AI Offi ce or the compet ent author ities, on all issues related to ensur ing complia nce with this Regulation.
OJ L, 12.7.2024
EN
ELI: http://data.europa.eu/eli/reg/2024/1689/oj 85/1445. The author ised representative shall te r minate the mandat e if it considers or has reason to consider the provider to be
acting contrar y to its obliga tions pursuant to this Regulation. In suc h a case, it shall also immediate ly inf or m the AI Off ice
about the te r mination of the mandate and the reasons theref or .
6. The obliga tion set out in this Ar ticle shall not apply to providers of g eneral-pur pose AI models that are released under
a free and open-source licence that allows f or the access, usage , modifi cation, and distr ibution of the model, and whose
parameter s, including the weights, the inf or mation on the model arch itecture, and the inf or mation on model usage , are
made publicly a vailable, unless the g eneral-pur pose AI models present systemic r isks .
SECTION 3
Oblig ations of providers of g ener al-pur pose AI models with syste mic r isk
Ar ticle 55
Obligations of pro viders of general-pur pose AI models with sys temic r isk
1. In addition t o the obligations listed in Ar ticles 53 and 54, provid ers of g eneral-pur pose AI models with systemic r isk
shall:
(a) perfo r m model evaluation in accordance with standardised prot ocols and too ls ref lecting the state of the ar t, including
conducting and documenting adversar ial te sting of the model with a view to identifying and mitiga ting syste mic r isks ;
(b) assess and mitig ate possible systemic r isks at Uni on level, including their sources, that ma y st em from the development,
the placing on the marke t, or the use of general-pur pose AI models with syste mic r isk;
(c) keep trac k of, document, and repor t, without undue dela y , to the AI Off ice and, as appropr iate, to national compet ent
author ities, relevant inf or mation about ser ious incidents and possible cor rective measures to address them;
(d) ensure an adequate level of cybersecur ity protect ion f or the gene ral-pur pose AI model with systemic r isk and the
physica l infrastr ucture of the model.
2. Providers of g eneral-pur pose AI models with systemic r isk ma y rely on codes of practice within the meaning of
Ar ticle 56 to demonstrate comp liance with the obliga tions set out in paragraph 1 of this Ar ticle, until a har monised
standard is published. Compliance with European har monised standards grants provid ers the presump tion of conf or mity to
the extent that those standards cover those obligations. Providers of general-pur pose AI models with syste mic r isks who do
not adhere to an approved code of practice or do not comply with a European har monised standard shall demonstrate
alt er native adequate means of compliance f or assessment b y the Commission.
3. Any inf or mation or documentation obtained pursuant to this Ar ticle, including trade secrets, shall be treate d in
accordance with the conf identiality obliga tions set out in Ar ticle 78.
SECTION 4
Codes of pr actice
Ar ticle 56
Codes of practice
1. The AI Offi ce shall encourage and f acilitate the dra wing up of codes of practice at Uni on level in order to contr ibute
to the proper application of this Regulation, taking into account international approaches.
2. The AI Office and the Board shall aim to ensure that the codes of practice co ver at least the obliga tions provid ed f or in
Ar ticles 53 and 55, including the f ollo wing issues:
EN
OJ L, 12.7.2024
86/144 ELI: http://data.europa.eu/eli/reg/2024/1689/oj(a) the means to ensure that the inf or mation refe r red to in Ar ticle 53(1), points (a) and (b), is kep t up t o dat e in light of
marke t and te chnological developments;
(b) the adequat e level of detail f or the summar y about the cont ent used f or training;
(c) the identifica tion of the type and nature of the systemic r isks at Union level, including their sources, where appropr iate;
(d) the measures, procedures and modalities f or the assessment and manag ement of the syste mic r isks at Union level,
including the documentation thereof, which shall be propor tionate t o the r isk s, take into consideration their sever ity
and probability and take into account the specif ic challeng es of tack ling those r isk s in light of the possible wa ys in
which suc h r isk s ma y emerge and materi alise along the AI value chain.
3. The AI Offi ce ma y in vite all provider s of general-pur pose AI models, as well as relevant national comp etent
author ities, to par ticipate in the dra wing-up of codes of practice. Civil society organisations, industr y , academia and other
relevant stak eholders, suc h as do wnstream provider s and independent exper ts, ma y suppor t the process.
4. The AI Offi ce and the Board shall aim to ensure that the codes of practice clearly set out their specif ic objectives and
contain commitments or measures, including ke y perfo r mance indicator s as appropr iate, t o ensure the ac hievement of
those objectives, and that they take due account of the needs and inte rests of all interest ed par ties, including affect ed
persons, at Union level.
5. The AI Office shall aim to ensure that par ticipants to the codes of practice repor t regularly to the AI Office on the
imp lementation of the commitments and the measures taken and their outcomes, including as measured agains t the k ey
perfo r mance indicator s as appropr iate. Ke y perfo r mance indicator s and repor ting commitments shall ref lect diffe rences in
size and capacity between var ious par ticipants.
6. The AI Offi ce and the Board shall regularly monitor and evaluate the achi evement of the objectives of the codes of
practice b y the par ticipants and their contr ibution to the proper application of this Regulation. The AI Office and the Board
shall assess whether the codes of practice cover the obliga tions provided f or in Ar ticles 53 and 55, and shall regularly
monitor and evaluate the achievement of their objectives. They shall publish their assessment of the adequacy of the codes
of practice.
The Commission ma y , b y wa y of an implementing act, approve a code of practice and give it a g eneral validity within the
Uni on. That implementing act shall be adop te d in accordance with the examination procedure refe r red to in Ar ticle 98(2).
7. The AI Office ma y invit e all providers of general-pur pose AI models to adhere to the codes of practice. For provider s
of g eneral-pur pose AI models not presenting syste mic r isks this adherence ma y be limit ed to the obligations provided f or in
Ar ticle 53, unless the y declare explicitly their interest to join the full code.
8. The AI Office shall, as appropr iate, also encourage and f acilitate the review and adaptation of the codes of practice, in
par ticular in light of emerging standards. The AI Off ice shall assist in the assessment of ava ilable standards.
9. Codes of practice shall be ready at the lat est by 2 Ma y 2025. The AI Off ice shall take the necessar y st eps, including
inviting provider s pursuant to paragraph 7.
If, by 2 Au gust 2025, a code of practice cannot be final ised, or if the AI Office deems it is not adequat e f ollowing its
assessment under paragraph 6 of this Ar ticle, the Commission ma y provid e, by means of imp lementing acts, common r ules
f or the implementation of the obliga tions provided f or in Ar ticles 53 and 55, including the issues set out in paragraph 2 of
this Ar ticle. Those imp lementing acts shall be adop ted in accordance with the examination procedure refer red to in Ar ticle
98(2).
OJ L, 12.7.2024
EN
ELI: http://data.europa.eu/eli/reg/2024/1689/oj 87/144CHAPTER VI
MEASURES IN SUPPOR T OF INNO V A TION
Ar ticle 57
AI regulator y sandbo xe s
1. Member Stat es shall ensure that their compet ent author ities establish at least one AI regulato r y sandbo x at national
level, which shall be operational by 2 August 2026. That sandbo x ma y also be established jointly with the comp etent
author ities of other Member States. The Commission ma y provide tec hnical suppor t, advice and to ols f or the establishment
and operation of AI regulatory sandbo xe s.
The obligation under the first subparagraph ma y also be fulfilled b y par ticipating in an existing sandbo x in so far as that
par ticipation provides an equivalent level of national coverag e f or the par ticipating Member Stat es.
2. Additional AI regulator y sandbo xes at regional or local level, or established jointly with the compet ent author ities of
other Member States ma y also be established.
3. The European Data Prot ection Super visor ma y also establish an AI regulator y sandbo x f or Uni on institutions, bodies,
off ices and agencies, and ma y ex ercise the roles and the tasks of national compet ent author ities in accordance with this
Chapt er .
4. Member Stat es shall ensure that the comp etent author ities refe r red to in paragraphs 1 and 2 allocate suffi cient
resources to comply with this Ar ticle effectively and in a timely manner . Where appropr iate, national compet ent author ities
shall cooperate with other relevant author ities, and ma y allow f or the in volvement of other actors within the AI ecosyste m.
This Ar ticle shall not affe ct other regulato r y sandbo xes established under Uni on or national law . Member Stat es shall ensure
an appropr iate level of cooperation between the author ities super vising those other sandbo xes and the national compet ent
author ities.
5. AI regulatory sandbo xe s established under paragraph 1 shall provid e f or a controlled en vironment that f osters
inno vation and f acilitates the development, training, te sting and validation of inno vative AI syste ms f or a limited time
bef ore their being placed on the marke t or put into ser vice pursuant to a specif ic sandbo x plan agreed between the
provid ers or prospective providers and the compet ent author ity . Such sandbo xes ma y include t esting in real wo rld
conditions super vised therein.
6. Compet ent author ities shall provid e, as appropr iat e, guidance, super vision and suppor t within the AI regulator y
sandbo x with a view to identifying r isks, in par ticular to fundamental r ights, health and saf ety , t esting, mitigation measures,
and their effe ctiveness in relation to the obligations and requirements of this Regulation and, where relevant, other Uni on
and national law super vised within the sandbo x.
7. Compet ent author ities shall pro vide provider s and prospective provid ers par ticipating in the AI regulator y sandbo x
with guidance on regulator y expectations and ho w to fulfil the requirements and obligations set out in this Regulation.
Upon request of the provider or prospective provid er of the AI system, the compet ent author ity shall provid e a wr itt en
proof of the activities successfully car r ied out in the sandbo x. The compet ent author ity shall also provide an exit repor t
detailing the activities car r ied out in the sandbo x and the related results and lear ning outcomes. Provi ders ma y use such
documentation to demonstrate their compliance with this Regulation through the conf or mity assessment process or
relevant marke t sur veillance activities. In this regard , the exit repor ts and the wr itten proof provid ed by the national
compet ent author ity shall be taken positively into account by market sur veillance author ities and notif ied bodies, with
a view to accelerating conf or mity assessment procedures t o a reasonable exte nt.
8. Subject t o the confidentiality provisions in Ar ticle 78, and with the agreement of the provider or prospective provider ,
the Commission and the Board shall be author ised to access the exit repor ts and shall take them into account, as
appropr iate, when ex ercising their tasks under this Regulation. If both the provid er or prospective provid er and the national
compet ent author ity explicitly agree, the exit repor t ma y be made publicly ava ilable through the sing le inf or mation
platf or m refer red to in this Ar ticle.
9. The establishment of AI regulator y sandbo xes shall aim to contr ibute to the f ollowing objectives:
(a) imp roving legal cer tainty t o achi eve regulator y complia nce with this Regulation or , where relevant, other applicable
Uni on and national law;
EN
OJ L, 12.7.2024
88/144 ELI: http://data.europa.eu/eli/reg/2024/1689/oj(b) suppor ting the shar ing of best practices through cooperation with the author ities inv olved in the AI regulator y
sandbo x;
(c) f oste r ing inno vation and comp etitiveness and f acilitating the development of an AI ecosyste m;
(d) contr ibuting to evidence-based regulatory lear ning;
(e) f acilitating and accelerating access to the Union marke t f or AI syste ms, in par ticular when provided by SMEs, including
star t-ups.
10. National comp etent author ities shall ensure that, to the exte nt the innovative AI syste ms inv olve the processing of
personal data or other wise fall under the super visor y remit of other national author ities or comp etent author ities providing
or suppor ting access t o data, the national data protect ion author ities and those other national or compet ent author ities are
associate d with the operation of the AI regulator y sandbo x and in volved in the super vision of those aspects to the extent of
their respective tasks and powers.
11. The AI regulato r y sandbo xes shall not affect the super visor y or cor rective po wers of the compet ent author ities
super vising the sandbo xes, including at regional or local level. Any significant r isks to health and saf ety and fundamental
r ights identifi ed dur ing the development and te sting of suc h AI systems shall result in an adequat e mitigation. National
compet ent author ities shall hav e the po wer to te mporar ily or per manently suspend the te sting process, or the par ticipation
in the sandbo x if no effective mitig ation is possible, and shall inf or m the AI Offi ce of suc h decision. National compet ent
author ities shall exer cise their super visor y powe rs within the limits of the relevant law , using their discretionar y po wers
when imp lementing legal provisions in respect of a specific AI regulator y sandbo x project, with the objective of suppor ting
inno vation in AI in the Uni on.
12. Providers and prospective provid ers par ticipating in the AI regulator y sandbo x shall remain liable under applicable
Uni on and national liability law f or any damage inf licted on third par ties as a result of the exper imentation taking place in
the sandbo x. How ever , provid ed that the prospective provider s obser ve the specif ic plan and the t er ms and conditions f or
their par ticipation and f ollow in good f aith the guidance given by the national compet ent author ity , no administrative fines
shall be im posed by the author ities f or infr ingements of this Regulation. Where other com petent author ities responsible f or
other Uni on and national la w were actively inv olved in the super vision of the AI system in the sandbo x and provid ed
guidance f or compliance, no administrative fine s shall be imp osed rega rding that law .
13. The AI regulato r y sandbo xe s shall be designed and imp lemented in suc h a wa y that, where relevant, they facilitat e
cross-border cooperation between national compet ent author ities.
14. National comp etent author ities shall coordinate their activities and cooperate within the framew ork of the Board.
15. National comp etent author ities shall inf or m the AI Office and the Board of the establishment of a sandbo x, and ma y
ask them f or suppor t and guidance. The AI Off ice shall make publicly ava ilable a list of planned and existing sandbo xes and
keep it up to date in order to encourage more inte raction in the AI regulator y sandbo xes and cross-border cooperation.
16. National comp etent author ities shall submit annual repor ts to the AI Office and to the Board, from one year af ter
the establishment of the AI regulatory sandbo x and ever y y ear thereaf ter until its term ination, and a final repor t. Those
repor ts shall provid e inf or mation on the progress and results of the imp lementation of those sandbo xes, including best
practices, incidents, lessons lear nt and recommendations on their setup and, where relevant, on the application and possible
revision of this Regulation, including its deleg ated and impl ementing acts, and on the application of other Uni on la w
super vised by the compet ent author ities within the sandbo x. The national compet ent author ities shall make those annual
repor ts or abstracts thereof a vailable to the public, online. The Commission shall, where appropr iat e, take the annual
repor ts into account when exer cising its tasks under this Regulation.
17. The Commission shall develop a sing le and dedicated interface containing all relevant inf or mation relate d to AI
regulator y sandbo xe s t o allow stak eholders t o interac t with AI regulato r y sandbo xes and to raise enquir ies with compet ent
author ities, and to seek non-binding guidance on the conf or mity of inno vative products, ser vices, business models
embedding AI tec hnologies, in accordance with Ar ticle 62(1), point (c). The Commission shall proactively coordinate with
national compet ent author ities, where relevant.
OJ L, 12.7.2024
EN
ELI: http://data.europa.eu/eli/reg/2024/1689/oj 89/144Ar ticle 58
Detail ed ar rangements f or , and functioning of, AI regulat or y sandbo xes
1. In order to av oid fragmentation across the Uni on, the Commission shall adop t imp lementing acts specifying the
detailed ar rangements f or the establishment, development, im plementation, operation and super vision of the AI regulator y
sandbo xes. The imp lementing acts shall include common pr inciples on the f ollo wing issues:
(a) eligibility and selection cr iteria f or par ticipation in the AI regulator y sandbo x;
(b) procedures f or the application, par ticipation, monitoring, exiting from and te r mination of the AI regulator y sandbo x,
including the sandbo x plan and the exit repor t ;
(c) the t er ms and conditions applicable to the par ticipants.
Those implementing acts shall be adop te d in accordance with the examination procedure refer red to in Ar ticle 98(2).
2. The implementing acts referred to in paragraph 1 shall ensure:
(a) that AI regulator y sandbo xes are open t o an y applying provider or prospective provid er of an AI system who fulfils
eligibility and selection cr iter ia, which shall be transparent and f air , and that national com petent author ities inf or m
applicants of their decision within three months of the application;
(b) that AI regulatory sandbo xes allow broad and equal access and k eep up with demand f or par ticipation; pro viders and
prospective pro viders ma y also submit applications in par tnerships with deplo y ers and other relevant third par ties;
(c) that the detailed ar rang ements f or , and conditions concer ning AI regulator y sandbo xe s suppor t, to the best exte nt
possible, f lexibility f or national comp etent author ities to establish and operate their AI regulatory sandbo xe s;
(d) that access to the AI regulator y sandbo xes is free of ch arge f or SMEs, including star t-ups, without prejudice to
ex ceptional costs that national comp etent author ities ma y reco ver in a f air and propor tionate manner ;
(e) that they facilitat e providers and prospective providers, by means of the lear ning outcomes of the AI regulator y
sandbo xes, in comp lying with conf or mity assessment obligations under this Regulation and the v oluntar y application
of the codes of conduct referred to in Ar ticle 95;
(f) that AI regulator y sandbo xes f acilitate the inv olvement of other relevant actor s within the AI ecosyste m, suc h as notif ied
bodies and standardisation organisations, SMEs, including star t-ups, ent er pr ises, innovat ors, te sting and exper imenta -
tion f acilities, research and exper imentation labs and European Digital Inno vation Hubs, centres of ex cellence,
individual researchers , in order to allow and facilitat e cooperation with the public and pr ivat e secto rs;
(g) that procedures, processes and administrative requirements f or application, selection, par ticipation and exiting the AI
regulator y sandbo x are sim ple, easily inte lligible, and clearly communicated in order to facilitat e the par ticipation of
SMEs, including star t-ups, with limited leg al and administrative capacities and are streamlined across the Uni on, in order
to av oid fragmentation and that par ticipation in an AI regulatory sandbo x established b y a Member Stat e, or b y the
European Data Protection Super visor is mutually and unif or mly recognised and car r ies the same leg al effects across the
Uni on;
(h) that par ticipation in the AI regulato r y sandbo x is limited to a per iod that is appropr iat e t o the complexity and scale of
the project and that ma y be exte nded by the national comp etent author ity ;
(i) that AI regulator y sandbo xes f acilitate the development of to ols and infrastr ucture f or te sting, bench marking, assessing
and explaining dimensions of AI syste ms relevant f or regulator y lear ning, such as accuracy , robustness and
cybersecur ity , as well as measures to mitiga te r isks to fundamental r ights and society at larg e.
3. Prospective provider s in the AI regulatory sandbo xe s, in par ticular SMEs and star t-ups, shall be directed, where
relevant, to pre-deplo yment ser vices such as guidance on the implementation of this Regulation, to other value-adding
ser vices suc h as help with standardisation documents and cer tif ication, te sting and exper imentation f acilities, European
Digital Inno vation Hubs and centres of excellence.
EN
OJ L, 12.7.2024
90/144 ELI: http://data.europa.eu/eli/reg/2024/1689/oj4. Where national comp etent author ities consider author ising testing in real wo rld conditions super vised within the
framew ork of an AI regulator y sandbo x to be established under this Ar ticle, they shall specifically agree the te r ms and
conditions of such te sting and, in par ticular , the appropr iat e saf eguards with the par ticipants, with a view t o protecting
fundamental r ights, health and saf ety . Where appropr iate, they shall cooperate with other national comp etent author ities
with a view to ensur ing consistent practices across the Uni on.
Ar ticle 59
F ur ther processing of personal data f or dev eloping cer tain AI systems in the public interest in the AI regulator y
sandbo x
1. In the AI regulato r y sandbo x, personal data lawfully collect ed f or other pur poses ma y be processed solely f or the
pur pose of developing, training and te sting cer tain AI systems in the sandbo x when all of the f ollowing conditions are met:
(a) AI syste ms shall be developed f or safeguarding substantial public intere st by a public author ity or another natural or
lega l person and in one or more of the f ollo wing areas:
(i) public safety and public health, including disease det ection, diagnosis prevention, control and treatment and
im provement of health care systems;
(ii) a high level of protect ion and im provement of the quality of the envir onment, protection of biodiversity , prot ection
ag ainst pollution, green transition measures, climat e ch ange mitiga tion and adap tation measures;
(iii) energy sustainability ;
(iv) saf ety and resilience of transpor t systems and mobility , cr itical infrastr ucture and networks;
(v) efficiency and quality of public administration and public ser vices;
(b) the data processed are necessar y f or comp lying with one or more of the requirements refer red to in Chap ter III,
Section 2 where those requirements cannot effe ctively be fulf illed b y processing anonymised , synthetic or other
non-personal data;
(c) there are effe ctive monitoring mechanisms to identify if any high r isks to the r ights and freedoms of the data subjects, as
referred t o in Ar ticle 35 of Regulation (EU) 2016/679 and in Ar ticle 39 of Regulation (EU) 2018/1725, ma y ar ise
dur ing the sandbo x exper imentation, as well as response mec hanisms to promp tly mitiga te those r isks and, where
necessar y , stop the processing;
(d) any personal data to be processed in the cont ext of the sandbo x are in a functionally separate , isolated and prot ected
data processing environment under the control of the prospective pro vider and only author ised persons have access to
those data;
(e) provid ers can fur ther share the or iginally collect ed data only in accordance with Uni on data protection law; any
personal data create d in the sandbo x cannot be shared outside the sandbo x;
(f) any processing of personal data in the context of the sandbo x neither leads to measures or decisions affecting the data
subjects nor does it affe ct the application of their r ights laid down in Uni on law on the prot ection of personal data;
(g) any personal data processed in the cont ext of the sandbo x are protect ed by means of appropr iate te chnical and
org anisational measures and deleted once the par ticipation in the sandbo x has te r minated or the personal data has
reac hed the end of its ret ention per iod;
(h) the logs of the processing of personal data in the cont ext of the sandbo x are kep t f or the duration of the par ticipation in
the sandbo x, unless provid ed other wise by Uni on or national law;
(i) a comp let e and detailed descr ipt ion of the process and rationale behind the training, te sting and validation of the AI
syste m is kep t t ogether with the te sting results as par t of the t echnical documentation refer red to in Annex IV ;
OJ L, 12.7.2024
EN
ELI: http://data.europa.eu/eli/reg/2024/1689/oj 91/144(j) a shor t summar y of the AI project developed in the sandbo x, its objectives and expecte d results is published on the
website of the compet ent author ities; this obligati on shall not co ver sensitive operational data in relation to the activities
of law enf orcement, border control, immigration or asylum author ities.
2. For the pur poses of the prevention, invest igation, detection or prosecution of cr iminal offe nces or the ex ecution of
cr iminal penalties, including saf eguarding aga inst and preventing threats to public secur ity , under the control and
responsibility of law enf orcement author ities, the processing of personal data in AI regulato r y sandbo xes shall be based on
a specific Union or national la w and subject to the same cumulative conditions as refe r red to in paragraph 1.
3. Paragraph 1 is without prejudice to Uni on or national law which ex cludes processing of personal data f or other
pur poses than those explicitly mentioned in that law , as well as to Union or national law la ying do wn the basis f or the
processing of personal data which is necessar y f or the pur pose of developing, te sting or training of inno vative AI systems or
any other legal basis, in comp liance with Union la w on the prot ection of personal data.
Ar ticle 60
T est ing of high-r isk AI sys tems in real w orld conditions outside AI regulator y sandbo xes
1. T esting of high-r isk AI systems in real world conditions outside AI regulator y sandbo xes ma y be conducted by
provid ers or prospective provider s of high-r isk AI systems list ed in Annex III, in accordance with this Ar ticle and the
real-world t esting plan referred to in this Ar ticle, without prejudice t o the prohibitions under Ar ticle 5.
The Commission shall, b y means of implementing acts, specify the detailed elements of the real-wo rld te sting plan. Those
imp lementing acts shall be adopt ed in accordance with the examination procedure refer red to in Ar ticle 98(2).
This paragraph shall be without prejudice to Uni on or national law on the te sting in real w orld conditions of high-r isk AI
syste ms related t o products covered by Union har monisation legislation list ed in Annex I.
2. Providers or prospective provid ers ma y conduct te sting of high-r isk AI syste ms refe r red t o in Annex III in real wo rld
conditions at any time bef ore the placing on the mark et or the putting into ser vice of the AI syste m on their own or in
par tnership with one or more deplo y ers or prospective deplo yers.
3. The te sting of high-r isk AI syste ms in real world conditions under this Ar ticle shall be without prejudice to any ethical
review that is required by Uni on or national law .
4. Providers or prospective provider s ma y conduct the t esting in real world conditions only where all of the f ollowing
conditions are met:
(a) the provid er or prospective provider has dra wn up a real-world te sting plan and submitted it to the market sur veillance
author ity in the Member Stat e where the te sting in real wo rld conditions is to be conducte d;
(b) the marke t sur veillance author ity in the Member Stat e where the t esting in real world conditions is to be conducte d has
approved the te sting in real wo rld conditions and the real-world testing plan; where the marke t sur veillance author ity
has not provided an answer within 30 da ys, the testing in real wo rld conditions and the real-world te sting plan shall be
understood to hav e been approved; where national law does not provid e f or a tacit approva l, the te sting in real wo rld
conditions shall remain subject to an author isation;
(c) the provid er or prospective provider , with the excep tion of provider s or prospective provider s of high-r isk AI syste ms
referred to in points 1, 6 and 7 of Annex III in the areas of law enf orcement, migration, asylum and border control
managem ent, and high-r isk AI systems referred to in point 2 of Annex III has registered the te sting in real wo rld
conditions in accordance with Ar ticle 71(4) with a Union-wide unique sing le identification number and with the
inf or mation specified in Annex IX; the provider or prospective provider of high-r isk AI systems refe r red to in points 1,
6 and 7 of Annex III in the areas of law enf orcement, migration, asylum and border control management, has register ed
the testing in real-world conditions in the secure non-public section of the EU database according to Ar ticle 49(4), point
(d), with a Union-wide unique sing le identifica tion number and with the inf or mation specified therein; the provid er or
prospective provid er of high-r isk AI systems refe r red to in point 2 of Annex III has regist ered the te sting in real-wo rld
conditions in accordance with Ar ticle 49(5);
EN
OJ L, 12.7.2024
92/144 ELI: http://data.europa.eu/eli/reg/2024/1689/oj(d) the provider or prospective provider conducting the testing in real wo rld conditions is established in the Union or has
appointed a legal representative who is established in the Uni on;
(e) data collected and processed f or the pur pose of the testing in real world conditions shall be transf er red to third
countr ies only provided that appropr iate and applicable saf eguards under Uni on law are imp lemented;
(f) the te sting in real w orld conditions does not last longer than necessar y t o ac hieve its objectives and in any case not
longer than six months, which ma y be exte nded f or an additional per iod of six months, subject to pr ior notif ication by
the pro vider or prospective provider to the marke t sur veillance author ity , accom panied b y an explanation of the need
f or suc h an extension;
(g) the subjects of the te sting in real wo rld conditions who are persons belonging to vulnerable groups due to their age or
disability , are appropr iately prot ected;
(h) where a pro vider or prospective pro vider org anises the te sting in real world conditions in cooperation with one or more
deplo yers or prospective deplo y ers, the latter have been inf or med of all aspects of the te sting that are relevant to their
decision to par ticipate, and given the relevant instr uctions f or use of the AI system refer red to in Ar ticle 13; the
provid er or prospective provider and the deplo y er or prospective deplo yer shall conclude an agreement specifying their
roles and responsibilities with a view to ensur ing comp liance with the provisions f or testing in real w orld conditions
under this Regulation and under other applicable Uni on and national law;
(i) the subjects of the t esting in real w orld conditions have given inf or med consent in accordance with Ar ticle 61, or in the
case of law enf orcement, where the seeking of inf or med consent would prevent the AI system from being test ed, the
te sting itself and the outcome of the testing in the real wo rld conditions shall not hav e an y neg ative eff ect on the
subjects, and their personal data shall be deleted af ter the te st is perfo r med;
(j) the te sting in real w orld conditions is effe ctively overseen by the provider or prospective pro vider , as well as by
deplo yers or prospective deplo y ers through persons who are suitably qualified in the relevant field and have the
necessar y capacity , training and author ity to perf or m their tasks ;
(k) the predictions, recommendations or decisions of the AI syste m can be effectively reversed and disreg arded.
5. Any subjects of the te sting in real w orld conditions, or their leg ally designated representative, as appropr iate, ma y ,
without any resulting detr iment and without having to provide any justifi cation, withdra w from the te sting at any time by
revok ing their inf or med consent and ma y request the immediate and per manent deletion of their personal data. The
withdra wal of the inf or med consent shall not affe ct the activities already car r ied out.
6. In accordance with Ar ticle 75, Member Stat es shall confer on their market sur veillance author ities the po wers of
requir ing providers and prospective provid ers t o provide inf or mation, of car r ying out unannounced remot e or on-site
inspections, and of perfor ming ch ecks on the conduct of the te sting in real w orld conditions and the related high-r isk AI
syste ms. Marke t sur veillance author ities shall use those po wers to ensure the safe development of testing in real wo rld
conditions.
7. Any ser ious incident identifi ed in the course of the te sting in real w orld conditions shall be repor te d to the national
marke t sur veillance author ity in accordance with Ar ticle 73. The provid er or prospective provid er shall adopt immediate
mitiga tion measures or , failing that, shall suspend the te sting in real world conditions until suc h mitigation take s place, or
other wise ter minate it. The provider or prospective pro vider shall establish a procedure f or the promp t recall of the AI
syste m upon suc h te r mination of the te sting in real world conditions.
8. Providers or prospective pro viders shall notify the national market sur veillance author ity in the Member Stat e where
the te sting in real wo rld conditions is to be conducte d of the suspension or term ination of the t esting in real wo rld
conditions and of the final outcomes.
9. The provid er or prospective provid er shall be liable under applicable Union and national liability law f or any damage
caused in the course of their testing in real w orld conditions.
OJ L, 12.7.2024
EN
ELI: http://data.europa.eu/eli/reg/2024/1689/oj 93/144Ar ticle 61
Infor med consent to par ticipate in tes ting in real w orld conditions outside AI regulat or y sandbo xes
1. For the pur pose of te sting in real wo rld conditions under Ar ticle 60, freely-given inf or med consent shall be obtained
from the subjects of te sting pr ior to their par ticipation in suc h te sting and af te r their hav ing been duly inf or med with
concise, clear , relevant, and understandable inf or mation regarding:
(a) the nature and objectives of the testing in real world conditions and the possible inconve nience that ma y be linke d to
their par ticipation;
(b) the conditions under which the t esting in real w orld conditions is to be conducted, including the expecte d duration of
the subject or subjects’ par ticipation;
(c) their r ights, and the guarantees regard ing their par ticipation, in par ticular their r ight to refuse to par ticipate in, and the
r ight to withdraw from, testing in real wo rld conditions at any time without any resulting detr iment and without having
to provid e any justificati on;
(d) the ar rang ements f or requesting the reversal or the disregarding of the predictions, recommendations or decisions of
the AI syste m;
(e) the Uni on-wide unique sing le identification number of the t esting in real world conditions in accordance with Ar ticle
60(4) point (c), and the contact details of the provid er or its lega l representative from whom fur ther inf or mation can be
obtained.
2. The inf or med consent shall be dated and documented and a cop y shall be given to the subjects of te sting or their leg al
representative.
Ar ticle 62
Measures f or pro viders and deplo y ers, in par ticular SMEs, including st ar t-ups
1. Member Stat es shall under take the f ollowing actions:
(a) provid e SMEs, including star t-ups, hav ing a registered off ice or a branc h in the Uni on, with pr ior ity access to the AI
regulator y sandbo xe s, to the exte nt that the y fulf il the eligibility conditions and selection cr iter ia; the pr ior ity access
shall not preclude other SMEs, including star t-ups, other than those refer red to in this paragraph from access to the AI
regulator y sandbo x, provided that they also fulf il the eligibility conditions and selection cr ite r ia;
(b) org anise specif ic aw areness raising and training activities on the application of this Regulation tailored to the needs of
SMEs including star t-ups, deplo y ers and, as appropr iat e, local public author ities;
(c) utilise existing dedicat ed channels and where appropr iate, establish new ones f or communication with SMEs including
star t-ups, deplo yers, other innovat ors and, as appropr iate, local public author ities to provide advice and respond to
quer ies about the implementation of this Regulation, including as rega rds par ticipation in AI regulator y sandbo xes;
(d) f acilitate the par ticipation of SMEs and other relevant stak eholders in the standardisation development process.
2. The specific interests and needs of the SME provid ers, including star t-ups, shall be take n into account when setting the
f ees f or conf or mity assessment under Ar ticle 43, reducing those f ees propor tionately to their size, marke t size and other
relevant indicator s.
3. The AI Office shall under take the f ollowing actions:
(a) provid e standardised tem plate s f or areas co vered by this Regulation, as specif ied by the Board in its request ;
(b) develop and maintain a sing le inf or mation platf or m provid ing easy to use inf or mation in relation to this Regulation f or
all operat ors across the Union;
EN
OJ L, 12.7.2024
94/144 ELI: http://data.europa.eu/eli/reg/2024/1689/oj(c) org anise appropr iate communication campaigns to raise a wareness about the obligati ons ar ising from this Regulation;
(d) evaluate and promot e the con vergence of best practices in public procurement procedures in relation to AI systems.
Ar ticle 63
Derogations f or specif ic operat ors
1. Microenter pr ises within the meaning of Recommendation 2003/361/EC ma y comply with cer tain elements of the
quality management system required by Ar ticle 17 of this Regulation in a simplified manner , provid ed that the y do not
hav e par tner ent er pr ises or linked enter pr ises within the meaning of that Recommendation. For that pur pose, the
Commission shall develop guidelines on the elements of the quality manag ement system which ma y be comp lied with in
a simplified manner consider ing the needs of microenterp r ises, without affecting the level of protection or the need f or
complia nce with the requirements in respect of high-r isk AI syste ms.
2. Paragraph 1 of this Ar ticle shall not be inter preted as ex emp ting those operat ors from fulfilling any other
requirements or oblig ations laid do wn in this Regulation, including those established in Ar ticles 9, 10, 11, 12, 13, 14, 15,
72 and 73.
C HAPTER VII
GO VERNANCE
SECTION 1
Gove r nance at Union le v el
Ar ticle 64
AI Off ice
1. The Commission shall develop Union exper tise and capabilities in the field of AI through the AI Off ice.
2. Member Stat es shall facilitat e the tasks entr uste d t o the AI Off ice, as ref lecte d in this Regulation.
Ar ticle 65
Es tablishment and s tr ucture of the European Ar tif icial Intelligence Board
1. A European Ar tif icial Intellig ence Board (the ‘Board’) is hereb y established.
2. The Board shall be comp osed of one representative per Member Stat e. The European Data Protection Super visor shall
par ticipate as obser ver . The AI Office shall also attend the Board’s meetings, without taking par t in the v otes. Other national
and Uni on author ities, bodies or exper ts ma y be invit ed to the meetings by the Board on a case by case basis, where the
issues discussed are of relevance f or them.
3. Each representative shall be designated by their Member Stat e f or a per iod of three y ears, renewable once.
4. Member Stat es shall ensure that their representatives on the Board:
(a) hav e the relevant compet ences and po wers in their Member State so as to contr ibute actively to the achievement of the
Board’s tasks refer red to in Ar ticle 66;
(b) are designate d as a sing le contact point vis-à-vis the Board and, where appropr iate, taking into account Member States’
needs, as a sing le contact point f or stakeholders;
OJ L, 12.7.2024
EN
ELI: http://data.europa.eu/eli/reg/2024/1689/oj 95/144(c) are emp ower ed to f acilitate consistency and coordination between national compet ent author ities in their Member State
as rega rds the imp lementation of this Regulation, including through the collection of relevant data and inf or mation f or
the pur pose of fulf illing their tasks on the Board.
5. The designated representatives of the Member States shall adop t the Board’s r ules of procedure by a two-thirds
majorit y . The r ules of procedure shall, in par ticular , la y do wn procedures f or the selection process, the duration of the
mandate of, and specif ications of the tasks of, the Chair , detailed ar rang ements f or voting, and the organi sation of the
Board’s activities and those of its sub-groups.
6. The Board shall establish two standing sub-groups to pro vide a platf or m f or cooperation and ex chang e among marke t
sur veillance author ities and notifying author ities about issues related to market sur veillance and notified bodies respectively .
The standing sub-group f or marke t sur veillance should act as the administrative cooperation group (ADCO) f or this
Regulation within the meaning of Ar ticle 30 of Regulation (EU) 2019/1020.
The Board ma y establish other standing or te mporar y sub-groups as appropr iate f or the pur pose of examining specif ic
issues. Where appropr iate, representatives of the advisor y f or um refer red t o in Ar ticle 67 ma y be in vited to suc h sub-groups
or t o specif ic meetings of those subgroups as obser vers.
7. The Board shall be org anised and operat ed so as to saf eguard the objectivity and imp ar tiality of its activities.
8. The Board shall be ch aired by one of the representatives of the Member Stat es. The AI Offi ce shall provid e the
secretar iat f or the Board, con vene the meetings upon request of the Chair , and prepare the age nda in accordance with the
tasks of the Board pursuant t o this Regulation and its r ules of procedure.
Ar ticle 66
T asks of the Board
The Board shall advise and assist the Commission and the Member Stat es in order t o facilitat e the consistent and eff ective
application of this Regulation. T o that end, the Board ma y in par ticular:
(a) contr ibut e to the coordination among national comp etent author ities responsible f or the application of this Regulation
and, in cooperation with and subject to the agreement of the marke t sur veillance author ities concer ned, suppor t joint
activities of marke t sur veillance author ities refe r red to in Ar ticle 74(11);
(b) collect and share te chnical and regulato r y exper tise and best practices among Member Stat es;
(c) provid e advice on the imp lementation of this Regulation, in par ticular as regards the enf orcement of r ules on
g eneral-pur pose AI models;
(d) contr ibut e to the har monisation of administrative practices in the Member States, including in relation t o the
derogat ion from the conf or mity assessment procedures refer red to in Ar ticle 46, the functioning of AI regulator y
sandbo xes, and te sting in real world conditions referred to in Ar ticles 57, 59 and 60;
(e) at the request of the Commission or on its ow n initiative, issue recommendations and wr itten opinions on an y relevant
matt ers related to the implementation of this Regulation and to its consiste nt and effective application, including:
(i) on the development and application of codes of conduct and codes of practice pursuant to this Regulation, as well
as of the Commission’s guidelines;
(ii) the evaluation and review of this Regulation pursuant to Ar ticle 112, including as rega rds the ser ious incident
repor ts refe r red to in Ar ticle 73, and the functioning of the EU database refe r red to in Ar ticle 71, the preparation
of the deleg ated or implementing acts, and as regard s possible alignments of this Regulation with the Uni on
har monisation legislation list ed in Annex I;
(iii) on tec hnical specif ications or existing standards regarding the requirements set out in Chapt er III, Section 2;
EN
OJ L, 12.7.2024
96/144 ELI: http://data.europa.eu/eli/reg/2024/1689/oj(iv) on the use of har monised standards or common specifications refer red to in Ar ticles 40 and 41;
(v) trends, suc h as European g lobal comp etitiveness in AI, the uptak e of AI in the Union, and the development of
digital skills;
(vi) trends on the evolving typology of AI value chains, in par ticular on the resulting implications in te r ms of
accountability ;
(vii) on the potential need f or amendment to Annex III in accordance with Ar ticle 7, and on the potential need f or
possible revision of Ar ticle 5 pursuant to Ar ticle 112, taking into account relevant ava ilable evidence and the
lat est developments in te chnology ;
(f) suppor t the Commission in promoting AI literac y , public aw areness and understanding of the benefi ts, r isks,
saf eguards and r ights and obliga tions in relation to the use of AI syste ms;
(g) f acilitate the development of common cr iteria and a shared understanding among market operat ors and compet ent
author ities of the relevant concep ts provid ed f or in this Regulation, including b y contr ibuting to the development of
bench mark s;
(h) cooperate, as appropr iate, with other Uni on institutions, bodies, offices and ag encies, as well as relevant Uni on exper t
groups and networks, in par ticular in the fie lds of product safety , cybersecur ity , comp etition, digital and media ser vices,
financ ial ser vices, consumer prot ection, data and fundamental r ights prot ection;
(i) contr ibut e to effe ctive cooperation with the compet ent author ities of third countr ies and with inter national
org anisations;
(j) assist national comp etent author ities and the Commission in developing the organi sational and tec hnical exper tise
required f or the imp lementation of this Regulation, including b y contr ibuting to the assessment of training needs f or
staff of Member States in volved in im plementing this Regulation;
(k) assist the AI Offi ce in suppor ting national comp et ent author ities in the establishment and development of AI
regulator y sandbo xe s, and f acilitate cooperation and inf or mation-shar ing among AI regulator y sandbo xes;
(l) contr ibut e to, and provide relevant advice on, the development of guidance documents;
(m) advise the Commission in relation t o inter national matt ers on AI;
(n) provid e opinions t o the Commission on the qualif ied aler ts regard ing g eneral-pur pose AI models;
(o) receive opinions by the Member Stat es on qualified aler ts regard ing g eneral-pur pose AI models, and on national
exper iences and practices on the monitoring and enf orcement of AI syste ms, in par ticular systems inte grating the
g eneral-pur pose AI models.
Ar ticle 67
Advisor y f or um
1. An advisor y f or um shall be established to provid e te chnical exper tise and advise the Board and the Commission, and
to contr ibut e to their tasks under this Regulation.
2. The membership of the advisor y f or um shall represent a balanced selection of stak eholders, including industr y ,
star t-ups, SMEs, civil society and academia. The membership of the advisor y f or um shall be balanced with rega rd to
commercial and non-commercial interests and, within the catego r y of commercial inte rests, with regard to SMEs and other
under takings.
3. The Commission shall appoint the members of the advisor y f or um, in accordance with the cr ite r ia set out in
paragraph 2, from amongst stakeholders with recognised exper tise in the fi eld of AI.
OJ L, 12.7.2024
EN
ELI: http://data.europa.eu/eli/reg/2024/1689/oj 97/1444. The t er m of offic e of the members of the advisor y f or um shall be two years, which ma y be extended by up to no more
than f our y ears.
5. The Fundamental Rights Ag ency , ENISA, the European Committe e f or Standardization (CEN), the European
Committe e f or Electrotec hnical Standardization (CENELEC), and the European T elecommunications Standards Institute
(ETSI) shall be per manent members of the advisor y f or um.
6. The advisor y f or um shall draw up its r ules of procedure. It shall elect tw o co-chairs from among its members, in
accordance with cr iter ia set out in paragraph 2. The ter m of off ice of the co-c hairs shall be two y ears, renewable once.
7. The advisor y f or um shall hold meetings at least twice a y ear . The advisor y f or um ma y invit e exper ts and other
stak eholders to its meetings.
8. The advisor y f or um ma y prepare opinions, recommendations and wr itt en contr ibutions at the request of the Board or
the Commission.
9. The advisor y f or um ma y establish standing or tem porar y sub-groups as appropr iate f or the pur pose of examining
specific questions related to the objectives of this Regulation.
10. The advisor y f or um shall prepare an annual repor t on its activities. That repor t shall be made publicly available .
Ar ticle 68
Scientif ic panel of independent exper ts
1. The Commission shall, by means of an imp lementing act, mak e provis ions on the establishment of a scientific panel
of independent exper ts (the ‘scientific panel’) inte nded to suppor t the enf orcement activities under this Regulation. That
imp lementing act shall be adop ted in accordance with the examination procedure refe r red to in Ar ticle 98(2).
2. The scientific panel shall consist of exper ts selected by the Commission on the basis of up-to -date scientific or
te chnical exper tise in the field of AI necessar y f or the tasks set out in paragraph 3, and shall be able to demonstrate meeting
all of the f ollowing conditions:
(a) hav ing par ticular exper tise and compet ence and scientific or t echnical exper tise in the fie ld of AI;
(b) independence from any provid er of AI syste ms or general-pur pose AI models;
(c) an ability to car r y out activities dilige ntly , accurately and objectively .
The Commission, in consultation with the Board, shall determine the number of exper ts on the panel in accordance with
the required needs and shall ensure f air g ender and g eographical representation.
3. The scientifi c panel shall advise and suppor t the AI Office, in par ticular with rega rd to the f ollowing tasks:
(a) suppor ting the im plementation and enf orcement of this Regulation as regard s g eneral-pur pose AI models and systems,
in par ticular by:
(i) aler ting the AI Office of possible systemic r isks at Uni on level of general-pur pose AI models, in accordance with
Ar ticle 90;
(ii) contr ibuting t o the development of to ols and methodologies f or evaluating capabilities of g eneral-pur pose AI
models and syste ms, including through bench mark s;
(iii) pro viding advice on the classif ication of g eneral-pur pose AI models with systemic r isk;
(iv) pro viding advice on the classif ication of var ious g eneral-pur pose AI models and systems;
EN
OJ L, 12.7.2024
98/144 ELI: http://data.europa.eu/eli/reg/2024/1689/oj(v) contr ibuting to the development of tools and tem plates;
(b) suppor ting the w ork of market sur veillance author ities, at their request ;
(c) suppor ting cross-border marke t sur veillance activities as referred to in Ar ticle 74(11), without prejudice t o the po wers
of marke t sur veillance author ities;
(d) suppor ting the AI Offi ce in car r ying out its duties in the cont ext of the Union safeguard procedure pursuant to
Ar ticle 81.
4. The exper ts on the scientific panel shall perfor m their tasks with impar tiality and objectivity , and shall ensure the
confi dentiality of inf or mation and data obtained in car r ying out their tasks and activities. They shall neither seek nor take
instr uctions from any one when ex ercising their tasks under paragraph 3. Each exper t shall draw up a declaration of
intere sts, which shall be made publicly ava ilable. The AI Office shall establish systems and procedures to actively manage
and prevent pote ntial conf licts of interest.
5. The im plementing act refer red to in paragraph 1 shall include provisions on the conditions, procedures and detailed
ar rangements f or the scientific panel and its members to issue aler ts, and to request the assistance of the AI Off ice f or the
perfo r mance of the tasks of the scientifi c panel.
Ar ticle 69
A ccess to the pool of exper ts by the Member States
1. Member States ma y call upon exper ts of the scientific panel to suppor t their enf orcement activities under this
Regulation.
2. The Member States ma y be required to pa y f ees f or the advice and suppor t provid ed by the exper ts. The str ucture and
the level of f ees as well as the scale and str ucture of reco verable costs shall be set out in the imp lementing act refer red to in
Ar ticle 68(1), taking into account the objectives of the adequat e imp lementation of this Regulation, cost-effectiveness and
the necessity of ensur ing effe ctive access to exper ts f or all Member States.
3. The Commission shall f acilitate timely access to the exper ts by the Member States, as needed, and ensure that the
combination of suppor t activities car r ied out by Uni on AI te sting suppor t pursuant to Ar ticle 84 and exper ts pursuant to
this Ar ticle is effi ciently organised and provides the best possible added value.
SECTION 2
National compe tent author ities
Ar ticle 70
Designation of national competent author ities and single points of cont act
1. Each Member State shall establish or designate as national compet ent author ities at least one notifying author ity and
at least one marke t sur veillance author ity f or the pur poses of this Regulation. Those national comp etent author ities shall
ex ercise their powers independently , impar tially and without bias so as to saf eguard the objectivity of their activities and
tasks , and to ensure the application and implementation of this Regulation. The members of those author ities shall refrain
from an y action incom patible with their duties. Provided that those pr inciples are obser ved, suc h activities and tasks ma y be
perfo r med by one or more designate d author ities, in accordance with the organisational needs of the Member Stat e.
2. Member Stat es shall communicate to the Commission the identity of the notifying author ities and the marke t
sur veillance author ities and the tasks of those author ities, as well as any subsequent ch ange s thereto . Member States shall
make publicly availa ble inf or mation on ho w compet ent author ities and sing le points of contact can be contact ed, through
electronic communication means by 2 Au gust 2025. Member Stat es shall designate a market sur veillance author ity to act as
the sing le point of contact f or this Regulation, and shall notify the Commission of the identity of the sing le point of contact.
The Commission shall make a list of the sing le points of contact publicly a vailable.
OJ L, 12.7.2024
EN
ELI: http://data.europa.eu/eli/reg/2024/1689/oj 99/1443. Member States shall ensure that their national compet ent author ities are provided with adequate tec hnical, financial
and human resources, and with infrastr ucture to fulf il their tasks effe ctively under this Regulation. In par ticular , the national
compet ent author ities shall hav e a suffi cient number of personnel per manently a vailable whose comp etences and exper tise
shall include an in-depth understanding of AI te ch nologies, data and data computing , personal data prot ection,
cybersecur ity , fundamental r ights, health and safety r isks and kno wledge of existing standards and legal requirements.
Member States shall assess and, if necessar y , update compet ence and resource requirements refe r red to in this paragraph on
an annual basis.
4. National compet ent author ities shall take appropr iate measures to ensure an adequate level of cybersecur ity .
5. When perfo r ming their tasks, the national com petent author ities shall act in accordance with the confi dentiality
obliga tions set out in Ar ticle 78.
6. By 2 Au gust 2025, and once ever y tw o years thereaf te r , Member Stat es shall repor t to the Commission on the status
of the financ ial and human resources of the national comp et ent author ities, with an assessment of their adequacy . The
Commission shall transmit that inf or mation to the Board f or discussion and possible recommendations.
7. The Commission shall f acilitate the exc hang e of exper ience between national comp et ent author ities.
8. National comp etent author ities ma y provid e guidance and advice on the imp lementation of this Regulation, in
par ticular to SMEs including star t-ups, taking into account the guidance and advice of the Board and the Commission, as
appropr iate. Whenever national comp etent author ities intend t o provide guidance and advice with rega rd to an AI system
in areas covered b y other Uni on law , the national comp et ent author ities under that Union la w shall be consult ed, as
appropr iate.
9. Where Uni on institutions, bodies, off ices or agencies fall within the scope of this Regulation, the European Data
Protection Super visor shall act as the compet ent author ity f or their super vision.
CHAPTER VIII
EU D A T ABASE FOR HIGH-RISK AI SYSTEMS
Ar ticle 71
EU dat abase f or high-r isk AI systems listed in Annex III
1. The Commission shall, in collaboration with the Member Stat es, set up and maintain an EU database containing
inf or mation refer red to in paragraphs 2 and 3 of this Ar ticle concer ning high-r isk AI systems refe r red to in Ar ticle 6(2)
which are registered in accordance with Ar ticles 49 and 60 and AI systems that are not considered as high-r isk pursuant to
Ar ticle 6(3) and which are registered in accordance with Ar ticle 6(4) and Ar ticle 49. When setting the functional
specific ations of suc h database, the Commission shall consult the relevant exper ts, and when updating the functional
specific ations of suc h database, the Commission shall consult the Board.
2. The data listed in Sections A and B of Annex VIII shall be ent ered into the EU database by the provid er or , where
applicable, by the author ised representative.
3. The data listed in Section C of Annex VIII shall be entered into the EU database by the deplo y er who is, or who acts on
behalf of, a public author ity , ag ency or body , in accordance with Ar ticle 49(3) and (4).
4. With the ex ception of the section refe r red to in Ar ticle 49(4) and Ar ticle 60(4), point (c), the inf or mation contained in
the EU database register ed in accordance with Ar ticle 49 shall be accessible and publicly a vailable in a user -fr iendly manner .
The inf or mation should be easily navig able and machi ne-readable. The inf or mation regist ered in accordance with Ar ticle 60
shall be accessible only to marke t sur veillance author ities and the Commission, unless the prospective provider or provid er
has given consent f or also making the inf or mation accessible the public.
5. The EU database shall contain personal data only in so f ar as necessar y f or collecting and processing inf or mation in
accordance with this Regulation. That inf or mation shall include the names and contact details of natural persons who are
responsible f or registeri ng the syste m and hav e the lega l author ity t o represent the provid er or the deplo y er , as applicable.
EN
OJ L, 12.7.2024
100/144 ELI: http://data.europa.eu/eli/reg/2024/1689/oj6. The Commission shall be the controller of the EU database. It shall make available to providers, prospective provider s
and deplo y ers adequat e tec hnical and administrative suppor t. The EU database shall comply with the applicable accessibility
requirements.
CHAPTER IX
POST -MARKET MONITORING, INFORMA TION SHARING AND MARKET SUR VEILL ANCE
SECTION 1
P ost-market monitor ing
Ar ticle 72
Po s t-mark et monitorin g by pro viders and pos t-mark et monitor ing plan for high-r isk AI sys tems
1. Providers shall establish and document a post-mark et monitoring syste m in a manner that is propor tionate to the
nature of the AI tec hnologies and the r isks of the high-r isk AI system.
2. The post-mark et monito r ing system shall actively and systematically collect, document and analyse relevant data
which ma y be provided by deplo y ers or which ma y be collected through other sources on the perform ance of high-r isk AI
syste ms throughout their lifetime, and which allow the provider to evaluate the continuous complia nce of AI systems with
the requirements set out in Chap ter III, Section 2. Where relevant, post-mark et monitoring shall include an analysis of the
interac tion with other AI syste ms. This obligati on shall not co ver sensitive operational data of deplo y ers which are
law -enf orcement author ities.
3. The post-mark et monitori ng system shall be based on a post-mark et monitoring plan. The post-mark et monitoring
plan shall be par t of the tec hnical documentation referred t o in Annex IV . The Commission shall adopt an imp lementing act
la ying do wn detailed provisions establishing a te mplat e f or the post-mark et monitoring plan and the list of elements to be
included in the plan b y 2 Febr uar y 2026. That imp lementing act shall be adop ted in accordance with the examination
procedure refe r red to in Ar ticle 98(2).
4. For high-r isk AI syste ms co vered by the Uni on har monisation legislation listed in Section A of Annex I, where
a post-market monitoring system and plan are already established under that legislation, in order t o ensure consiste ncy ,
av oid duplications and minimise additional burdens, provid ers shall have a choice of integrat ing, as appropr iate, the
necessar y elements descr ibed in paragraphs 1, 2 and 3 using the te mplat e refer red in paragraph 3 into syste ms and plans
already existing under that legislation, provid ed that it ac hieves an equivalent level of protect ion.
The fir st subparagraph of this paragraph shall also apply to high-r isk AI syste ms refe r red to in point 5 of Annex III placed
on the market or put into ser vice by financ ial institutions that are subject to requirements under Uni on financ ial ser vices
law regarding their inter nal gove r nance, ar rang ements or processes.
SECTION 2
Shar ing of inf or mation on ser ious incidents
Ar ticle 73
Repor ting of ser ious incidents
1. Providers of high-r isk AI syste ms placed on the Union marke t shall repor t any ser ious incident to the marke t
sur veillance author ities of the Member Stat es where that incident occur red.
OJ L, 12.7.2024
EN
ELI: http://data.europa.eu/eli/reg/2024/1689/oj 101/1442. The repor t refe r red to in paragraph 1 shall be made immediate ly af ter the provider has established a causal link
between the AI system and the ser ious incident or the reasonable likelihood of suc h a link , and, in any event, not later than
15 da ys af te r the provid er or , where applicable, the deplo y er , becomes aw are of the ser ious incident.
The per iod f or the repor ting refer red to in the fi rst subparagraph shall take account of the sever ity of the ser ious incident.
3. Notwithstanding paragraph 2 of this Ar ticle, in the event of a widespread infr ingem ent or a ser ious incident as
def ined in Ar ticle 3, point (49)(b), the repor t refe r red to in paragraph 1 of this Ar ticle shall be provid ed immediately , and
not lat er than tw o da ys af t er the provider or , where applicable, the deplo y er becomes aw are of that incident.
4. Notwithstanding paragraph 2, in the event of the death of a person, the repor t shall be provided immediately af te r the
provid er or the deplo y er has established, or as soon as it suspects, a causal relationship between the high-r isk AI syste m and
the ser ious incident, but not lat er than 10 da ys af te r the date on which the provider or , where applicable, the deplo y er
becomes aw are of the ser ious incident.
5. Where necessar y to ensure timely repor ting, the pro vider or , where applicable, the deplo yer , ma y submit an initial
repor t that is incom plete, f ollo wed by a comp let e repor t.
6. Following the repor ting of a ser ious incident pursuant to paragraph 1, the provid er shall, without dela y , perf or m the
necessar y in vestigations in relation to the ser ious incident and the AI syste m concer ned. This shall include a r isk assessment
of the incident, and cor rective action.
The provider shall cooperate with the compet ent author ities, and where relevant with the notif ied body concer ned, dur ing
the invest igations refer red to in the fi rst subparagraph, and shall not perfo r m any investig ation which in volves alt er ing the
AI syste m concer ned in a wa y which ma y affe ct any subsequent evaluation of the causes of the incident, pr ior to inf or ming
the comp et ent author ities of suc h action.
7. Upon receiving a notif ication related to a ser ious incident refer red to in Ar ticle 3, point (49)(c), the relevant marke t
sur veillance author ity shall inf or m the national public author ities or bodies referred to in Ar ticle 77(1). The Commission
shall develop dedicated guidance to f acilitate comp liance with the obligations set out in paragraph 1 of this Ar ticle. That
guidance shall be issued by 2 August 2025, and shall be assessed regularly .
8. The market sur veillance author ity shall take appropr iate measures, as provid ed f or in Ar ticle 19 of Regulation (EU)
2019/1020, within seven da ys from the date it received the notif ication refer red to in paragraph 1 of this Ar ticle, and shall
f ollo w the notification procedures as provided in that Regulation.
9. For high-r isk AI syste ms refe r red to in Annex III that are placed on the market or put into ser vice b y provider s that are
subject to Uni on legislative instr uments la ying do wn repor ting obligati ons equivalent t o those set out in this Regulation, the
notif ication of ser ious incidents shall be limit ed to those referred t o in Ar ticle 3, point (49)(c).
10. For high-r isk AI syste ms which are saf ety components of devices, or are themselves devices, co vered by Regulations
(EU) 2017/745 and (EU) 2017/746, the notification of ser ious incidents shall be limit ed to those refer red to in Ar ticle 3,
point (49)(c) of this Regulation, and shall be made to the national compet ent author ity chosen f or that pur pose by the
Member Stat es where the incident occur red.
11. National compet ent author ities shall immediately notify the Commission of any ser ious incident, whether or not
they hav e take n action on it, in accordance with Ar ticle 20 of Regulation (EU) 2019/1020.
SECTION 3
Enf or cement
Ar ticle 74
Marke t sur v eillance and control of AI sys tems in the Union mark et
1. Regulation (EU) 2019/1020 shall apply to AI systems covered b y this Regulation. For the pur poses of the eff ective
enf orcement of this Regulation:
EN
OJ L, 12.7.2024
102/144 ELI: http://data.europa.eu/eli/reg/2024/1689/oj(a) any reference to an economic operator under Regulation (EU) 2019/1020 shall be understood as including all operat ors
identifie d in Ar ticle 2(1) of this Regulation;
(b) any reference to a product under Regulation (EU) 2019/1020 shall be understood as including all AI syste ms f alling
within the scope of this Regulation.
2. As par t of their repor ting obliga tions under Ar ticle 34(4) of Regulation (EU) 2019/1020, the marke t sur veillance
author ities shall repor t annually to the Commission and relevant national competition author ities any inf or mation
identifie d in the course of marke t sur veillance activities that ma y be of potential interest f or the application of Uni on law on
competition r ules. They shall also annually repor t t o the Commission about the use of prohibite d practices that occur red
dur ing that y ear and about the measures take n.
3. For high-r isk AI systems related to products cover ed by the Uni on har monisation legislation listed in Section A of
Annex I, the marke t sur veillance author ity f or the pur poses of this Regulation shall be the author ity responsible f or marke t
sur veillance activities designated under those leg al acts.
By derogat ion from the first subparagraph, and in appropr iate circumstances, Member Stat es ma y designate another
relevant author ity to act as a market sur veillance author ity , provid ed the y ensure coordination with the relevant sect oral
marke t sur veillance author ities responsible f or the enf orcement of the Union har monisation legislation listed in Annex I.
4. The procedures refe r red to in Ar ticles 79 to 83 of this Regulation shall not apply to AI syste ms relate d to products
covered by the Uni on har monisation legislation listed in section A of Annex I, where such lega l acts already pro vide f or
procedures ensur ing an equivalent level of prot ection and hav ing the same objective. In such cases, the relevant sect oral
procedures shall apply instead.
5. Without prejudice to the po wers of market sur veillance author ities under Ar ticle 14 of Regulation (EU) 2019/1020,
f or the pur pose of ensur ing the effe ctive enf orcement of this Regulation, marke t sur veillance author ities ma y ex ercise the
powe rs refe r red to in Ar ticle 14(4), points (d) and (j), of that Regulation remotely , as appropr iat e.
6. For high-r isk AI syste ms placed on the marke t, put into ser vice, or used by financ ial institutions regulated by Uni on
financ ial ser vices law , the marke t sur veillance author ity f or the pur poses of this Regulation shall be the relevant national
author ity responsible f or the financ ial super vision of those institutions under that legislation in so f ar as the placing on the
marke t, putting into ser vice, or the use of the AI system is in direct connection with the provision of those fi nancial
ser vices.
7. By wa y of derogation from paragraph 6, in appropr iat e circumstances, and provided that coordination is ensured,
another relevant author ity ma y be identified by the Member State as marke t sur veillance author ity f or the pur poses of this
Regulation.
National market sur veillance author ities super vising regulated credit institutions regulate d under Directive 2013/36/EU,
which are par ticipating in the Sing le Super visor y Mec hanism established by Regulation (EU) No 1024/2013, should repor t,
without dela y , to the European Central Bank an y inf or mation identified in the course of their marke t sur veillance activities
that ma y be of potent ial interest f or the pr udential super visor y tasks of the European Central Bank specified in that
Regulation.
8. For high-r isk AI syste ms listed in point 1 of Annex III to this Regulation, in so f ar as the systems are used f or la w
enf orcement pur poses, border manag ement and justice and democracy , and f or high-r isk AI systems listed in points 6, 7
and 8 of Annex III to this Regulation, Member States shall designate as market sur veillance author ities f or the pur poses of
this Regulation either the comp etent data prot ection super visor y author ities under Regulation (EU) 2016/679 or Directive
(EU) 2016/680, or any other author ity designate d pursuant to the same conditions laid down in Ar ticles 41 to 44 of
Directive (EU) 2016/680. Market sur veillance activities shall in no wa y affect the independence of judicial author ities, or
other wise inte r f ere with their activities when acting in their judicial capacity .
9. Where Uni on institutions, bodies, off ices or agencies fall within the scope of this Regulation, the European Data
Protection Super visor shall act as their market sur veillance author ity , excep t in relation to the Cour t of Justice of the
European Union acting in its judicial capacity .
10. Member States shall f acilitate coordination between marke t sur veillance author ities designate d under this Regulation
and other relevant national author ities or bodies which super vise the application of Uni on har monisation legislation list ed
in Annex I, or in other Uni on law , that might be relevant f or the high-r isk AI syste ms refer red to in Annex III.
OJ L, 12.7.2024
EN
ELI: http://data.europa.eu/eli/reg/2024/1689/oj 103/14411. Market sur veillance author ities and the Commission shall be able to propose joint activities, including joint
invest igations, to be conducted by either market sur veillance author ities or mark et sur veillance author ities jointly with the
Commission, that have the aim of promoting compliance, identifying non-compliance, raising aw areness or providing
guidance in relation t o this Regulation with respect to specif ic cate gor ies of high-r isk AI systems that are f ound to present
a ser ious r isk across tw o or more Member States in accordance with Ar ticle 9 of Regulation (EU) 2019/1020. The AI Off ice
shall provid e coordination suppor t f or joint investig ations.
12. Without prejudice to the powers provided f or under Regulation (EU) 2019/1020, and where relevant and limited to
what is necessar y to fulfil their tasks , the marke t sur veillance author ities shall be granted full access by provid ers to the
documentation as well as the training, validation and te sting data sets used f or the development of high-r isk AI systems,
including, where appropr iate and subject to secur ity saf eguards, through application programming interfaces (API) or other
relevant te ch nical means and tools enabling remot e access.
13. Market sur veillance author ities shall be granted access to the source code of the high-r isk AI syste m upon a reasoned
request and only when both of the f ollowing conditions are fulfilled:
(a) access to source code is necessar y to assess the conf or mity of a high-r isk AI syste m with the requirements set out in
Chapt er III, Section 2; and
(b) te sting or auditing procedures and ver ificati ons based on the data and documentation provided by the provider have
been exhaust ed or pro ved insuff icient.
14. Any inf or mation or documentation obtained by marke t sur veillance author ities shall be treate d in accordance with
the confidentiality obligations set out in Ar ticle 78.
Ar ticle 75
Mutual assis tance, market sur v eillance and control of general-pur pose AI sys tems
1. Where an AI system is based on a general-pur pose AI model, and the model and the system are developed by the
same provider , the AI Office shall hav e po wers t o monito r and super vise comp liance of that AI syste m with obligations
under this Regulation. T o car r y out its monitoring and super vision tasks, the AI Offi ce shall hav e all the powers of a marke t
sur veillance author ity pro vided f or in this Section and Regulation (EU) 2019/1020.
2. Where the relevant marke t sur veillance author ities hav e suffi cient reason to consider general-pur pose AI systems that
can be used directly b y deplo y ers f or at least one pur pose that is classif ied as high-r isk pursuant t o this Regulation t o be
non-comp liant with the requirements laid do wn in this Regulation, the y shall cooperate with the AI Office to car r y out
complia nce evaluations, and shall inf or m the Board and other market sur veillance author ities according ly .
3. Where a marke t sur veillance author ity is unable to conclude its invest igation of the high-r isk AI system because of its
inability to access cer tain inf or mation related to the general-pur pose AI model despite having made all appropr iate eff or ts
to obtain that inf or mation, it ma y submit a reasoned request t o the AI Office, by which access to that inf or mation shall be
enf orced. In that case, the AI Off ice shall supply to the applicant author ity without dela y , and in any event within 30 da ys,
any inf or mation that the AI Office considers to be relevant in order to establish whether a high-r isk AI system is
non-comp liant. Marke t sur veillance author ities shall safegua rd the confidentiality of the inf or mation that the y obtain in
accordance with Ar ticle 78 of this Regulation. The procedure provided f or in Chapt er VI of Regulation (EU) 2019/1020
shall apply mutatis mutandis.
Ar ticle 76
Super vision of testing in real w orld conditions by market sur v eillance author ities
1. Market sur veillance author ities shall hav e compet ences and powers to ensure that testing in real world conditions is in
accordance with this Regulation.
EN
OJ L, 12.7.2024
104/144 ELI: http://data.europa.eu/eli/reg/2024/1689/oj2. Where t esting in real wo rld conditions is conducte d f or AI systems that are super vised within an AI regulator y
sandbo x under Ar ticle 58, the marke t sur veillance author ities shall ver ify the compliance with Ar ticle 60 as par t of their
super visor y role f or the AI regulator y sandbo x. Those author ities ma y , as appropr iate, allo w the testing in real wo rld
conditions to be conducte d by the provid er or prospective provid er , in deroga tion from the conditions set out in Ar ticle
60(4), points (f) and (g).
3. Where a market sur veillance author ity has been inf or med b y the prospective provider , the provid er or any third par ty
of a ser ious incident or has other grounds f or consider ing that the conditions set out in Ar ticles 60 and 61 are not met, it
ma y take either of the f ollowing decisions on its te r r itory , as appropr iate:
(a) to suspend or t er minate the te sting in real world conditions;
(b) to require the provid er or prospective provider and the deplo yer or prospective deplo y er to modify an y aspect of the
te sting in real world conditions.
4. Where a marke t sur veillance author ity has take n a decision refe r red to in paragraph 3 of this Ar ticle, or has issued an
objection within the meaning of Ar ticle 60(4), point (b), the decision or the objection shall indicate the grounds theref or
and ho w the provid er or prospective pro vider can ch alleng e the decision or objection.
5. Where applicable, where a marke t sur veillance author ity has taken a decision refer red to in paragraph 3, it shall
communicate the grounds theref or to the marke t sur veillance author ities of other Member Stat es in which the AI system
has been test ed in accordance with the te sting plan.
Ar ticle 77
P o w ers of author ities protecting fundamental r ights
1. National public author ities or bodies which super vise or enf orce the respect of oblig ations under Uni on la w
protect ing fundamental r ights, including the r ight to non-discr imination, in relation to the use of high-r isk AI syste ms
referred to in Annex III shall hav e the po wer t o request and access any documentation create d or maintained under this
Regulation in accessible language and f or mat when access t o that documentation is necessar y f or eff ectively fulf illing their
mandates within the limits of their jur isdiction. The relevant public author ity or body shall inf or m the marke t sur veillance
author ity of the Member State concer ned of any suc h request.
2. By 2 No vember 2024, each Member State shall identify the public author ities or bodies referred to in paragraph 1 and
make a list of them publicly ava ilable. Member Stat es shall notify the list t o the Commission and to the other Member
Stat es, and shall keep the list up to date.
3. Where the documentation refer red t o in paragraph 1 is insuff icient to ascer tain whether an infr ing ement of
obliga tions under Uni on law prot ecting fundamental r ights has occur red, the public author ity or body refer red to in
paragraph 1 ma y make a reasoned request to the marke t sur veillance author ity , to org anise te sting of the high-r isk AI
syste m through tec hnical means. The market sur veillance author ity shall org anise the te sting with the close inv olvement of
the requesting public author ity or body within a reasonable time f ollo wing the request.
4. Any inf or mation or documentation obtained by the national public author ities or bodies referred to in paragraph 1 of
this Ar ticle pursuant to this Ar ticle shall be treated in accordance with the confidentiality obligations set out in Ar ticle 78.
Ar ticle 78
Conf identiality
1. The Commission, marke t sur veillance author ities and notif ied bodies and any other natural or lega l person in volved
in the application of this Regulation shall, in accordance with Uni on or national law , respect the confi dentiality of
inf or mation and data obtained in car r ying out their tasks and activities in such a manner as to prot ect, in par ticular:
OJ L, 12.7.2024
EN
ELI: http://data.europa.eu/eli/reg/2024/1689/oj 105/144(a) the intellectual proper ty r ights and confidential business inf or mation or trade secrets of a natural or lega l person,
including source code, excep t in the cases refe r red to in Ar ticle 5 of Directive (EU) 2016/943 of the European
Parliame nt and of the Council (
57
);
(b) the eff ective imp lementation of this Regulation, in par ticular f or the pur poses of inspections, investig ations or audits;
(c) public and national secur ity inte rests;
(d) the conduct of cr iminal or administrative proceedings;
(e) inf or mation classif ied pursuant to Uni on or national law .
2. The author ities inv olved in the application of this Regulation pursuant to paragraph 1 shall request only data that is
str ictly necessar y f or the assessment of the r isk posed by AI syste ms and f or the exercise of their powers in accordance with
this Regulation and with Regulation (EU) 2019/1020. They shall put in place adequate and eff ective cybersecur ity measures
to protect the secur ity and conf identiality of the inf or mation and data obtained, and shall delete the data collected as soon
as it is no longe r needed f or the pur pose f or which it was obtained, in accordance with applicable Union or national law .
3. Without prejudice to paragraphs 1 and 2, inf or mation exc hang ed on a confidential basis between the national
compet ent author ities or between national compet ent author ities and the Commission shall not be disclosed without pr ior
consultation of the or iginating national compet ent author ity and the deplo yer when high-r isk AI systems refe r red to in
point 1, 6 or 7 of Annex III are used b y law enf orcement, border control, immigration or asylum author ities and when such
disclosure wo uld jeopardise public and national secur ity interests. This ex ch ange of inf or mation shall not cover sensitive
operational data in relation to the activities of law enf orcement, border control, immigration or asylum author ities.
When the law enf orcement, immigration or asylum author ities are provid ers of high-r isk AI systems referred to in point 1,
6 or 7 of Annex III, the te chnical documentation refer red to in Annex IV shall remain within the premises of those
author ities. Those author ities shall ensure that the mark et sur veillance author ities refe r red to in Ar ticle 74(8) and (9), as
applicable, can, upon request, immediate ly access the documentation or obtain a cop y thereof. Only staff of the marke t
sur veillance author ity holding the appropr iate level of secur ity clearance shall be allowed to access that documentation or
any cop y thereof.
4. Paragraphs 1, 2 and 3 shall not affe ct the r ights or obliga tions of the Commission, Member States and their relevant
author ities, as well as those of notif ied bodies, with regard to the exc hang e of inf or mation and the dissemination of
war nings, including in the cont ext of cross-border cooperation, nor shall they affect the oblig ations of the par ties concer ned
to provid e inf or mation under cr iminal law of the Member Stat es.
5. The Commission and Member Stat es ma y ex chang e, where necessar y and in accordance with relevant provisions of
inter national and trade agreements, confi dential inf or mation with regulator y author ities of third countr ies with which they
hav e concluded bilat eral or multilate ral conf identiality ar rang ements guarante eing an adequate level of confidentiality .
Ar ticle 79
Procedure at national lev el f or dealing with AI systems presenting a r isk
1. AI syste ms presenting a r isk shall be understood as a ‘product presenting a r isk’ as defined in Ar ticle 3, point 19 of
Regulation (EU) 2019/1020, in so f ar as the y present r isks to the health or saf ety , or to fundamental r ights, of persons.
2. Where the marke t sur veillance author ity of a Member Stat e has sufficient reason to consider an AI system to present
a r isk as refer red to in paragraph 1 of this Ar ticle, it shall car r y out an evaluation of the AI syste m concer ned in respect of
its comp liance with all the requirements and obligations laid do wn in this Regulation. Pa r ticular attention shall be given to
AI syste ms presenting a r isk to vulnerable groups. Where r isks to fundamental r ights are identified, the market sur veillance
author ity shall also inf or m and fully cooperate with the relevant national public author ities or bodies referred to in Ar ticle
77(1). The relevant operat ors shall cooperate as necessar y with the marke t sur veillance author ity and with the other
national public author ities or bodies refe r red to in Ar ticle 77(1).
EN
OJ L, 12.7.2024
106/144 ELI: http://data.europa.eu/eli/reg/2024/1689/oj
(
57
) Directive (EU) 2016/943 of the European Parliament and of the Council of 8 June 2016 on the prot ection of undisclosed know-ho w
and business inf or mation (trade secrets) against their unlaw ful acquisition, use and disclosure (OJ L 157, 15.6.2016, p. 1).Where, in the course of that evaluation, the market sur veillance author ity or , where applicable the market sur veillance
author ity in cooperation with the national public author ity referred to in Ar ticle 77(1), finds that the AI system does not
comply with the requirements and obligations laid do wn in this Regulation, it shall without undue dela y require the relevant
operato r t o take all appropr iate cor rective actions to br ing the AI syste m into comp liance, t o withdra w the AI system from
the mark et, or to recall it within a per iod the marke t sur veillance author ity ma y prescr ibe, and in any event within the
shor ter of 15 wo rking da ys, or as provided f or in the relevant Uni on har monisation legislation.
The marke t sur veillance author ity shall inf or m the relevant notif ied body according ly . Ar ticle 18 of Regulation (EU)
2019/1020 shall apply to the measures refe r red to in the second subparagraph of this paragraph.
3. Where the mark et sur veillance author ity considers that the non-comp liance is not restr ict ed to its national te r r itory , it
shall inf or m the Commission and the other Member Stat es without undue dela y of the results of the evaluation and of the
actions which it has required the operato r to take .
4. The operator shall ensure that all appropr iat e cor rective action is take n in respect of all the AI syste ms concer ned that
it has made available on the Union marke t.
5. Where the operator of an AI system does not take adequate cor rective action within the per iod refer red to in
paragraph 2, the market sur veillance author ity shall take all appropr iat e provisional measures to prohibit or restr ict the AI
syste m’s being made available on its national marke t or put into ser vice, to withdra w the product or the standalone AI
syste m from that marke t or to recall it. That author ity shall without undue dela y notify the Commission and the other
Member Stat es of those measures.
6. The notification refer red t o in paragraph 5 shall include all ava ilable details, in par ticular the inf or mation necessar y
f or the identification of the non-comp liant AI syste m, the or igin of the AI syste m and the supply ch ain, the nature of the
non-comp liance alleged and the r isk in volved, the nature and duration of the national measures taken and the arguments
put f or ward by the relevant operat or . In par ticular , the market sur veillance author ities shall indicate whether the
non-comp liance is due to one or more of the f ollo wing:
(a) non-comp liance with the prohibition of the AI practices refe r red t o in Ar ticle 5;
(b) a f ailure of a high-r isk AI system to meet requirements set out in Chapt er III, Section 2;
(c) shor tcomings in the har monised standards or common specifications refer red to in Ar ticles 40 and 41 confer r ing
a presump tion of conf or mity ;
(d) non-comp liance with Ar ticle 50.
7. The marke t sur veillance author ities other than the mark et sur veillance author ity of the Member State initiating the
procedure shall, without undue dela y , inf or m the Commission and the other Member States of any measures adopt ed and of
any additional inf or mation at their disposal relating to the non-compliance of the AI system concer ned, and, in the event of
disagreement with the notif ied national measure, of their objections.
8. Where, within three months of receipt of the notification refe r red to in paragraph 5 of this Ar ticle, no objection has
been raised by either a marke t sur veillance author ity of a Member State or b y the Commission in respect of a provis ional
measure taken b y a marke t sur veillance author ity of another Member Stat e, that measure shall be deemed justifi ed. This
shall be without prejudice to the procedural r ights of the concer ned operator in accordance with Ar ticle 18 of Regulation
(EU) 2019/1020. The three-month per iod refe r red to in this paragraph shall be reduced to 30 da ys in the event of
non-comp liance with the prohibition of the AI practices refe r red t o in Ar ticle 5 of this Regulation.
9. The marke t sur veillance author ities shall ensure that appropr iat e restr ictive measures are taken in respect of the
product or the AI syste m concer ned, such as withdra wal of the product or the AI syste m from their marke t, without undue
dela y .
Ar ticle 80
Procedure f or dealing with AI systems classif ied by the pro vider as non-high-r isk in application of Annex III
1. Where a mark et sur veillance author ity has sufficient reason to consider that an AI system classif ied by the provider as
non-high-r isk pursuant to Ar ticle 6(3) is indeed high-r isk, the mark et sur veillance author ity shall car r y out an evaluation of
the AI system concer ned in respect of its classific ation as a high-r isk AI syste m based on the conditions set out in Ar ticle
6(3) and the Commission guidelines.
OJ L, 12.7.2024
EN
ELI: http://data.europa.eu/eli/reg/2024/1689/oj 107/1442. Where, in the course of that evaluation, the market sur veillance author ity fi nds that the AI syste m concer ned is
high-r isk, it shall without undue dela y require the relevant pro vider to take all necessar y actions to br ing the AI syste m into
complia nce with the requirements and obligations laid do wn in this Regulation, as well as take appropr iate cor rective action
within a per iod the marke t sur veillance author ity ma y prescr ibe.
3. Where the marke t sur veillance author ity considers that the use of the AI system concer ned is not restr icte d to its
national terr itory , it shall inf or m the Commission and the other Member States without undue dela y of the results of the
evaluation and of the actions which it has required the pro vider to take .
4. The provid er shall ensure that all necessar y action is taken to br ing the AI system into compliance with the
requirements and obliga tions laid do wn in this Regulation. Where the provider of an AI system concer ned does not br ing
the AI syste m into compliance with those requirements and obligati ons within the per iod refe r red to in paragraph 2 of this
Ar ticle, the provid er shall be subject to fine s in accordance with Ar ticle 99.
5. The provid er shall ensure that all appropr iate cor rective action is take n in respect of all the AI systems concer ned that
it has made available on the Union marke t.
6. Where the pro vider of the AI system concer ned does not take adequate cor rective action within the per iod refe r red to
in paragraph 2 of this Ar ticle, Ar ticle 79(5) t o (9) shall apply .
7. Where, in the course of the evaluation pursuant to paragraph 1 of this Ar ticle, the mark et sur veillance author ity
establishes that the AI system was misclassified by the provider as non-high-r isk in order to circum vent the application of
requirements in Chapt er III, Section 2, the provid er shall be subject to fines in accordance with Ar ticle 99.
8. In ex ercising their po wer to monitor the application of this Ar ticle, and in accordance with Ar ticle 11 of Regulation
(EU) 2019/1020, marke t sur veillance author ities ma y per f or m appropr iate ch ecks, taking into account in par ticular
inf or mation stored in the EU database refe r red to in Ar ticle 71 of this Regulation.
Ar ticle 81
Union safeguard procedure
1. Where, within three months of receipt of the notif ication refe r red to in Ar ticle 79(5), or within 30 da ys in the case of
non-comp liance with the prohibition of the AI practices refer red to in Ar ticle 5, objections are raised b y the marke t
sur veillance author ity of a Member State t o a measure taken by another marke t sur veillance author ity , or where the
Commission considers the measure to be contrar y to Uni on law , the Commission shall without undue dela y ent er into
consultation with the marke t sur veillance author ity of the relevant Member Stat e and the operat or or operat ors, and shall
evaluate the national measure. On the basis of the results of that evaluation, the Commission shall, within six months, or
within 60 da ys in the case of non-comp liance with the prohibition of the AI practices refe r red to in Ar ticle 5, star ting from
the notification refe r red t o in Ar ticle 79(5), decide whether the national measure is justified and shall notify its decision to
the market sur veillance author ity of the Member State concer ned. The Commission shall also inf or m all other marke t
sur veillance author ities of its decision.
2. Where the Commission considers the measure taken by the relevant Member Stat e to be justified, all Member Stat es
shall ensure that they take appropr iat e restr ictive measures in respect of the AI syste m concer ned, such as requir ing the
withdra wal of the AI system from their mark et without undue dela y , and shall inf or m the Commission according ly . Where
the Commission considers the national measure to be unjustified, the Member Stat e concer ned shall withdra w the measure
and shall inf or m the Commission according ly .
3. Where the national measure is considered justifi ed and the non-comp liance of the AI syste m is attr ibuted to
shor tcomings in the har monised standards or common specific ations refer red to in Ar ticles 40 and 41 of this Regulation,
the Commission shall apply the procedure provided f or in Ar ticle 11 of Regulation (EU) No 1025/2012.
Ar ticle 82
Compliant AI systems which present a r isk
1. Where, having perfor med an evaluation under Ar ticle 79, af te r consulting the relevant national public author ity
referred to in Ar ticle 77(1), the mark et sur veillance author ity of a Member State finds that although a high-r isk AI system
complie s with this Regulation, it never theless presents a r isk to the health or saf ety of persons, to fundamental r ights, or t o
other aspects of public inte rest protection, it shall require the relevant operato r to take all appropr iate measures to ensure
that the AI system concer ned, when placed on the market or put into ser vice, no longer presents that r isk without undue
dela y , within a per iod it ma y prescr ibe.
EN
OJ L, 12.7.2024
108/144 ELI: http://data.europa.eu/eli/reg/2024/1689/oj2. The provider or other relevant operato r shall ensure that cor rective action is taken in respect of all the AI syste ms
concer ned that it has made available on the Uni on market within the timeline prescr ibed by the market sur veillance
author ity of the Member State refer red to in paragraph 1.
3. The Member Stat es shall immediately inf or m the Commission and the other Member States of a finding under
paragraph 1. That inf or mation shall include all a vailable details, in par ticular the data necessar y f or the identifica tion of the
AI syste m concer ned, the or igin and the supply ch ain of the AI system, the nature of the r isk in v olved and the nature and
duration of the national measures take n.
4. The Commission shall without undue dela y ent er into consultation with the Member Stat es concer ned and the
relevant operators, and shall evaluate the national measures taken. On the basis of the results of that evaluation, the
Commission shall decide whether the measure is justified and, where necessar y , propose other appropr iate measures.
5. The Commission shall immediately communicate its decision to the Member Stat es concer ned and to the relevant
operato rs. It shall also inf or m the other Member States.
Ar ticle 83
For mal non-compliance
1. Where the marke t sur veillance author ity of a Member State mak es one of the f ollo wing findings, it shall require the
relevant provid er to put an end to the non-compliance concer ned, within a per iod it ma y prescr ibe:
(a) the CE marking has been affixed in violation of Ar ticle 48;
(b) the CE marking has not been aff ix ed;
(c) the EU declaration of conf or mity referred to in Ar ticle 47 has not been dra wn up;
(d) the EU declaration of conf or mity referred to in Ar ticle 47 has not been dra wn up cor rectly ;
(e) the registration in the EU database refe r red t o in Ar ticle 71 has not been car r ied out ;
(f) where applicable, no author ised representative has been appointed;
(g) te chnical documentation is not available.
2. Where the non-compliance refe r red to in paragraph 1 persists, the marke t sur veillance author ity of the Member State
concer ned shall take appropr iate and propor tionate measures to restr ict or prohibit the high-r isk AI system being made
ava ilable on the marke t or to ensure that it is recalled or withdrawn from the mark et without dela y .
Ar ticle 84
Union AI testing suppor t s tr uctures
1. The Commission shall designate one or more Union AI testing suppor t str uctures to perfo r m the tasks listed under
Ar ticle 21(6) of Regulation (EU) 2019/1020 in the area of AI.
2. Without prejudice t o the tasks refer red to in paragraph 1, Uni on AI te sting suppor t str uctures shall also provide
independent tec hnical or scientific advice at the request of the Board, the Commission, or of market sur veillance author ities.
OJ L, 12.7.2024
EN
ELI: http://data.europa.eu/eli/reg/2024/1689/oj 109/144SECTION 4
Remedies
Ar ticle 85
Right t o lodge a complaint with a market sur v eillance author ity
Without prejudice t o other administrative or judicial remedies, any natural or legal person hav ing grounds to consider that
there has been an infr ing ement of the provisions of this Regulation ma y submit complaints t o the relevant marke t
sur veillance author ity .
In accordance with Regulation (EU) 2019/1020, suc h complaints shall be take n into account f or the pur pose of conducting
marke t sur veillance activities, and shall be handled in line with the dedicat ed procedures established theref or by the marke t
sur veillance author ities.
Ar ticle 86
Right t o explanation of individual decision-making
1. Any affect ed person subject to a decision which is take n by the deplo y er on the basis of the output from a high-r isk AI
syste m listed in Annex III, with the ex ception of syste ms listed under point 2 thereof, and which produces lega l eff ects or
similarly signifi cantly affe cts that person in a wa y that they consider t o hav e an adverse impact on their health, saf ety or
fundamental r ights shall hav e the r ight to obtain from the deplo y er clear and meaningful explanations of the role of the AI
syste m in the decision-making procedure and the main elements of the decision take n.
2. Paragraph 1 shall not apply to the use of AI syste ms f or which excep tions from, or restr ictions to, the obliga tion
under that paragraph f ollow from Union or national law in compliance with Union la w .
3. This Ar ticle shall apply only to the exte nt that the r ight refer red to in paragraph 1 is not other wise provided f or under
Uni on law .
Ar ticle 87
Repor ting of infr ingements and protection of repor ting persons
Directive (EU) 2019/1937 shall apply to the repor ting of infr ingem ents of this Regulation and the protection of persons
repor ting suc h infr ingem ents.
SECTION 5
Super vision, inv estig ation, enf or cement and monitor ing in r espect of providers of g ener al-pur pose AI models
Ar ticle 88
Enf orcement of the obligations of pro viders of general-pur pose AI models
1. The Commission shall hav e ex clusive po wers t o super vise and enf orce Chapt er V , taking into account the procedural
guarantees under Ar ticle 94. The Commission shall entr ust the imp lementation of these tasks to the AI Office, without
prejudice to the powers of org anisation of the Commission and the division of compet ences between Member Stat es and
the Uni on based on the T reaties.
2. Without prejudice to Ar ticle 75(3), marke t sur veillance author ities ma y request the Commission to ex ercise the
powe rs laid do wn in this Section, where that is necessar y and propor tionate to assist with the fulfilment of their tasks under
this Regulation.
EN
OJ L, 12.7.2024
110/144 ELI: http://data.europa.eu/eli/reg/2024/1689/ojAr ticle 89
Monitor ing actions
1. For the pur pose of car r ying out the tasks assigned to it under this Section, the AI Off ice ma y take the necessar y
actions t o monitor the effe ctive im plementation and comp liance with this Regulation by provid ers of general-pur pose AI
models, including their adherence to approve d codes of practice.
2. Downstream provid ers shall hav e the r ight t o lodg e a comp laint alleging an infr ingem ent of this Regulation.
A comp laint shall be duly reasoned and indicate at least:
(a) the point of contact of the provid er of the general-pur pose AI model concer ned;
(b) a descr iption of the relevant f acts, the provisions of this Regulation concer ned, and the reason why the downstream
provid er considers that the provid er of the general-purpo se AI model concer ned infr inged this Regulation;
(c) any other inf or mation that the do wnstream provider that sent the request considers relevant, including, where
appropr iate, inf or mation gathered on its ow n initiative.
Ar ticle 90
Aler ts of systemic r isks by the scientif ic panel
1. The scientifi c panel ma y provid e a qualified aler t to the AI Off ice where it has reason to suspect that:
(a) a g eneral-pur pose AI model poses concrete identifia ble r isk at Uni on level; or
(b) a g eneral-pur pose AI model meets the conditions refer red to in Ar ticle 51.
2. Upon suc h qualif ied aler t, the Commission, through the AI Offi ce and af te r hav ing inf or med the Board, ma y ex ercise
the powers laid down in this Section f or the pur pose of assessing the matt er . The AI Office shall inf or m the Board of any
measure according t o Ar ticles 91 t o 94.
3. A qualified aler t shall be duly reasoned and indicate at least:
(a) the point of contact of the provid er of the general-pur pose AI model with systemic r isk concer ned;
(b) a descr ipt ion of the relevant facts and the reasons f or the aler t by the scientifi c panel;
(c) any other inf or mation that the scientific panel considers to be relevant, including, where appropr iat e, inf or mation
g athered on its own initiative.
Ar ticle 91
Po w er to request document ation and inf or mation
1. The Commission ma y request the provider of the g eneral-pur pose AI model concer ned to provide the documentation
dra wn up by the provider in accordance with Ar ticles 53 and 55, or any additional inf or mation that is necessar y f or the
pur pose of assessing comp liance of the pro vider with this Regulation.
2. Bef ore sending the request f or inf or mation, the AI Offi ce ma y initiate a str uctured dialogue with the provid er of the
g eneral-pur pose AI model.
3. Upon a duly substantiated request from the scientific panel, the Commission ma y issue a request f or inf or mation to
a provider of a g eneral-pur pose AI model, where the access to inf or mation is necessar y and propor tionate f or the fulfilment
of the tasks of the scientific panel under Ar ticle 68(2).
OJ L, 12.7.2024
EN
ELI: http://data.europa.eu/eli/reg/2024/1689/oj 111/1444. The request f or inf or mation shall state the lega l basis and the pur pose of the request, specify what inf or mation is
required, set a per iod within which the inf or mation is to be provid ed, and indicate the fine s provided f or in Ar ticle 101 f or
supplying incor rect, incom plete or misleading inf or mation.
5. The provider of the g eneral-pur pose AI model concer ned, or its representative shall supply the inf or mation requested.
In the case of lega l persons, comp anies or fir ms, or where the provider has no lega l personality , the persons author ised to
represent them by la w or by their statutes, shall supply the inf or mation requested on behalf of the provider of the
g eneral-pur pose AI model concer ned. Lawy ers duly author ised to act ma y supply inf or mation on behalf of their clients. The
clients shall never theless remain fully responsible if the inf or mation supplied is incomplet e, incor rect or misleading.
Ar ticle 92
Po w er to conduct evaluations
1. The AI Office, af ter consulting the Board, ma y conduct evaluations of the g eneral-pur pose AI model concer ned:
(a) to assess comp liance of the provid er with obliga tions under this Regulation, where the inf or mation gathered pursuant
to Ar ticle 91 is insufficient ; or
(b) to investig ate syste mic r isk s at Uni on level of g eneral-pur pose AI models with syste mic r isk, in par ticular f ollowing
a qualified aler t from the scientific panel in accordance with Ar ticle 90(1), point (a).
2. The Commission ma y decide to appoint independent exper ts to car r y out evaluations on its behalf, including from the
scientific panel established pursuant to Ar ticle 68. Independent exper ts appointed f or this task shall meet the cr ite r ia
outlined in Ar ticle 68(2).
3. For the pur poses of paragraph 1, the Commission ma y request access to the g eneral-pur pose AI model concer ned
through APIs or fur ther appropr iat e te chnical means and to ols, including source code.
4. The request f or access shall state the lega l basis, the pur pose and reasons of the request and set the per iod within
which the access is to be provided, and the fines provided f or in Ar ticle 101 f or f ailure to provide access.
5. The pro viders of the general-purpo se AI model concer ned or its representative shall supply the inf or mation requested.
In the case of lega l persons, comp anies or fir ms, or where the provider has no lega l personality , the persons author ised to
represent them by law or b y their statute s, shall provide the access request ed on behalf of the pro vider of the
g eneral-pur pose AI model concer ned.
6. The Commission shall adopt imp lementing acts setting out the detailed ar rang ements and the conditions f or the
evaluations, including the detailed ar rang ements f or inv olving independent exper ts, and the procedure f or the selection
thereof. Those im plementing acts shall be adop ted in accordance with the examination procedure referred to in Ar ticle
98(2).
7. Pr ior to requesting access t o the g eneral-pur pose AI model concer ned, the AI Offi ce ma y initiate a str uctured dialogue
with the pro vider of the g eneral-pur pose AI model to gather more inf or mation on the inter nal t esting of the model, intern al
saf eguards f or preventing systemic r isks, and other inte r nal procedures and measures the provid er has taken to mitigat e
suc h r isks.
Ar ticle 93
Po w er to request measures
1. Where necessar y and appropr iat e, the Commission ma y request provid ers to :
(a) take appropr iate measures to comp ly with the obligations set out in Ar ticles 53 and 54;
EN
OJ L, 12.7.2024
112/144 ELI: http://data.europa.eu/eli/reg/2024/1689/oj(b) imp lement mitig ation measures, where the evaluation car r ied out in accordance with Ar ticle 92 has given r ise to ser ious
and substantiate d concer n of a syste mic r isk at Union level;
(c) restr ict the making available on the market, withdra w or recall the model.
2. Bef ore a measure is requested, the AI Office ma y initiate a str uctured dialogue with the provider of the
g eneral-pur pose AI model.
3. If, dur ing the str uctured dialogue refe r red t o in paragraph 2, the pro vider of the gene ral-pur pose AI model with
syste mic r isk offers commitments to implement mitigation measures to address a syste mic r isk at Union level, the
Commission ma y , b y decision, make those commitments binding and declare that there are no fur ther grounds f or action.
Ar ticle 94
Procedural r ights of economic operat ors of the general-pur pose AI model
Ar ticle 18 of Regulation (EU) 2019/1020 shall apply mutatis mutandis t o the providers of the general-pur pose AI model,
without prejudice to more specific procedural r ights provided f or in this Regulation.
C HAPTER X
CODES OF CONDUCT AND GUIDELINES
Ar ticle 95
Codes of conduct for v olunt ar y application of specif ic requirements
1. The AI Offi ce and the Member States shall encourage and facilitat e the drawing up of codes of conduct, including
related gover nance mechanisms, inte nded to f oster the v oluntar y application to AI systems, other than high-r isk AI systems,
of some or all of the requirements set out in Chap ter III, Section 2 taking into account the a vailable tec hnical solutions and
industr y best practices allowi ng f or the application of suc h requirements.
2. The AI Office and the Member States shall f acilitate the dra wing up of codes of conduct concer ning the v oluntar y
application, including by deplo y ers, of specific requirements t o all AI syste ms, on the basis of clear objectives and k ey
perfo r mance indicato rs to measure the achievement of those objectives, including elements suc h as, but not limited t o:
(a) applicable elements provided f or in Uni on ethical guidelines f or tr ustwo r th y AI;
(b) assessing and minimising the imp act of AI syste ms on envir onmental sustainability , including as regards energy-eff icient
programming and te chniques f or the efficient design, training and use of AI;
(c) promoting AI literac y , in par ticular that of persons dealing with the development, operation and use of AI;
(d) f acilitating an inclusive and diverse design of AI syste ms, including through the establishment of inclusive and diverse
development te ams and the promotion of stak eholders’ par ticipation in that process;
(e) assessing and preventing the nega tive imp act of AI syste ms on vulnerable persons or groups of vulnerable persons,
including as regards accessibility f or persons with a disability , as well as on g ender equality .
3. Codes of conduct ma y be drawn up by individual provid ers or deplo y ers of AI syste ms or b y organisations
representing them or b y both, including with the inv olvement of any intere sted stakeholders and their representative
org anisations, including civil society org anisations and academia. Codes of conduct ma y cover one or more AI syste ms
taking into account the similar ity of the intende d pur pose of the relevant systems.
4. The AI Office and the Member Stat es shall take into account the specific interests and needs of SMEs, including
star t-ups, when encouraging and f acilitating the dra wing up of codes of conduct.
OJ L, 12.7.2024
EN
ELI: http://data.europa.eu/eli/reg/2024/1689/oj 113/144Ar ticle 96
Guidelines from the Commission on the implementa tion of this Regulation
1. The Commission shall develop guidelines on the practical imp lementation of this Regulation, and in par ticular on:
(a) the application of the requirements and obliga tions refe r red to in Ar ticles 8 to 15 and in Ar ticle 25;
(b) the prohibite d practices refe r red to in Ar ticle 5;
(c) the practical implementation of the provisions relate d to substantial modification;
(d) the practical implementation of transparency obliga tions laid do wn in Ar ticle 50;
(e) detailed inf or mation on the relationship of this Regulation with the Union har monisation legislation listed in Annex I,
as well as with other relevant Uni on law , including as regards consistency in their enf orcement ;
(f) the application of the definition of an AI system as set out in Ar ticle 3, point (1).
When issuing suc h guidelines, the Commission shall pa y par ticular atte ntion to the needs of SMEs including star t-ups, of
local public author ities and of the sect ors most likely to be affe cted by this Regulation.
The guidelines refe r red to in the first subparagraph of this paragraph shall take due account of the g enerally ac knowledg ed
state of the ar t on AI, as well as of relevant har monised standards and common specif ications that are refer red to in
Ar ticles 40 and 41, or of those har monised standards or te chnical specific ations that are set out pursuant to Uni on
har monisation law .
2. At the request of the Member States or the AI Office, or on its own initiative, the Commission shall update guidelines
previously adop ted when deemed necessar y .
CHAPTER XI
DELEGA TION OF PO WER AND COMM ITTEE PR OCEDURE
Ar ticle 97
Exe rcise of the delegation
1. The po wer to adop t deleg ated acts is confe r red on the Commission subject to the conditions laid do wn in this Ar ticle.
2. The po wer to adop t deleg ated acts refe r red to in Ar ticle 6(6) and (7), Ar ticle 7(1) and (3), Ar ticle 11(3), Ar ticle 43(5)
and (6), Ar ticle 47(5), Ar ticle 51(3), Ar ticle 52(4) and Ar ticle 53(5) and (6) shall be conferr ed on the Commission f or
a per iod of five y ears from 1 August 2024. The Commission shall dra w up a repor t in respect of the delegation of power
not lat er than nine months bef ore the end of the five-year per iod. The deleg ation of power shall be tacitly extende d f or
per iods of an identical duration, unless the European Parliame nt or the Council opposes suc h extension not later than three
months bef ore the end of each per iod.
3. The delegation of power refe r red to in Ar ticle 6(6) and (7), Ar ticle 7(1) and (3), Ar ticle 11(3), Ar ticle 43(5) and (6),
Ar ticle 47(5), Ar ticle 51(3), Ar ticle 52(4) and Ar ticle 53(5) and (6) ma y be revok ed at an y time by the European Pa rliament
or b y the Council. A decision of revoc ation shall put an end to the deleg ation of power specif ied in that decision. It shall
take effect the da y f ollowing that of its publication in the Off icial Jour nal of the Eur opean Union or at a later date specified
therein. It shall not affe ct the validity of any delegat ed acts already in f orce.
4. Bef ore adopting a delegat ed act, the Commission shall consult exper ts designate d by each Member Stat e in accordance
with the pr inciples laid do wn in the Inte r institutional Agreement of 13 Apr il 2016 on Bett er Law-Making .
EN
OJ L, 12.7.2024
114/144 ELI: http://data.europa.eu/eli/reg/2024/1689/oj5. As soon as it adop ts a deleg ated act, the Commission shall notify it simultaneously t o the European Pa rliament and to
the Council.
6. Any delegat ed act adop te d pursuant to Ar ticle 6(6) or (7), Ar ticle 7(1) or (3), Ar ticle 11(3), Ar ticle 43(5) or (6),
Ar ticle 47(5), Ar ticle 51(3), Ar ticle 52(4) or Ar ticle 53(5) or (6) shall ent er into f orce only if no objection has been
expressed by either the European Parliame nt or the Council within a per iod of three months of notification of that act to
the European Pa rliament and the Council or if, bef ore the expir y of that per iod, the European Parliament and the Council
hav e both inf or med the Commission that they will not object. That per iod shall be extended by three months at the
initiative of the European Parliame nt or of the Council.
Ar ticle 98
Committee procedure
1. The Commission shall be assist ed by a committee. That committee shall be a committ ee within the meaning of
Regulation (EU) No 182/2011.
2. Where refe rence is made to this paragraph, Ar ticle 5 of Regulation (EU) No 182/2011 shall apply .
C HAPTER XII
PENAL TIES
Ar ticle 99
Pe nalties
1. In accordance with the te r ms and conditions laid down in this Regulation, Member Stat es shall la y do wn the r ules on
penalties and other enf orcement measures, which ma y also include war nings and non-monetar y measures, applicable to
infr ingements of this Regulation b y operat ors, and shall take all measures necessar y to ensure that the y are properly and
effe ctively implement ed, thereby taking into account the guidelines issued b y the Commission pursuant to Ar ticle 96. The
penalties provided f or shall be effe ctive, propor tionate and dissuasive. They shall tak e into account the interests of SMEs,
including star t-ups, and their economic viability .
2. The Member States shall, without dela y and at the lat est by the date of entr y into application, notify the Commission
of the r ules on penalties and of other enf orcement measures refe r red to in paragraph 1, and shall notify it, without dela y , of
any subsequent amendment to them.
3. Non-compliance with the prohibition of the AI practices refe r red to in Ar ticle 5 shall be subject t o administrative
fine s of up t o EUR 35 000 000 or , if the offe nder is an under taking, up t o 7 % of its to tal wo rldwide annual tur nover f or the
preceding fi nancial y ear , whichever is higher .
4. Non-compliance with any of the f ollowi ng provisions relate d to operato rs or notified bodies, other than those laid
do wn in Ar ticles 5, shall be subject to administrative fines of up to EUR 15 000 000 or , if the offe nder is an under taking, up
to 3 % of its total worldwide annual tur nover f or the preceding financ ial y ear , whichever is higher:
(a) obliga tions of provid ers pursuant to Ar ticle 16;
(b) obliga tions of author ised representatives pursuant t o Ar ticle 22;
(c) obliga tions of im por ters pursuant to Ar ticle 23;
(d) obliga tions of distr ibutors pursuant to Ar ticle 24;
(e) obliga tions of deplo yers pursuant to Ar ticle 26;
(f) requirements and obliga tions of notified bodies pursuant to Ar ticle 31, Ar ticle 33(1), (3) and (4) or Ar ticle 34;
(g) transparency obliga tions f or provid ers and deplo yers pursuant to Ar ticle 50.
OJ L, 12.7.2024
EN
ELI: http://data.europa.eu/eli/reg/2024/1689/oj 115/1445. The supply of incor rect, incom plete or misleading inf or mation to notif ied bodies or national compet ent author ities in
reply to a request shall be subject to administrative fi nes of up to EUR 7 500 000 or , if the offe nder is an under taking, up t o
1 % of its total worldwide annual tur nover f or the preceding fi nancial y ear , whichever is higher .
6. In the case of SMEs, including star t-ups, each fine referred to in this Ar ticle shall be up to the percentage s or amount
referred to in paragraphs 3, 4 and 5, whichever thereof is low er .
7. When deciding whether to im pose an administrative fine and when deciding on the amount of the administrative fi ne
in each individual case, all relevant circumstances of the specif ic situation shall be take n into account and, as appropr iate,
regard shall be given to the f ollowing:
(a) the nature, gra vity and duration of the infr ingem ent and of its consequences, taking into account the pur pose of the AI
syste m, as well as, where appropr iate, the number of affect ed persons and the level of damage suffer ed by them;
(b) whether administrative fines have already been applied by other market sur veillance author ities to the same operator f or
the same infr ingem ent ;
(c) whether administrative fine s hav e already been applied by other author ities to the same operato r f or infr ing ements of
other Uni on or national law , when suc h infr ing ements result from the same activity or omission constituting a relevant
infr ingement of this Regulation;
(d) the size, the annual tur nover and marke t share of the operat or committing the infr ing ement ;
(e) any other aggravating or mitiga ting fact or applicable t o the circumstances of the case, suc h as financ ial benefits gained,
or losses av oided, directly or indirectly , from the infr ingem ent ;
(f) the degree of cooperation with the national compet ent author ities, in order to remedy the infr ingem ent and mitigat e the
possible adverse effe cts of the infr ingement;
(g) the degree of responsibility of the operator taking into account the te chnical and org anisational measures imp lemented
by it ;
(h) the manner in which the infr ing ement became kno wn t o the national comp etent author ities, in par ticular whether , and
if so to what exte nt, the operato r notif ied the infr ingem ent ;
(i) the inte ntional or neg lig ent ch aracter of the infr ingement;
(j) any action taken by the operator t o mitigat e the har m suffer ed by the affe cted persons.
8. Each Member State shall la y do wn r ules on to what extent administrative fi nes ma y be imp osed on public author ities
and bodies established in that Member State.
9. Depending on the leg al syste m of the Member States, the r ules on administrative fine s ma y be applied in such
a manner that the fines are imp osed by compet ent national cour ts or by other bodies, as applicable in those Member Stat es.
The application of suc h r ules in those Member Stat es shall hav e an equivalent effect.
10. The ex ercise of powers under this Ar ticle shall be subject to appropr iate procedural saf eguards in accordance with
Uni on and national law , including eff ective judicial remedies and due process.
11. Member Stat es shall, on an annual basis, repor t to the Commission about the administrative fines the y have issued
dur ing that y ear , in accordance with this Ar ticle, and about any related litigation or judicial proceedings.
Ar ticle 100
A dminis trativ e f ines on Union instit utions, bodies, off ices and agencies
1. The European Data Protection Super visor ma y imp ose administrative fine s on Uni on institutions, bodies, offices and
age ncies f alling within the scope of this Regulation. When deciding whether to im pose an administrative fine and when
deciding on the amount of the administrative fine in each individual case, all relevant circumstances of the specific situation
shall be take n into account and due rega rd shall be given to the f ollowi ng:
EN
OJ L, 12.7.2024
116/144 ELI: http://data.europa.eu/eli/reg/2024/1689/oj(a) the nature, gravity and duration of the infr ingem ent and of its consequences, taking into account the pur pose of the AI
syste m concer ned, as well as, where appropr iate, the number of affe cted persons and the level of damage suff ered by
them;
(b) the degree of responsibility of the Uni on institution, body , off ice or ag ency , taking into account te ch nical and
org anisational measures imp lemented by them;
(c) any action take n b y the Uni on institution, body , office or age ncy to mitigat e the damage suffered by affe cted persons;
(d) the degree of cooperation with the European Data Prot ection Super visor in order to remedy the infr ing ement and
mitiga te the possible adverse effe cts of the infr ing ement, including comp liance with any of the measures previously
ordered b y the European Data Prot ection Super visor against the Union institution, body , off ice or age ncy concer ned
with regard t o the same subject matt er ;
(e) any similar previous infr ingements by the Uni on institution, body , off ice or age ncy ;
(f) the manner in which the infr ingem ent became kno wn to the European Data Prot ection Super visor , in par ticular
whether , and if so to what extent, the Uni on institution, body , off ice or age ncy notif ied the infr ingem ent ;
(g) the annual budg et of the Uni on institution, body , off ice or age ncy .
2. Non-compliance with the prohibition of the AI practices refe r red to in Ar ticle 5 shall be subject t o administrative
fine s of up to EUR 1 500 000.
3. The non-com pliance of the AI syste m with any requirements or obligati ons under this Regulation, other than those
laid down in Ar ticle 5, shall be subject to administrative fine s of up to EUR 750 000.
4. Bef ore taking decisions pursuant to this Ar ticle, the European Data Protect ion Super visor shall give the Uni on
institution, body , off ice or agency which is the subject of the proceedings conducted b y the European Data Protection
Super visor the oppor tunity of being heard on the matt er regard ing the possible infr ingem ent. The European Data
Protection Super visor shall base his or her decisions only on elements and circumstances on which the par ties concer ned
hav e been able to comment. Comp lainants, if an y , shall be associated closely with the proceedings.
5. The r ights of defence of the par ties concer ned shall be fully respected in the proceedings. They shall be entitled to
hav e access to the European Data Protection Super visor ’s fi le, subject to the legitimat e interest of individuals or
under takings in the protect ion of their personal data or business secrets.
6. Funds collected by imp osition of fine s in this Ar ticle shall contr ibut e to the g eneral budg et of the Uni on. The fine s
shall not affect the effe ctive operation of the Uni on institution, body , off ice or age ncy fine d.
7. The European Data Prot ection Super visor shall, on an annual basis, notify the Commission of the administrative fine s
it has imp osed pursuant t o this Ar ticle and of any litig ation or judicial proceedings it has initiated.
Ar ticle 101
Fin es for pro viders of general-pur pose AI models
1. The Commission ma y impose on provid ers of general-pur pose AI models fines not ex ceeding 3 % of their annual total
wo rldwide tur nove r in the preceding financial y ear or EUR 15 000 000, whichever is higher ., when the Commission finds
that the provider intent ionally or neg ligently:
(a) infr inged the relevant provisions of this Regulation;
(b) f ailed to comply with a request f or a document or f or inf or mation pursuant to Ar ticle 91, or supplied incor rect,
incom plete or misleading inf or mation;
(c) f ailed to comp ly with a measure request ed under Ar ticle 93;
OJ L, 12.7.2024
EN
ELI: http://data.europa.eu/eli/reg/2024/1689/oj 117/144(d) f ailed to make available to the Commission access to the g eneral-pur pose AI model or g eneral-pur pose AI model with
syste mic r isk with a view to conducting an evaluation pursuant to Ar ticle 92.
In fixing the amount of the fine or per iodic penalty pa yment, regard shall be had to the nature, gra vity and duration of the
infr ingement, taking due account of the pr inciples of propor tionality and appropr iat eness. The Commission shall also into
account commitments made in accordance with Ar ticle 93(3) or made in relevant codes of practice in accordance with
Ar ticle 56.
2. Bef ore adop ting the decision pursuant to paragraph 1, the Commission shall communicate its preliminar y findings to
the provid er of the gene ral-pur pose AI model and give it an oppor tunity to be heard.
3. Fines im posed in accordance with this Ar ticle shall be effe ctive, propor tionate and dissuasive.
4. Inf or mation on fi nes imp osed under this Ar ticle shall also be communicated to the Board as appropr iate.
5. The Cour t of Justice of the European Uni on shall hav e unlimit ed jur isdiction to review decisions of the Commission
fixing a fi ne under this Ar ticle. It ma y cancel, reduce or increase the fi ne imposed.
6. The Commission shall adop t imp lementing acts containing detailed ar rang ements and procedural saf eguards f or
proceedings in view of the possible adop tion of decisions pursuant to paragraph 1 of this Ar ticle. Those imp lementing acts
shall be adop ted in accordance with the examination procedure refer red to in Ar ticle 98(2).
C HAPTER XIII
FINAL PR O VISIONS
Ar ticle 102
Amendment to Regulation (EC) No 300/2008
In Ar ticle 4(3) of Regulation (EC) No 300/2008, the f ollowing subparagraph is added:
‘ When adopti ng detailed measures related to te chnical specif ications and procedures f or approva l and use of secur ity
equipment concer ning Ar tificial Intellig ence syste ms within the meaning of Regulation (EU) 2024/1689 of the European
Parliame nt and of the Council (*), the requirements set out in Chap ter III, Section 2, of that Regulation shall be taken into
account.
(*) Regulation (EU) 2024/1689 of the European Parliament and of the Council of 13 June 2024 la ying down har monised
r ules on ar tif icial intellig ence and amending Regulations (EC) No 300/2008, (EU) No 167/2013, (EU) No 168/2013,
(EU) 2018/858, (EU) 2018/1139 and (EU) 2019/2144 and Directives 2014/90/EU, (EU) 2016/797 and (EU)
2020/1828 (Ar tificial Inte lligence A ct) (OJ L, 2024/1689, 12.7.2024, ELI: http://data.europa.eu/eli/reg/
2024/1689/oj).’ .
Ar ticle 103
Amendment to Regulation (EU) No 167/2013
In Ar ticle 17(5) of Regulation (EU) No 167/2013, the f ollo wing subparagraph is added:
‘ When adopting delegat ed acts pursuant to the first subparagraph concer ning ar tif icial intellig ence syste ms which are safety
compo nents within the meaning of Regulation (EU) 2024/1689 of the European Pa rliament and of the Council (*), the
requirements set out in Chap ter III, Section 2, of that Regulation shall be taken into account.
(*) Regulation (EU) 2024/1689 of the European Parliament and of the Council of 13 June 2024 la ying down har monised
r ules on ar tif icial intellig ence and amending Regulations (EC) No 300/2008, (EU) No 167/2013, (EU) No 168/2013,
(EU) 2018/858, (EU) 2018/1139 and (EU) 2019/2144 and Directives 2014/90/EU, (EU) 2016/797 and (EU)
2020/1828 (Ar tificial Inte lligence A ct) (OJ L, 2024/1689, 12.7.2024, ELI: http://data.europa.eu/eli/reg/
2024/1689/oj).’ .
EN
OJ L, 12.7.2024
118/144 ELI: http://data.europa.eu/eli/reg/2024/1689/ojAr ticle 104
Amendment to Regulation (EU) No 168/2013
In Ar ticle 22(5) of Regulation (EU) No 168/2013, the f ollo wing subparagraph is added:
‘ When adopting deleg ated acts pursuant to the first subparagraph concer ning Ar tif icial Intell igence syste ms which are saf ety
compo nents within the meaning of Regulation (EU) 2024/1689 of the European Pa rliament and of the Council (*), the
requirements set out in Chap ter III, Section 2, of that Regulation shall be taken into account.
(*) Regulation (EU) 2024/1689 of the European Parliament and of the Council of 13 June 2024 la ying down har monised
r ules on ar tif icial intellig ence and amending Regulations (EC) No 300/2008, (EU) No 167/2013, (EU) No 168/2013,
(EU) 2018/858, (EU) 2018/1139 and (EU) 2019/2144 and Directives 2014/90/EU, (EU) 2016/797 and (EU)
2020/1828 (Ar tificial Inte lligence A ct) (OJ L, 2024/1689, 12.7.2024, ELI: http://data.europa.eu/eli/reg/
2024/1689/oj).’ .
Ar ticle 105
Amendment to Directiv e 2014/90/EU
In Ar ticle 8 of Directive 2014/90/EU, the f ollowing paragraph is added:
‘5. For Ar tif icial Inte lligence syste ms which are safety comp onents within the meaning of Regulation (EU) 2024/1689 of
the European Pa rliament and of the Council (*), when car r ying out its activities pursuant to paragraph 1 and when adop ting
te chnical specif ications and t esting standards in accordance with paragraphs 2 and 3, the Commission shall take into
account the requirements set out in Chap ter III, Section 2, of that Regulation.
(*) Regulation (EU) 2024/1689 of the European Parliament and of the Council of 13 June 2024 la ying down har monised
r ules on ar tif icial intellig ence and amending Regulations (EC) No 300/2008, (EU) No 167/2013, (EU) No 168/2013,
(EU) 2018/858, (EU) 2018/1139 and (EU) 2019/2144 and Directives 2014/90/EU, (EU) 2016/797 and (EU)
2020/1828 (Ar tificial Inte lligence A ct) (OJ L, 2024/1689, 12.7.2024, ELI: http://data.europa.eu/eli/reg/
2024/1689/oj).’ .
Ar ticle 106
Amendment to Directiv e (EU) 2016/797
In Ar ticle 5 of Directive (EU) 2016/797, the f ollo wing paragraph is added:
‘12. When adop ting delegat ed acts pursuant to paragraph 1 and im plementing acts pursuant to paragraph 11
concer ning Ar tificial Intellig ence systems which are safety components within the meaning of Regulation (EU) 2024/1689
of the European Pa rliament and of the Council (*), the requirements set out in Chapt er III, Section 2, of that Regulation shall
be take n into account.
(*) Regulation (EU) 2024/1689 of the European Parliament and of the Council of 13 June 2024 la ying down har monised
r ules on ar tif icial intellig ence and amending Regulations (EC) No 300/2008, (EU) No 167/2013, (EU) No 168/2013,
(EU) 2018/858, (EU) 2018/1139 and (EU) 2019/2144 and Directives 2014/90/EU, (EU) 2016/797 and (EU)
2020/1828 (Ar tificial Inte lligence A ct) (OJ L, 2024/1689, 12.7.2024, ELI: http://data.europa.eu/eli/reg/
2024/1689/oj).’ .
OJ L, 12.7.2024
EN
ELI: http://data.europa.eu/eli/reg/2024/1689/oj 119/144Ar ticle 107
Amendment to Regulation (EU) 2018/858
In Ar ticle 5 of Regulation (EU) 2018/858 the f ollowi ng paragraph is added:
‘4. When adop ting deleg ated acts pursuant to paragraph 3 concer ning Ar tif icial Intellig ence syste ms which are safety
compo nents within the meaning of Regulation (EU) 2024/1689 of the European Pa rliament and of the Council (*), the
requirements set out in Chap ter III, Section 2, of that Regulation shall be taken into account.
(*) Regulation (EU) 2024/1689 of the European Parliament and of the Council of 13 June 2024 la ying down har monised
r ules on ar tif icial intellig ence and amending Regulations (EC) No 300/2008, (EU) No 167/2013, (EU) No 168/2013,
(EU) 2018/858, (EU) 2018/1139 and (EU) 2019/2144 and Directives 2014/90/EU, (EU) 2016/797 and (EU)
2020/1828 (Ar tificial Inte lligence A ct) (OJ L, 2024/1689, 12.7.2024, ELI: http://data.europa.eu/eli/reg/
2024/1689/oj).’ .
Ar ticle 108
Amendments t o Regulation (EU) 2018/1139
Regulation (EU) 2018/1139 is amended as f ollo ws:
(1) in Ar ticle 17, the f ollowing paragraph is added:
‘3. Without prejudice to paragraph 2, when adop ting im plementing acts pursuant to paragraph 1 concer ning
Ar tif icial Intellig ence systems which are safety comp onents within the meaning of Regulation (EU) 2024/1689 of the
European Parliame nt and of the Council (*), the requirements set out in Chapt er III, Section 2, of that Regulation shall be
tak en into account.
(*) Regulation (EU) 2024/1689 of the European Parliament and of the Council of 13 June 2024 la ying do wn
har monised r ules on ar tif icial intellig ence and amending Regulations (EC) No 300/2008, (EU) No 167/2013, (EU)
No 168/2013, (EU) 2018/858, (EU) 2018/1139 and (EU) 2019/2144 and Directives 2014/90/EU, (EU) 2016/797
and (EU) 2020/1828 (Ar tif icial Intellig ence A ct) (OJ L, 2024/1689, 12.7.2024, ELI: http://data.europa.
eu/eli/reg/2024/1689/oj).’;
(2) in Ar ticle 19, the f ollowing paragraph is added:
‘4. When adop ting deleg ated acts pursuant to paragraphs 1 and 2 concer ning Ar tif icial Intel ligence syste ms which
are saf ety comp onents within the meaning of Regulation (EU) 2024/1689, the requirements set out in Chapt er III,
Section 2, of that Regulation shall be taken into account.’;
(3) in Ar ticle 43, the f ollowing paragraph is added:
‘4. When adop ting im plementing acts pursuant to paragraph 1 concer ning Ar tif icial Inte lligence systems which are
saf ety compo nents within the meaning of Regulation (EU) 2024/1689, the requirements set out in Chapt er III,
Section 2, of that Regulation shall be taken into account.’;
(4) in Ar ticle 47, the f ollowing paragraph is added:
‘3. When adop ting deleg ated acts pursuant to paragraphs 1 and 2 concer ning Ar tif icial Intel ligence syste ms which
are saf ety comp onents within the meaning of Regulation (EU) 2024/1689, the requirements set out in Chapt er III,
Section 2, of that Regulation shall be taken into account.’;
(5) in Ar ticle 57, the f ollowing subparagraph is added:
‘ When adop ting those imp lementing acts concer ning Ar tif icial Inte lligence syste ms which are saf ety comp onents within
the meaning of Regulation (EU) 2024/1689, the requirements set out in Chapt er III, Section 2, of that Regulation shall
be take n into account.’;
EN
OJ L, 12.7.2024
120/144 ELI: http://data.europa.eu/eli/reg/2024/1689/oj(6) in Ar ticle 58, the f ollowing paragraph is added:
‘3. When adop ting deleg ated acts pursuant to paragraphs 1 and 2 concer ning Ar tif icial Intel ligence syste ms which
are saf ety comp onents within the meaning of Regulation (EU) 2024/1689, the requirements set out in Chapt er III,
Section 2, of that Regulation shall be taken into account.’ .
Ar ticle 109
Amendment to Regulation (EU) 2019/2144
In Ar ticle 11 of Regulation (EU) 2019/2144, the f ollowing paragraph is added:
‘3. When adop ting the imp lementing acts pursuant to paragraph 2, concer ning ar tif icial intellig ence systems which are
saf ety comp onents within the meaning of Regulation (EU) 2024/1689 of the European Parliame nt and of the Council (*),
the requirements set out in Chapt er III, Section 2, of that Regulation shall be take n into account.
(*) Regulation (EU) 2024/1689 of the European Parliament and of the Council of 13 June 2024 la ying down har monised
r ules on ar tif icial intellig ence and amending Regulations (EC) No 300/2008, (EU) No 167/2013, (EU) No 168/2013,
(EU) 2018/858, (EU) 2018/1139 and (EU) 2019/2144 and Directives 2014/90/EU, (EU) 2016/797 and (EU)
2020/1828 (Ar tificial Inte lligence A ct) (OJ L, 2024/1689, 12.7.2024, ELI: http://data.europa.eu/eli/reg/
2024/1689/oj).’ .
Ar ticle 110
Amendment to Directiv e (EU) 2020/1828
In Annex I to Directive (EU) 2020/1828 of the European Parliament and of the Council (
58
), the f ollowing point is added:
‘(68) Regulation (EU) 2024/1689 of the European Parliament and of the Council of 13 June 2024 la ying down har monised
r ules on ar tificial inte lligence and amending Regulations (EC) No 300/2008, (EU) No 167/2013, (EU) No 168/2013,
(EU) 2018/858, (EU) 2018/1139 and (EU) 2019/2144 and Directives 2014/90/EU, (EU) 2016/797 and (EU)
2020/1828 (Ar tif icial Inte lligence A ct) (OJ L, 2024/1689, 12.7.2024, ELI: http://data.europa.eu/eli/reg/
2024/1689/oj).’.
Ar ticle 111
AI systems already placed on the mark et or put into ser vice and general-pur pose AI models already placed on the
marked
1. Without prejudice to the application of Ar ticle 5 as referred to in Ar ticle 113(3), point (a), AI syste ms which are
compo nents of the larg e-scale IT systems established b y the leg al acts listed in Annex X that hav e been placed on the marke t
or put into ser vice bef ore 2 Augu st 2027 shall be brought into compliance with this Regulation b y 31 December 2030.
The requirements laid do wn in this Regulation shall be taken into account in the evaluation of each larg e-scale IT system
established by the leg al acts listed in Annex X t o be under take n as provid ed f or in those lega l acts and where those lega l acts
are replaced or amended.
2. Without prejudice to the application of Ar ticle 5 as refer red t o in Ar ticle 113(3), point (a), this Regulation shall apply
to operators of high-r isk AI systems, other than the syste ms refe r red to in paragraph 1 of this Ar ticle, that hav e been placed
on the mark et or put into ser vice bef ore 2 Au gust 2026, only if, as from that date, those syste ms are subject to significant
ch anges in their designs. In any case, the providers and deplo y ers of high-r isk AI systems intended to be used by public
author ities shall take the necessar y steps to comply with the requirements and obliga tions of this Regulation by 2 August
2030.
3. Providers of general-purpo se AI models that have been placed on the marke t bef ore 2 August 2025 shall take the
necessar y st eps in order to comp ly with the obliga tions laid do wn in this Regulation by 2 Augu st 2027.
OJ L, 12.7.2024
EN
ELI: http://data.europa.eu/eli/reg/2024/1689/oj 121/144
(
58
) Directive (EU) 2020/1828 of the European Parliament and of the Council of 25 No vember 2020 on representative actions f or the
prot ection of the collective interests of consumers and repealing Directive 2009/22/EC (OJ L 409, 4.12.2020, p. 1).Ar ticle 112
Evaluation and review
1. The Commission shall assess the need f or amendment of the list set out in Annex III and of the list of prohibite d AI
practices laid do wn in Ar ticle 5, once a year f ollowing the entr y into f orce of this Regulation, and until the end of the per iod
of the deleg ation of power laid down in Ar ticle 97. The Commission shall submit the findi ngs of that assessment t o the
European Parliament and the Council.
2. By 2 August 2028 and ever y f our y ears thereaf ter , the Commission shall evaluate and repor t t o the European
Parliame nt and to the Council on the f ollowing:
(a) the need f or amendments extending existing area headings or adding new area headings in Annex III;
(b) amendments to the list of AI systems requir ing additional transparency measures in Ar ticle 50;
(c) amendments enhancing the effe ctiveness of the super vision and gove r nance system.
3. By 2 Au gust 2029 and ever y f our years thereaf ter , the Commission shall submit a repor t on the evaluation and review
of this Regulation to the European Parliament and t o the Council. The repor t shall include an assessment with regard to the
str ucture of enf orcement and the possible need f or a Union age ncy to resolve any identified shor tcomings. On the basis of
the find ings, that repor t shall, where appropr iate, be accom panied by a proposal f or amendment of this Regulation. The
repor ts shall be made public.
4. The repor ts refer red to in paragraph 2 shall pa y specif ic attention t o the f ollowing:
(a) the status of the fi nancial, tec hnical and human resources of the national comp et ent author ities in order to eff ectively
perfo r m the tasks assigned to them under this Regulation;
(b) the stat e of penalties, in par ticular administrative fine s as refer red to in Ar ticle 99(1), applied by Member Stat es f or
infr ingements of this Regulation;
(c) adop ted har monised standards and common specifications developed to suppor t this Regulation;
(d) the number of under takings that ent er the market af te r the entr y into application of this Regulation, and how many of
them are SMEs.
5. By 2 August 2028, the Commission shall evaluate the functioning of the AI Offi ce, whether the AI Office has been
given sufficient powers and comp etences to fulf il its tasks, and whether it would be relevant and needed f or the proper
imp lementation and enf orcement of this Regulation to upgrade the AI Offi ce and its enf orcement compet ences and to
increase its resources. The Commission shall submit a repor t on its evaluation to the European Parliame nt and to the
Council.
6. By 2 Augu st 2028 and ever y f our y ears thereaf te r , the Commission shall submit a repor t on the review of the progress
on the development of standardisation deliverables on the energy-eff icient development of g eneral-pur pose AI models, and
asses the need f or fur ther measures or actions, including binding measures or actions. The repor t shall be submitted to the
European Parliament and t o the Council, and it shall be made public.
7. By 2 Au gust 2028 and ever y three y ears thereaf ter , the Commission shall evaluate the imp act and effe ctiveness of
v oluntar y codes of conduct to f oste r the application of the requirements set out in Chapt er III, Section 2 f or AI syste ms
other than high-r isk AI syste ms and possibly other additional requirements f or AI syste ms other than high-r isk AI systems,
including as regards environmental sustainability .
8. For the pur poses of paragraphs 1 to 7, the Board, the Member Stat es and national compet ent author ities shall provide
the Commission with inf or mation upon its request and without undue dela y .
9. In car r ying out the evaluations and reviews refer red to in paragraphs 1 to 7, the Commission shall take into account
the positions and find ings of the Board, of the European Parliame nt, of the Council, and of other relevant bodies or sources.
EN
OJ L, 12.7.2024
122/144 ELI: http://data.europa.eu/eli/reg/2024/1689/oj10. The Commission shall, if necessar y , submit appropr iat e proposals to amend this Regulation, in par ticular taking into
account developments in t echnology , the effect of AI syste ms on health and safety , and on fundamental r ights, and in light
of the state of progress in the inf or mation society .
11. T o guide the evaluations and reviews referred t o in paragraphs 1 to 7 of this Ar ticle, the AI Office shall under take to
develop an objective and par ticipative methodology f or the evaluation of r isk levels based on the cr ite r ia outlined in the
relevant Ar ticles and the inclusion of new syste ms in:
(a) the list set out in Annex III, including the extension of existing area headings or the addition of new area headings in
that Annex;
(b) the list of prohibited practices set out in Ar ticle 5; and
(c) the list of AI syste ms requir ing additional transparency measures pursuant to Ar ticle 50.
12. Any amendment to this Regulation pursuant to paragraph 10, or relevant deleg ated or imp lementing acts, which
concer ns sectoral Uni on har monisation legislation listed in Section B of Annex I shall take into account the regulator y
specific ities of each secto r , and the existing gover nance, conf or mity assessment and enf orcement mechanisms and
author ities established therein.
13. By 2 Au gust 2031, the Commission shall car r y out an assessment of the enf orcement of this Regulation and shall
repor t on it t o the European Pa rliament, the Council and the European Economic and Social Committee, taking into
account the first y ears of application of this Regulation. On the basis of the fi ndings, that repor t shall, where appropr iate, be
accom panied by a proposal f or amendment of this Regulation with regard to the str ucture of enf orcement and the need f or
a Uni on age ncy to resolve any identifie d shor tcomings.
Ar ticle 113
Entr y into force and application
This Regulation shall ent er into f orce on the twentieth da y f ollowing that of its publication in the Of ficial Jour nal of the
Europe an Union.
It shall apply from 2 August 2026.
How ever:
(a) Chapt ers I and II shall apply from 2 Febr uar y 2025;
(b) Chapt er III Section 4, Chap ter V , Chapt er VII and Chap ter XII and Ar ticle 78 shall apply from 2 Augu st 2025, with the
ex ception of Ar ticle 101;
(c) Ar ticle 6(1) and the cor responding obligations in this Regulation shall apply from 2 August 2027.
This Regulation shall be binding in its entirety and directly applicable in all Member Stat es.
Done at Br ussels, 13 June 2024.
For the Eur opean P arliament
The Pr esident
R. METSOL A
For the Council
The Pr esident
M. MICHEL
OJ L, 12.7.2024
EN
ELI: http://data.europa.eu/eli/reg/2024/1689/oj 123/144ANNEX I
Lis t of Union har monisation legislation
Section A . List of Uni on har monisation legislation based on the New Legislative Framewo rk
1. Directive 2006/42/EC of the European Parliament and of the Council of 17 Ma y 2006 on machi ner y , and amending
Directive 95/16/EC (OJ L 157, 9.6.2006, p. 24);
2. Directive 2009/48/EC of the European Pa rliament and of the Council of 18 June 2009 on the safety of to ys (OJ
L 170, 30.6.2009, p. 1);
3. Directive 2013/53/EU of the European Parliame nt and of the Council of 20 November 2013 on recreational craf t
and personal wat ercraf t and repealing Directive 94/25/EC (OJ L 354, 28.12.2013, p. 90);
4. Directive 2014/33/EU of the European Parliament and of the Council of 26 Febr uar y 2014 on the har monisation of
the la ws of the Member States relating t o lif ts and saf ety comp onents f or lif ts (OJ L 96, 29.3.2014, p. 251);
5. Directive 2014/34/EU of the European Parliament and of the Council of 26 Febr uar y 2014 on the har monisation of
the la ws of the Member States relating to equipment and protect ive systems intende d f or use in pote ntially explosive
atmospheres (OJ L 96, 29.3.2014, p. 309);
6. Directive 2014/53/EU of the European Parliament and of the Council of 16 Apr il 2014 on the har monisation of the
law s of the Member States relating to the making ava ilable on the marke t of radio equipment and repealing Directive
1999/5/EC (OJ L 153, 22.5.2014, p. 62);
7. Directive 2014/68/EU of the European Pa rliament and of the Council of 15 Ma y 2014 on the har monisation of the
law s of the Member States relating to the making available on the marke t of pressure equipment (OJ L 189,
27.6.2014, p. 164);
8. Regulation (EU) 2016/424 of the European Parliament and of the Council of 9 Marc h 2016 on cablewa y
installations and repealing Directive 2000/9/EC (OJ L 81, 31.3.2016, p. 1);
9. Regulation (EU) 2016/425 of the European Pa rliament and of the Council of 9 March 2016 on personal prot ective
equipment and repealing Council Directive 89/686/EEC (OJ L 81, 31.3.2016, p. 51);
10. Regulation (EU) 2016/426 of the European Parliament and of the Council of 9 March 2016 on appliances bur ning
g aseous fuels and repealing Directive 2009/142/EC (OJ L 81, 31.3.2016, p. 99);
11. Regulation (EU) 2017/745 of the European Pa rliament and of the Council of 5 Apr il 2017 on medical devices,
amending Directive 2001/83/EC, Regulation (EC) No 178/2002 and Regulation (EC) No 1223/2009 and repealing
Council Directives 90/385/EEC and 93/42/EEC (OJ L 117, 5.5.2017, p. 1);
12. Regulation (EU) 2017/746 of the European Parliament and of the Council of 5 Apr il 2017 on in vitro diagnostic
medical devices and repealing Directive 98/79/EC and Commission Decision 2010/227/EU (OJ L 117, 5.5.2017,
p. 176).
Section B. List of other Uni on har monisation legislation
13. Regulation (EC) No 300/2008 of the European Parliame nt and of the Council of 11 March 2008 on common r ules
in the fie ld of civil a viation secur ity and repealing Regulation (EC) No 2320/2002 (OJ L 97, 9.4.2008, p. 72);
14. Regulation (EU) No 168/2013 of the European Pa rliament and of the Council of 15 Januar y 2013 on the approval
and marke t sur veillance of two- or three-wheel vehicles and quadr icy cles (OJ L 60, 2.3.2013, p. 52);
15. Regulation (EU) No 167/2013 of the European Parliament and of the Council of 5 Febr uar y 2013 on the approval
and marke t sur veillance of agr icultural and f orestr y vehicles (OJ L 60, 2.3.2013, p. 1);
EN
OJ L, 12.7.2024
124/144 ELI: http://data.europa.eu/eli/reg/2024/1689/oj16. Directive 2014/90/EU of the European Parliament and of the Council of 23 July 2014 on mar ine equipment and
repealing Council Directive 96/98/EC (OJ L 257, 28.8.2014, p. 146);
17. Directive (EU) 2016/797 of the European Parliament and of the Council of 11 Ma y 2016 on the interop erability of
the rail syste m within the European Union (OJ L 138, 26.5.2016, p. 44);
18. Regulation (EU) 2018/858 of the European Parliament and of the Council of 30 Ma y 2018 on the approval and
marke t sur veillance of motor vehicles and their trailers, and of systems, compo nents and separate tec hnical units
intende d f or such vehicles, amending Regulations (EC) No 715/2007 and (EC) No 595/2009 and repealing Directive
2007/46/EC (OJ L 151, 14.6.2018, p. 1);
19. Regulation (EU) 2019/2144 of the European Parliament and of the Council of 27 No vember 2019 on type-appro val
requirements f or motor vehicles and their trailers, and syste ms, compo nents and separate tec hnical units intended
f or suc h vehicles, as rega rds their g eneral safety and the prot ection of vehicle occupants and vulnerable road users,
amending Regulation (EU) 2018/858 of the European Parliament and of the Council and repealing Regulations (EC)
No 78/2009, (EC) No 79/2009 and (EC) No 661/2009 of the European Parliament and of the Council and
Commission Regulations (EC) No 631/2009, (EU) No 406/2010, (EU) No 672/2010, (EU) No 1003/2010,
(EU) No 1005/2010, (EU) No 1008/2010, (EU) No 1009/2010, (EU) No 19/2011, (EU) No 109/2011, (EU)
No 458/2011, (EU) No 65/2012, (EU) No 130/2012, (EU) No 347/2012, (EU) No 351/2012, (EU) No 1230/2012
and (EU) 2015/166 (OJ L 325, 16.12.2019, p. 1);
20. Regulation (EU) 2018/1139 of the European Parliament and of the Council of 4 July 2018 on common r ules in the
fie ld of civil aviat ion and establishing a European Uni on A viation Saf ety Ag ency , and amending Regulations (EC)
No 2111/2005, (EC) No 1008/2008, (EU) No 996/2010, (EU) No 376/2014 and Directives 2014/30/EU and
2014/53/EU of the European Pa rliament and of the Council, and repealing Regulations (EC) No 552/2004 and (EC)
No 216/2008 of the European Pa rliament and of the Council and Council Regulation (EEC) No 3922/91 (OJ L 212,
22.8.2018, p. 1), in so f ar as the design, production and placing on the market of aircraf ts refe r red to in Ar ticle 2(1),
points (a) and (b) thereof, where it concer ns unmanned aircraf t and their engines, propellers, par ts and equipment to
control them remotely , are concer ned.
OJ L, 12.7.2024
EN
ELI: http://data.europa.eu/eli/reg/2024/1689/oj 125/144ANNEX II
Lis t of cr iminal offences referred to in Ar ticle 5(1), f irst subparag raph, point (h)(iii)
Cr iminal offences refe r red t o in Ar ticle 5(1), fir st subparagraph, point (h)(iii):
— t er ror ism,
— trafficking in human beings,
— sexual exploitation of children, and child por nography ,
— illicit traffi ck ing in narcotic dr ugs or psy ch otropic substances,
— illicit traffi ck ing in weapons, munitions or explosives,
— murder , gr ievous bodily injur y ,
— illicit trade in human org ans or tissue,
— illicit traffi ck ing in nuclear or radioactive materi als,
— kidnapping, illegal restraint or hostage -taking,
— cr imes within the jur isdiction of the Inte r national Cr iminal Cour t,
— unla wful seizure of aircraf t or ships,
— rape,
— en vironmental cr ime,
— org anised or ar med robber y ,
— sabotag e,
— par ticipation in a cr iminal org anisation inv olved in one or more of the offences listed above .
EN
OJ L, 12.7.2024
126/144 ELI: http://data.europa.eu/eli/reg/2024/1689/ojANNEX III
High-r isk AI sys tems referred to in Ar ticle 6(2)
High-r isk AI systems pursuant to Ar ticle 6(2) are the AI systems listed in an y of the f ollo wing areas:
1. Biometr ics, in so f ar as their use is per mitted under relevant Union or national law:
(a) remote biometr ic identifica tion syste ms.
This shall not include AI systems intende d to be used f or biometr ic ver ifi cation the sole pur pose of which is to
confir m that a specific natural person is the person he or she claims to be;
(b) AI systems inte nded to be used f or biometr ic cate gor isation, according to sensitive or protect ed attr ibutes or
ch aracter istics based on the inference of those attr ibut es or ch aracteristics ;
(c) AI systems inte nded t o be used f or emotion recognition.
2. Cr itical infrastr ucture: AI systems intended to be used as safety comp onents in the managem ent and operation of
cr itical digital infrastr ucture, road traff ic, or in the supply of wate r , g as, heating or electr icity .
3. Education and v ocational training:
(a) AI syste ms intended to be used to determine access or admission or to assign natural persons to educational and
v ocational training institutions at all levels;
(b) AI syste ms inte nded to be used to evaluate lear ning outcomes, including when those outcomes are used to steer
the lear ning process of natural persons in educational and v ocational training institutions at all levels;
(c) AI syste ms inte nded to be used f or the pur pose of assessing the appropr iate level of education that an individual
will receive or will be able to access, in the context of or within educational and v ocational training institutions
at all levels;
(d) AI systems inte nded to be used f or monitoring and det ecting prohibite d behavi our of students dur ing te sts in the
cont ext of or within educational and vocational training institutions at all levels.
4. Emplo yment, wo rkers’ managem ent and access to self-emplo yment:
(a) AI syste ms intended to be used f or the recr uitment or selection of natural persons, in par ticular t o place targe ted
job adver tisements, to analyse and filter job applications, and to evaluate candidates;
(b) AI systems intended to be used to mak e decisions aff ecting te r ms of work -relate d relationships, the promotion or
te r mination of wo rk -related contractual relationships, t o allocat e tasks based on individual behavi our or personal
traits or ch aracter istics or to monito r and evaluate the perf or mance and behavi our of persons in such
relationships.
5. A ccess to and enjo yment of essential pr ivat e ser vices and essential public ser vices and benefits:
(a) AI syste ms intended to be used b y public author ities or on behalf of public author ities t o evaluate the eligibility
of natural persons f or essential public assistance benefits and ser vices, including healthcare ser vices, as well as to
grant, reduce, revok e, or reclaim such benefi ts and ser vices;
(b) AI syste ms intended to be used to evaluate the creditwo r thiness of natural persons or establish their credit score,
with the ex ception of AI systems used f or the pur pose of detect ing financ ial fraud;
(c) AI syste ms intended to be used f or r isk assessment and pr icing in relation to natural persons in the case of life
and health insurance;
OJ L, 12.7.2024
EN
ELI: http://data.europa.eu/eli/reg/2024/1689/oj 127/144(d) AI systems inte nded to evaluate and classify emerge ncy calls by natural persons or to be used to dispatch , or to
establish pr ior ity in the dispatching of, emergency fir st response ser vices, including b y police, firef ighters and
medical aid, as well as of emerg ency healthcare patient tr iage systems.
6. Law enf orcement, in so f ar as their use is per mitted under relevant Union or national law:
(a) AI syste ms intended to be used by or on behalf of law enf orcement author ities, or b y Uni on institutions, bodies,
off ices or age ncies in suppor t of law enf orcement author ities or on their behalf to assess the r isk of a natural
person becoming the victim of cr iminal offences;
(b) AI syste ms intended to be used by or on behalf of law enf orcement author ities or by Uni on institutions, bodies,
off ices or age ncies in suppor t of la w enf orcement author ities as polygraphs or similar tools;
(c) AI syste ms intended to be used by or on behalf of law enf orcement author ities, or b y Uni on institutions, bodies,
off ices or age ncies, in suppor t of la w enf orcement author ities t o evaluate the reliability of evidence in the course
of the invest igation or prosecution of cr iminal offences;
(d) AI syste ms intende d to be used by law enf orcement author ities or on their behalf or b y Uni on institutions,
bodies, offices or age ncies in suppor t of law enf orcement author ities f or assessing the r isk of a natural person
offending or re-offending not solely on the basis of the profil ing of natural persons as referred to in Ar ticle 3(4)
of Directive (EU) 2016/680, or to assess personality traits and ch aracter istics or past cr iminal behavi our of
natural persons or groups;
(e) AI syste ms intended to be used by or on behalf of law enf orcement author ities or by Uni on institutions, bodies,
off ices or age ncies in suppor t of law enf orcement author ities f or the profil ing of natural persons as refer red to in
Ar ticle 3(4) of Directive (EU) 2016/680 in the course of the detection, invest igation or prosecution of cr iminal
offenc es.
7. Migration, asylum and border control management, in so f ar as their use is per mitte d under relevant Uni on or
national law:
(a) AI systems intended to be used by or on behalf of compet ent public author ities or by Uni on institutions, bodies,
off ices or age ncies as polygraphs or similar too ls;
(b) AI systems intended to be used by or on behalf of compet ent public author ities or by Uni on institutions, bodies,
off ices or agencies to assess a r isk, including a secur ity r isk, a r isk of ir regular migration, or a health r isk, posed
by a natural person who inte nds to ent er or who has entered into the terr itory of a Member State;
(c) AI systems intended to be used by or on behalf of compet ent public author ities or by Uni on institutions, bodies,
off ices or agencies to assist compet ent public author ities f or the examination of applications f or asylum, visa or
residence per mits and f or associate d complaints with rega rd to the eligibility of the natural persons applying f or
a status, including relate d assessments of the reliability of evidence;
(d) AI systems intended to be used by or on behalf of comp etent public author ities, or by Uni on institutions, bodies,
off ices or agencies, in the cont ext of migration, asylum or border control management, f or the pur pose of
detect ing, recognising or identifying natural persons, with the ex ception of the ver ificati on of trave l documents.
8. Ad ministration of justice and democratic processes:
(a) AI syste ms intended to be used by a judicial author ity or on their behalf to assist a judicial author ity in
researchi ng and inter preting f acts and the law and in applying the law to a concrete set of f acts, or to be used in
a similar wa y in alt er native dispute resolution;
EN
OJ L, 12.7.2024
128/144 ELI: http://data.europa.eu/eli/reg/2024/1689/oj(b) AI syste ms intended t o be used f or inf luencing the outcome of an election or refere ndum or the v oting
behavio ur of natural persons in the ex ercise of their vot e in elections or refe renda. This does not include AI
syste ms to the output of which natural persons are not directly exposed, such as too ls used t o org anise, opti mise
or str ucture political camp aigns from an administrative or logistical point of view .
OJ L, 12.7.2024
EN
ELI: http://data.europa.eu/eli/reg/2024/1689/oj 129/144ANNEX IV
T echnical document ation referred to in Ar ticle 11(1)
The te chnical documentation refe r red to in Ar ticle 11(1) shall contain at least the f ollowing inf or mation, as applicable to
the relevant AI syste m:
1. A g eneral descr iption of the AI syste m including:
(a) its inte nded pur pose, the name of the provider and the version of the syste m ref lecting its relation t o previous
versions;
(b) ho w the AI system inte racts with, or can be used to interac t with, hardware or sof tware, including with other AI
syste ms, that are not par t of the AI syste m itself, where applicable;
(c) the versions of relevant sof tware or fir m ware, and any requirements related to version update s;
(d) the descr ipt ion of all the f or ms in which the AI system is placed on the marke t or put into ser vice, suc h as
sof tware pack ages embedded into hardware, do wnloads, or APIs;
(e) the descr ipt ion of the hardware on which the AI system is intended to r un;
(f) where the AI system is a comp onent of products, photographs or illustrations showing exte r nal f eatures, the
marking and inte r nal la y out of those products;
(g) a basic descr ipt ion of the user -interface provided t o the deplo yer;
(h) instr uctions f or use f or the deplo y er , and a basic descr ipt ion of the user -inte rface pro vided to the deplo yer , where
applicable;
2. A detailed descr iption of the elements of the AI syste m and of the process f or its development, including:
(a) the methods and st eps perfo r med f or the development of the AI syste m, including, where relevant, recourse to
pre-trained systems or tools pro vided by third par ties and how those were used, integrat ed or modifi ed b y the
provid er ;
(b) the design specif ications of the syste m, namely the g eneral logic of the AI system and of the algor ithms; the k ey
design choices including the rationale and assump tions made, including with regard to persons or groups of
persons in respect of who, the system is intended to be used; the main classific ation ch oices; what the syste m is
designed to op timise f or , and the relevance of the different parameters; the descr iption of the expected output
and output quality of the system; the decisions about any possible trade-off made regard ing the t echnical
solutions adopt ed t o comply with the requirements set out in Chapt er III, Section 2;
(c) the descr iption of the system arch itecture explaining how sof tware comp onents build on or feed into each other
and integrat e into the overall processing; the comp utational resources used to develop, train, test and validat e the
AI system;
(d) where relevant, the data requirements in t er ms of datasheets descr ibing the training methodologies and
te chniques and the training data sets used, including a g eneral descr iption of these data sets, inf or mation about
their prove nance, scope and main ch aracter istics; ho w the data was obtained and selected; labelling procedures
(e.g. f or super vised lear ning), data cleaning methodologies (e.g. outliers detection);
(e) assessment of the human o versight measures needed in accordance with Ar ticle 14, including an assessment of
the te ch nical measures needed to f acilitate the inte r pretation of the outputs of AI syste ms by the deplo yers, in
accordance with Ar ticle 13(3), point (d);
(f) where applicable, a detailed descr ipt ion of pre-deter mined chang es to the AI system and its perfo r mance,
tog ether with all the relevant inf or mation relate d to the te chnical solutions adop te d to ensure continuous
complia nce of the AI system with the relevant requirements set out in Chapt er III, Section 2;
(g) the validation and te sting procedures used, including inf or mation about the validation and te sting data used and
their main ch aracteristics ; metr ics used t o measure accuracy , robustness and compliance with other relevant
requirements set out in Chap ter III, Section 2, as well as pote ntially discr iminat or y imp acts; test logs and all t est
repor ts dated and signed by the responsible persons, including with regard to pre-determ ined ch anges as refe r red
to under point (f);
EN
OJ L, 12.7.2024
130/144 ELI: http://data.europa.eu/eli/reg/2024/1689/oj(h) cybersecur ity measures put in place;
3. Detailed inf or mation about the monitoring, functioning and control of the AI syste m, in par ticular with regard to :
its capabilities and limitations in perf or mance, including the degrees of accuracy f or specif ic persons or groups of
persons on which the system is intended to be used and the overall expected level of accuracy in relation to its
intende d pur pose; the f oreseeable unintended outcomes and sources of r isks t o health and safety , fundamental r ights
and discr imination in view of the intended pur pose of the AI system; the human oversight measures needed in
accordance with Ar ticle 14, including the te chnical measures put in place to f acilitate the inte r pretation of the
outputs of AI syste ms by the deplo yers; specifications on input data, as appropr iate;
4. A descr iption of the appropr iat eness of the perf or mance metr ics f or the specific AI syste m;
5. A detailed descr iption of the r isk managem ent syste m in accordance with Ar ticle 9;
6. A descr iption of relevant ch anges made by the provid er to the syste m through its lif ecy cle;
7. A list of the har monised standards applied in full or in par t the refere nces of which hav e been published in the
Off icial Jour nal of the European Union; where no suc h har monised standards hav e been applied, a detailed descr iption
of the solutions adop ted to meet the requirements set out in Chapt er III, Section 2, including a list of other relevant
standards and te chnical specifications applied;
8. A cop y of the EU declaration of conf or mity refer red to in Ar ticle 47;
9. A detailed descr iption of the system in place to evaluate the AI system perf or mance in the post-mark et phase in
accordance with Ar ticle 72, including the post-mark et monito r ing plan refer red to in Ar ticle 72(3).
OJ L, 12.7.2024
EN
ELI: http://data.europa.eu/eli/reg/2024/1689/oj 131/144ANNEX V
EU declaration of confor mity
The EU declaration of conf or mity refer red to in Ar ticle 47, shall contain all of the f ollowi ng inf or mation:
1. AI syste m name and type and any additional unambiguous refe rence allowi ng the identifi cation and traceability of
the AI syste m;
2. The name and address of the provid er or , where applicable, of their author ised representative;
3. A statem ent that the EU declaration of conf or mity referred t o in Ar ticle 47 is issued under the sole responsibility of
the provid er ;
4. A stat ement that the AI syste m is in conf or mity with this Regulation and, if applicable, with any other relevant
Uni on law that provides f or the issuing of the EU declaration of conf or mity refe r red to in Ar ticle 47;
5. Where an AI syste m inv olves the processing of personal data, a statement that that AI syste m comp lies with
Regulations (EU) 2016/679 and (EU) 2018/1725 and Directive (EU) 2016/680;
6. Ref erences to any relevant har monised standards used or any other common specification in relation to which
conf or mity is declared;
7. Where applicable, the name and identifica tion number of the notified body , a descr iption of the conf or mity
assessment procedure perf or med, and identification of the cer tif icat e issued;
8. The place and date of issue of the declaration, the name and function of the person who signed it, as well as an
indication f or , or on behalf of whom, that person signed, a signature.
EN
OJ L, 12.7.2024
132/144 ELI: http://data.europa.eu/eli/reg/2024/1689/ojANNEX VI
Conform ity assessment procedure based on inter nal control
1. The conf or mity assessment procedure based on inte r nal control is the conf or mity assessment procedure based on
points 2, 3 and 4.
2. The pro vider ver ifi es that the established quality management system is in comp liance with the requirements of
Ar ticle 17.
3. The provider examines the inf or mation contained in the te chnical documentation in order to assess the compliance
of the AI system with the relevant essential requirements set out in Chapt er III, Section 2.
4. The provider also ver ifies that the design and development process of the AI system and its post-mark et monitoring
as refer red to in Ar ticle 72 is consiste nt with the te chnical documentation.
OJ L, 12.7.2024
EN
ELI: http://data.europa.eu/eli/reg/2024/1689/oj 133/144ANNEX VII
Conf or mity based on an assessment of the quality management system and an assessment of the
technical documentation
1. Introduction
Conf or mity based on an assessment of the quality management syste m and an assessment of the t echnical
documentation is the conf or mity assessment procedure based on points 2 to 5.
2. Over view
The approved quality managem ent syste m f or the design, development and t esting of AI systems pursuant to
Ar ticle 17 shall be examined in accordance with point 3 and shall be subject to sur veillance as specified in point 5.
The te chnical documentation of the AI system shall be examined in accordance with point 4.
3. Quality managem ent syste m
3.1. The application of the provider shall include:
(a) the name and address of the provid er and, if the application is lodged by an author ised representative, also their
name and address;
(b) the list of AI syste ms covered under the same quality managem ent syste m;
(c) the t echnical documentation f or each AI system covered under the same quality managem ent syste m;
(d) the documentation concer ning the quality managem ent syste m which shall co ver all the aspects listed under
Ar ticle 17;
(e) a descr iption of the procedures in place to ensure that the quality management system remains adequate and
effe ctive;
(f) a wr itten declaration that the same application has not been lodg ed with any other notified body .
3.2. The quality managem ent system shall be assessed b y the notified body , which shall determine whether it satisfies the
requirements refe r red to in Ar ticle 17.
The decision shall be notif ied to the provider or its author ised representative.
The notification shall contain the conclusions of the assessment of the quality management syste m and the reasoned
assessment decision.
3.3. The quality manag ement syste m as approved shall continue to be implement ed and maintained by the provid er so
that it remains adequate and eff icient.
3.4. Any intended chang e to the approve d quality manag ement system or the list of AI syste ms covered by the latter shall
be brought to the attention of the notified body by the pro vider .
The proposed ch anges shall be examined by the notified body , which shall decide whether the modif ied quality
managem ent system continues to satisfy the requirements referred to in point 3.2 or whether a reassessment is
necessar y .
The notif ied body shall notify the provider of its decision. The notif ication shall contain the conclusions of the
examination of the ch anges and the reasoned assessment decision.
4. Control of the t echnical documentation.
4.1. In addition to the application refer red to in point 3, an application with a notif ied body of their choice shall be
lodg ed by the provider f or the assessment of the te chnical documentation relating to the AI system which the
provid er inte nds to place on the market or put into ser vice and which is covered by the quality managem ent system
referred to under point 3.
4.2. The application shall include:
(a) the name and address of the provider;
(b) a wr itten declaration that the same application has not been lodg ed with any other notified body ;
(c) the t echnical documentation refer red to in Annex IV .
EN
OJ L, 12.7.2024
134/144 ELI: http://data.europa.eu/eli/reg/2024/1689/oj4.3. The tec hnical documentation shall be examined b y the notified body . Where relevant, and limited to what is
necessar y to fulf il its tasks, the notif ied body shall be granted full access t o the training, validation, and te sting data
sets used, including, where appropr iate and subject to secur ity safegua rds, through API or other relevant t echnical
means and too ls enabling remote access.
4.4. In examining the te ch nical documentation, the notif ied body ma y require that the provider supply fur ther evidence
or car r y out fur ther te sts so as to enable a proper assessment of the conf or mity of the AI syste m with the
requirements set out in Chapt er III, Section 2. Where the notif ied body is not satisf ied with the te sts car r ied out by
the provid er , the notif ied body shall itself directly car r y out adequat e te sts, as appropr iate.
4.5. Where necessar y to assess the conf or mity of the high-r isk AI syste m with the requirements set out in Chapt er III,
Section 2, af ter all other reasonable means to ver ify conf or mity hav e been exhaust ed and hav e pro ven to be
insuffic ient, and upon a reasoned request, the notif ied body shall also be grante d access to the training and trained
models of the AI system, including its relevant parameters. Such access shall be subject t o existing Union law on the
protect ion of intellectual proper ty and trade secrets.
4.6. The decision of the notif ied body shall be notif ied t o the provider or its author ised representative. The notif ication
shall contain the conclusions of the assessment of the t echnical documentation and the reasoned assessment
decision.
Where the AI system is in conf or mity with the requirements set out in Chapt er III, Section 2, the notif ied body shall
issue a Union te chnical documentation assessment cer tif icat e. The cer tif icat e shall indicate the name and address of
the provider , the conclusions of the examination, the conditions (if any) f or its validity and the data necessar y f or the
identifica tion of the AI syste m.
The cer tif icat e and its annexe s shall contain all relevant inf or mation to allow the conf or mity of the AI system to be
evaluated, and to allow f or control of the AI system while in use, where applicable.
Where the AI syste m is not in conf or mity with the requirements set out in Chap ter III, Section 2, the notif ied body
shall refuse to issue a Union te ch nical documentation assessment cer tif icat e and shall inf or m the applicant
according ly , giving detailed reasons f or its refusal.
Where the AI syste m does not meet the requirement relating to the data used t o train it, re-training of the AI system
will be needed pr ior to the application f or a new conf or mity assessment. In this case, the reasoned assessment
decision of the notif ied body refusing to issue the Uni on te chnical documentation assessment cer tif icat e shall
contain specific considerations on the quality data used to train the AI syste m, in par ticular on the reasons f or
non-comp liance.
4.7. Any chang e to the AI system that could affe ct the comp liance of the AI system with the requirements or its intended
pur pose shall be assessed by the notified body which issued the Uni on te chnical documentation assessment
cer tif icat e. The provider shall inf or m such notif ied body of its intention to introduce any of the abovementioned
ch anges, or if it other wise becomes aw are of the occur rence of suc h ch anges. The inte nded ch anges shall be assessed
by the notif ied body , which shall decide whether those ch anges require a new conf or mity assessment in accordance
with Ar ticle 43(4) or whether they could be addressed by means of a supplement t o the Uni on t echnical
documentation assessment cer tif icat e. In the latter case, the notif ied body shall assess the chang es, notify the
provid er of its decision and, where the ch ange s are approve d, issue to the provider a supplement to the Uni on
te chnical documentation assessment cer tif icate.
5. Sur veillance of the approve d quality managem ent syste m.
5.1. The pur pose of the sur veillance car r ied out by the notified body referred to in P oint 3 is to mak e sure that the
provid er duly complie s with the te r ms and conditions of the approve d quality managem ent syste m.
5.2. For assessment pur poses, the provider shall allow the notif ied body t o access the premises where the design,
development, testing of the AI systems is taking place. The provid er shall fur ther share with the notif ied body all
necessar y inf or mation.
5.3. The notified body shall car r y out per iodic audits to mak e sure that the provid er maintains and applies the quality
managem ent system and shall provide the provid er with an audit repor t. In the context of those audits, the notif ied
body ma y car r y out additional tests of the AI syste ms f or which a Uni on te ch nical documentation assessment
cer tif icat e was issued.
OJ L, 12.7.2024
EN
ELI: http://data.europa.eu/eli/reg/2024/1689/oj 135/144ANNEX VIII
Infor mation to be submitted upon the registration of high-r isk AI systems in accordance with
Ar ticle 49
Section A — Inf or mation to be submitted by provid ers of high-r isk AI syste ms in accordance with Ar ticle 49(1)
The f ollowing inf or mation shall be provid ed and thereaf te r k ept up to date with rega rd to high-r isk AI systems t o be
register ed in accordance with Ar ticle 49(1):
1. The name, address and contact details of the provid er ;
2. Where submission of inf or mation is car r ied out by another person on behalf of the provider , the name, address and
contact details of that person;
3. The name, address and contact details of the author ised representative, where applicable;
4. The AI system trade name and any additional unambiguous refe rence allo wing the identification and traceability of
the AI syste m;
5. A descr iption of the inte nded pur pose of the AI system and of the comp onents and functions suppor te d through
this AI system;
6. A basic and concise descr iption of the inf or mation used by the syste m (data, inputs) and its operating logic;
7. The status of the AI system (on the mark et, or in ser vice; no longer placed on the market/in ser vice, recalled);
8. The type, number and expir y date of the cer tif icate issued b y the notif ied body and the name or identification
number of that notif ied body , where applicable;
9. A scanned cop y of the cer tif icat e referred to in point 8, where applicable;
10. Any Member Stat es in which the AI syste m has been placed on the marke t, put into ser vice or made ava ilable in the
Uni on;
11. A cop y of the EU declaration of conf or mity refer red to in Ar ticle 47;
12. Electronic instr uctions f or use; this inf or mation shall not be provid ed f or high-r isk AI syste ms in the areas of law
enf orcement or migration, asylum and border control management referred to in Annex III, points 1, 6 and 7;
13. A URL f or additional inf or mation (optional).
Section B — Inf or mation to be submitt ed by provider s of high-r isk AI syste ms in accordance with Ar ticle 49(2)
The f ollowing inf or mation shall be provided and thereaf ter k ept up to dat e with regard to AI syste ms t o be regist ered in
accordance with Ar ticle 49(2):
1. The name, address and contact details of the provid er ;
2. Where submission of inf or mation is car r ied out by another person on behalf of the provider , the name, address and
contact details of that person;
3. The name, address and contact details of the author ised representative, where applicable;
4. The AI system trade name and any additional unambiguous refe rence allo wing the identification and traceability of
the AI syste m;
5. A descr iption of the intended pur pose of the AI system;
6. The condition or conditions under Ar ticle 6(3)based on which the AI syste m is considered to be not-high-r isk;
7. A shor t summar y of the grounds on which the AI system is considered to be not-high-r isk in application of the
procedure under Ar ticle 6(3);
8. The status of the AI system (on the mark et, or in ser vice; no longer placed on the market/in ser vice, recalled);
9. Any Member Stat es in which the AI syste m has been placed on the marke t, put into ser vice or made ava ilable in the
Uni on.
EN
OJ L, 12.7.2024
136/144 ELI: http://data.europa.eu/eli/reg/2024/1689/ojSection C — Inf or mation to be submitt ed by deplo y ers of high-r isk AI systems in accordance with Ar ticle 49(3)
The f ollowing inf or mation shall be provid ed and thereaf te r k ept up to date with rega rd to high-r isk AI systems t o be
register ed in accordance with Ar ticle 49(3):
1. The name, address and contact details of the deplo y er ;
2. The name, address and contact details of the person submitting inf or mation on behalf of the deplo y er ;
3. The URL of the entr y of the AI syste m in the EU database b y its provider;
4. A summar y of the fi ndings of the fundamental r ights impact assessment conducted in accordance with Ar ticle 27;
5. A summar y of the data protect ion imp act assessment car r ied out in accordance with Ar ticle 35 of Regulation (EU)
2016/679 or Ar ticle 27 of Directive (EU) 2016/680 as specified in Ar ticle 26(8) of this Regulation, where
applicable.
OJ L, 12.7.2024
EN
ELI: http://data.europa.eu/eli/reg/2024/1689/oj 137/144ANNEX IX
Inf or mation t o be submitted upon the registration of high-r isk AI systems listed in Annex III in
relation t o testi ng in real w orld conditions in accordance with Ar ticle 60
The f ollowing inf or mation shall be provid ed and thereaf ter kep t up to date with regard to te sting in real world conditions to
be registered in accordance with Ar ticle 60:
1. A Uni on-wide unique sing le identifica tion number of the te sting in real world conditions;
2. The name and contact details of the provider or prospective provid er and of the deplo y ers in v olved in the te sting in
real w orld conditions;
3. A br ief descr iption of the AI syste m, its inte nded pur pose, and other inf or mation necessar y f or the identification of
the syste m;
4. A summar y of the main charact er istics of the plan f or te sting in real world conditions;
5. Inf or mation on the suspension or te r mination of the te sting in real world conditions.
EN
OJ L, 12.7.2024
138/144 ELI: http://data.europa.eu/eli/reg/2024/1689/ojANNEX X
Union legislative acts on larg e-scale IT sys tems in the area of Freedom, Secur ity and Justice
1. Sc hengen Inf or mation System
(a) Regulation (EU) 2018/1860 of the European Pa rliament and of the Council of 28 November 2018 on the use of
the Sc hengen Inf or mation System f or the retur n of illegally sta ying third-countr y nationals (OJ L 312,
7.12.2018, p. 1).
(b) Regulation (EU) 2018/1861 of the European Parliament and of the Council of 28 November 2018 on the
establishment, operation and use of the Sc hengen Inf or mation System (SIS) in the fie ld of border ch ec k s, and
amending the Convention im plementing the Scheng en Agreement, and amending and repealing Regulation (EC)
No 1987/2006 (OJ L 312, 7.12.2018, p. 14).
(c) Regulation (EU) 2018/1862 of the European Parliament and of the Council of 28 November 2018 on the
establishment, operation and use of the Sche ngen Inf or mation Syste m (SIS) in the fie ld of police cooperation and
judicial cooperation in cr iminal matt ers, amending and repealing Council Decision 2007/533/JHA, and
repealing Regulation (EC) No 1986/2006 of the European Parliame nt and of the Council and Commission
Decision 2010/261/EU (OJ L 312, 7.12.2018, p. 56).
2. Visa Inf or mation Syst em
(a) Regulation (EU) 2021/1133 of the European Parliament and of the Council of 7 July 2021 amending
Regulations (EU) No 603/2013, (EU) 2016/794, (EU) 2018/1862, (EU) 2019/816 and (EU) 2019/818 as regard s
the establishment of the conditions f or accessing other EU inf or mation syste ms f or the pur poses of the Visa
Inf or mation Syste m (OJ L 248, 13.7.2021, p. 1).
(b) Regulation (EU) 2021/1134 of the European Parliament and of the Council of 7 July 2021 amending
Regulations (EC) No 767/2008, (EC) No 810/2009, (EU) 2016/399, (EU) 2017/2226, (EU) 2018/1240, (EU)
2018/1860, (EU) 2018/1861, (EU) 2019/817 and (EU) 2019/1896 of the European Parliament and of the
Council and repealing Council Decisions 2004/512/EC and 2008/633/JHA, f or the pur pose of ref or ming the
Visa Inf or mation Syst em (OJ L 248, 13.7.2021, p. 11).
3. Eurodac
Regulation (EU) 2024/1358 of the European Parliament and of the Council of 14 Ma y 2024 on the establishment of
‘Eurodac’ f or the comp ar ison of biometr ic data in order to eff ectively apply Regulations (EU) 2024/1315 and (EU)
2024/1350 of the European Parliament and of the Council and Council Directive 2001/55/EC and to identify
illegally sta ying third-countr y nationals and stat eless persons and on requests f or the compari son with Eurodac data
by Member Stat es’ law enf orcement author ities and Europol f or law enf orcement pur poses, amending Regulations
(EU) 2018/1240 and (EU) 2019/818 of the European Pa rliament and of the Council and repealing Regulation (EU)
No 603/2013 of the European Parliament and of the Council (OJ L, 2024/1358, 22.5.2024, ELI: http://data.europa.
eu/eli/reg/2024/1358/oj).
4. Entr y/Exit System
Regulation (EU) 2017/2226 of the European Pa rliament and of the Council of 30 November 2017 establishing an
Entr y/Exit System (EES) t o regist er entr y and exit data and refusal of entr y data of third-countr y nationals crossing
the exte r nal borders of the Member Stat es and determining the conditions f or access to the EES f or la w enf orcement
pur poses, and amending the Conve ntion im plementing the Sc hengen Agreement and Regulations (EC)
No 767/2008 and (EU) No 1077/2011 (OJ L 327, 9.12.2017, p. 20).
5. European T ravel Inf or mation and Author isation Syste m
(a) Regulation (EU) 2018/1240 of the European Parliament and of the Council of 12 Sep tember 2018 establishing
a European T ravel Inf or mation and Au thor isation Syst em (ETIAS) and amending Regulations (EU)
No 1077/2011, (EU) No 515/2014, (EU) 2016/399, (EU) 2016/1624 and (EU) 2017/2226 (OJ L 236,
19.9.2018, p. 1).
(b) Regulation (EU) 2018/1241 of the European Parliame nt and of the Council of 12 Sept ember 2018 amending
Regulation (EU) 2016/794 f or the pur pose of establishing a European T rave l Inf or mation and Author isation
Syste m (ETIAS) (OJ L 236, 19.9.2018, p. 72).
OJ L, 12.7.2024
EN
ELI: http://data.europa.eu/eli/reg/2024/1689/oj 139/1446. European Cr iminal Records Inf or mation Syst em on third-countr y nationals and stat eless persons
Regulation (EU) 2019/816 of the European Parliament and of the Council of 17 Apr il 2019 establishing
a centralised system f or the identifica tion of Member States holding conviction inf or mation on third-countr y
nationals and statele ss persons (ECRIS- TCN) to supplement the European Cr iminal Records Inf or mation Syste m and
amending Regulation (EU) 2018/1726 (OJ L 135, 22.5.2019, p. 1).
7. Interop erability
(a) Regulation (EU) 2019/817 of the European Parliament and of the Council of 20 Ma y 2019 on establishing
a framework f or interoperability between EU inf or mation syste ms in the fi eld of borders and visa and amending
Regulations (EC) No 767/2008, (EU) 2016/399, (EU) 2017/2226, (EU) 2018/1240, (EU) 2018/1726 and (EU)
2018/1861 of the European Pa rliament and of the Council and Council Decisions 2004/512/EC and
2008/633/JHA (OJ L 135, 22.5.2019, p. 27).
(b) Regulation (EU) 2019/818 of the European Parliament and of the Council of 20 Ma y 2019 on establishing
a framework f or interoperability between EU inf or mation syste ms in the field of police and judicial cooperation,
asylum and migration and amending Regulations (EU) 2018/1726, (EU) 2018/1862 and (EU) 2019/816 (OJ
L 135, 22.5.2019, p. 85).
EN
OJ L, 12.7.2024
140/144 ELI: http://data.europa.eu/eli/reg/2024/1689/ojANNEX XI
T echnical document ation referred to in Ar ticle 53(1), point (a) — technical document ation for
pro viders of general-pur pose AI models
Section 1
Inf or mation to be provided by all providers of general-pur pose AI models
The te chnical documentation refer red to in Ar ticle 53(1), point (a) shall contain at least the f ollowing inf or mation as
appropr iate t o the size and r isk prof ile of the model:
1. A g eneral descr iption of the general-pur pose AI model including:
(a) the tasks that the model is inte nded to perf or m and the type and nature of AI syste ms in which it can be
integrat ed;
(b) the accep table use policies applicable;
(c) the dat e of release and methods of distr ibution;
(d) the arch itec ture and number of parameter s;
(e) the modality (e.g. text, imag e) and f or mat of in puts and outputs;
(f) the licence.
2. A detailed descr iption of the elements of the model refe r red to in point 1, and relevant inf or mation of the process
f or the development, including the f ollowi ng elements:
(a) the t echnical means (e.g. instr uctions of use, infrastr ucture, tools) required f or the g eneral-pur pose AI model to
be integrat ed in AI systems;
(b) the design specific ations of the model and training process, including training methodologies and t echniques,
the ke y design choices including the rationale and assum ptions made; what the model is designed to op timise f or
and the relevance of the diffe rent parameters, as applicable;
(c) inf or mation on the data used f or training, testing and validation, where applicable, including the type and
prove nance of data and curation methodologies (e.g. cleaning, filter ing, etc.), the number of data points, their
scope and main charact er istics; ho w the data was obtained and selecte d as well as all other measures to detect the
unsuitability of data sources and methods to det ect identifia ble biases, where applicable;
(d) the computational resources used to train the model (e.g. number of f loating point operations), training time,
and other relevant details related to the training;
(e) kno wn or estimated energy consump tion of the model.
With regard to point (e), where the energy consump tion of the model is unknow n, the energy consump tion ma y be
based on inf or mation about comp utational resources used.
Section 2
Ad ditional inf or mation to be provided b y provid ers of g eneral-pur pose AI models with syste mic r isk
1. A detailed descr ipt ion of the evaluation strategies, including evaluation results, on the basis of a vailable public
evaluation prot ocols and t ools or other wise of other evaluation methodologies. Evaluation strategies shall include
evaluation cr ite r ia, metr ics and the methodology on the identifica tion of limitations.
2. Where applicable, a detailed descr iption of the measures put in place f or the pur pose of conducting inter nal and/or
external adversar ial te sting (e.g. red te aming), model adap tations, including alignment and fine -tuning.
OJ L, 12.7.2024
EN
ELI: http://data.europa.eu/eli/reg/2024/1689/oj 141/1443. Where applicable, a detailed descr iption of the system arch itecture explaining ho w sof tware com ponents build or
f eed into each other and integrat e into the overa ll processing.
EN
OJ L, 12.7.2024
142/144 ELI: http://data.europa.eu/eli/reg/2024/1689/ojANNEX XII
T ransparency infor mation refe r red t o in Ar ticle 53(1), point (b) — technical document ation for
pro viders of general-pur pose AI models t o do wnstre am pro viders that integ rate the model into their
AI system
The inf or mation refer red to in Ar ticle 53(1), point (b) shall contain at least the f ollowing:
1. A g eneral descr iption of the general-pur pose AI model including:
(a) the tasks that the model is inte nded to perfor m and the type and nature of AI systems into which it can be
integrat ed;
(b) the accep table use policies applicable;
(c) the dat e of release and methods of distr ibution;
(d) ho w the model interac ts, or can be used to interact, with hardware or sof tware that is not par t of the model
itself, where applicable;
(e) the versions of relevant sof tware related to the use of the general-pur pose AI model, where applicable;
(f) the arch itec ture and number of parameter s;
(g) the modality (e.g. text, imag e) and f or mat of in puts and outputs;
(h) the licence f or the model.
2. A descr iption of the elements of the model and of the process f or its development, including:
(a) the te ch nical means (e.g. instr uctions f or use, infrastr ucture, tools) required f or the general-pur pose AI model to
be integrat ed into AI syste ms;
(b) the modality (e.g. te xt, image, etc.) and f or mat of the inputs and outputs and their maximum size (e.g. cont ext
window length, etc.);
(c) inf or mation on the data used f or training, testing and validation, where applicable, including the type and
prove nance of data and curation methodologies.
OJ L, 12.7.2024
EN
ELI: http://data.europa.eu/eli/reg/2024/1689/oj 143/144ANNEX XIII
Cr iter ia f or the designation of general-pur pose AI models with systemic r isk refe r red to in Ar ticle 51
For the pur pose of det er mining that a g eneral-pur pose AI model has capabilities or an imp act equivalent to those set out in
Ar ticle 51(1), point (a), the Commission shall take into account the f ollowing cr ite r ia:
(a) the number of parameter s of the model;
(b) the quality or size of the data set, f or example measured through to kens;
(c) the amount of comp utation used f or training the model, measured in f loating point operations or indicate d by
a combination of other var iables such as estimated cost of training, estimat ed time required f or the training, or
estimated energy consump tion f or the training;
(d) the input and output modalities of the model, suc h as text to te xt (large languag e models), te xt to imag e,
multi-modality , and the state of the ar t thresholds f or determi ning high-impact capabilities f or each modality , and
the specific type of in puts and outputs (e.g. biological sequences);
(e) the benchmarks and evaluations of capabilities of the model, including consider ing the number of tasks without
additional training, adap tability to lear n new , distinct tasks, its level of autonom y and scalability , the t ools it has
access to ;
(f) whether it has a high impact on the inte r nal marke t due to its reac h, which shall be presumed when it has been
made ava ilable to at least 10 000 registered business users established in the Union;
(g) the number of register ed end-users.
EN
OJ L, 12.7.2024
144/144 ELI: http://data.europa.eu/eli/reg/2024/1689/oj